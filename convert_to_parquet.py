"""
Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ CSV Ð² Parquet Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹
"""
import pandas as pd
import numpy as np
from pathlib import Path
import time

def convert_csv_to_parquet(csv_path, parquet_path, delimiter='|', encoding='windows-1251'):
    """
    ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ CSV Ð² Parquet Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…
    """
    print(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {csv_path}...")
    start = time.time()

    # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ CSV
    df = pd.read_csv(
        csv_path,
        delimiter=delimiter,
        encoding=encoding,
        thousands=',',
        low_memory=False
    )

    csv_time = time.time() - start
    csv_size = Path(csv_path).stat().st_size / (1024**2)  # MB

    print(f"âœ“ CSV Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° {csv_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {csv_size:.2f} MB")
    print(f"  Ð¤Ð¾Ñ€Ð¼Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…: {df.shape}")
    print(f"  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

    # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…
    print("\nÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…...")

    categorical_features = ['segment_group', 'obs_month', 'obs_quarter']

    for col in df.columns:
        # ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        if col in categorical_features:
            df[col] = df[col].astype('category')
            print(f"  {col}: category (unique={df[col].nunique()})")
            continue

        col_type = df[col].dtype

        # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… Ñ‚Ð¸Ð¿Ð¾Ð²
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()

            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)

    print(f"âœ“ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°")
    print(f"  ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Parquet
    print(f"\nÐ¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² {parquet_path}...")
    start = time.time()

    df.to_parquet(
        parquet_path,
        engine='pyarrow',
        compression='snappy',  # Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ (Ð¼Ð¾Ð¶Ð½Ð¾ 'gzip' Ð´Ð»Ñ Ð¼ÐµÐ½ÑŒÑˆÐµÐ³Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°)
        index=False
    )

    parquet_time = time.time() - start
    parquet_size = Path(parquet_path).stat().st_size / (1024**2)  # MB

    print(f"âœ“ Parquet ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð·Ð° {parquet_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {parquet_size:.2f} MB")
    print(f"  ÐšÐ¾Ð¼Ð¿Ñ€ÐµÑÑÐ¸Ñ: {(1 - parquet_size/csv_size)*100:.1f}%")

    # Ð¢ÐµÑÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Parquet
    print(f"\nÐ¢ÐµÑÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Parquet...")
    start = time.time()
    df_test = pd.read_parquet(parquet_path)
    load_time = time.time() - start

    print(f"âœ“ Parquet Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° {load_time:.2f} ÑÐµÐº")
    print(f"  Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ: {csv_time/load_time:.1f}x")
    print(f"  Ð¢Ð¸Ð¿Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: {(df.dtypes == df_test.dtypes).all()}")

    return df


def convert_all_datasets():
    """ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÑÐµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°"""
    data_dir = Path("data")
    output_dir = Path("output")

    files_to_convert = [
        # Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        (data_dir / "churn_train_ul.csv", data_dir / "churn_train_ul.parquet"),
        (data_dir / "churn_prod_ul.csv", data_dir / "churn_prod_ul.parquet"),

        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        (output_dir / "train_processed.csv", output_dir / "train_processed.parquet"),
        (output_dir / "val_processed.csv", output_dir / "val_processed.parquet"),
        (output_dir / "test_processed.csv", output_dir / "test_processed.parquet"),
        (output_dir / "prod_processed.csv", output_dir / "prod_processed.parquet"),
    ]

    print("="*60)
    print("ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET")
    print("="*60)

    for csv_path, parquet_path in files_to_convert:
        if not csv_path.exists():
            print(f"\nâš  ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½: {csv_path} (Ñ„Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½)")
            continue

        print(f"\n{'='*60}")
        print(f"Ð¤Ð°Ð¹Ð»: {csv_path.name}")
        print(f"{'='*60}")

        convert_csv_to_parquet(csv_path, parquet_path)

    print(f"\n{'='*60}")
    print("âœ“ Ð’Ð¡Ð• Ð¤ÐÐ™Ð›Ð« ÐšÐžÐÐ’Ð•Ð Ð¢Ð˜Ð ÐžÐ’ÐÐÐ«")
    print(f"{'='*60}")


def compare_read_speeds():
    """Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ CSV vs Parquet"""
    print("\n" + "="*60)
    print("Ð¡Ð ÐÐ’ÐÐ•ÐÐ˜Ð• Ð¡ÐšÐžÐ ÐžÐ¡Ð¢Ð˜ Ð§Ð¢Ð•ÐÐ˜Ð¯")
    print("="*60)

    csv_path = Path("output/train_processed.csv")
    parquet_path = Path("output/train_processed.parquet")

    if not csv_path.exists() or not parquet_path.exists():
        print("âš  Ð¤Ð°Ð¹Ð»Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð´Ð»Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ")
        return

    # CSV
    print("\nðŸ“„ CSV:")
    start = time.time()
    df_csv = pd.read_csv(csv_path, delimiter='|')
    csv_time = time.time() - start
    csv_size = csv_path.stat().st_size / (1024**2)
    print(f"  Ð’Ñ€ÐµÐ¼Ñ: {csv_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€: {csv_size:.2f} MB")
    print(f"  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: {df_csv.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

    # Parquet
    print("\nðŸš€ Parquet:")
    start = time.time()
    df_parquet = pd.read_parquet(parquet_path)
    parquet_time = time.time() - start
    parquet_size = parquet_path.stat().st_size / (1024**2)
    print(f"  Ð’Ñ€ÐµÐ¼Ñ: {parquet_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€: {parquet_size:.2f} MB")
    print(f"  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: {df_parquet.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

    print(f"\nðŸ“Š Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢:")
    print(f"  Parquet Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ Ð² {csv_time/parquet_time:.1f}x Ñ€Ð°Ð·")
    print(f"  Parquet Ð¼ÐµÐ½ÑŒÑˆÐµ Ð² {csv_size/parquet_size:.1f}x Ñ€Ð°Ð·")

    # Ð§Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ðµ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸)
    print(f"\nðŸ“– Ð§ÐÐ¡Ð¢Ð˜Ð§ÐÐžÐ• Ð§Ð¢Ð•ÐÐ˜Ð• (5 ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº):")

    columns_to_read = ['cli_code', 'observation_point', 'target_churn_3m',
                      'segment_group', 'avg_activity_6m']

    start = time.time()
    df_partial = pd.read_parquet(parquet_path, columns=columns_to_read)
    partial_time = time.time() - start

    print(f"  Parquet (Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾): {partial_time:.2f} ÑÐµÐº")
    print(f"  Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ: {parquet_time/partial_time:.1f}x (Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð¿Ð¾Ð»Ð½Ñ‹Ð¼ Ñ‡Ñ‚ÐµÐ½Ð¸ÐµÐ¼)")
    print(f"  âš  CSV Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ðµ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº!")


if __name__ == "__main__":
    # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²ÑÐµ Ñ„Ð°Ð¹Ð»Ñ‹
    convert_all_datasets()

    # Ð¡Ñ€Ð°Ð²Ð½Ð¸Ñ‚ÑŒ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ
    compare_read_speeds()
