{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ===============================================================================\n",
    "# ФИНАЛЬНЫЙ АНАЛИЗ И РЕКОМЕНДАЦИИ\n",
    "# ===============================================================================\n",
    "\n",
    "**Цель:** Глубокий анализ лучших моделей и финальные рекомендации для банка\n",
    "\n",
    "**Анализ включает:**\n",
    "- Итоговая таблица всех 40 экспериментов\n",
    "- PSI анализ (Population Stability Index)\n",
    "- Percentile/Decile анализ\n",
    "- Feature Importance анализ\n",
    "- Correlation vs Importance анализ\n",
    "- ROC-AUC кривые\n",
    "- Финальные рекомендации для внедрения\n",
    "\n",
    "**Дата:** 2025-01-13  \n",
    "**Random seed:** 42\n",
    "\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "# Настройки визуализации\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ФИНАЛЬНЫЙ АНАЛИЗ И РЕКОМЕНДАЦИИ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация\"\"\"\n",
    "    \n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # ПУТИ\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    MODELS_DIR = Path(\"models\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # КОЛОНКИ\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    \n",
    "    # СЕГМЕНТЫ\n",
    "    SEGMENTS = {\n",
    "        'Segment 1': {\n",
    "            'name': 'Small Business',\n",
    "            'train': 'seg1_train.parquet',\n",
    "            'test': 'seg1_test.parquet',\n",
    "            'correlation': 'correlation_segment1.csv'\n",
    "        },\n",
    "        'Segment 2': {\n",
    "            'name': 'Middle + Large Business',\n",
    "            'train': 'seg2_train.parquet',\n",
    "            'test': 'seg2_test.parquet',\n",
    "            'correlation': 'correlation_segment2.csv'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # PSI THRESHOLDS\n",
    "    PSI_THRESHOLD_GOOD = 0.1\n",
    "    PSI_THRESHOLD_WARNING = 0.25\n",
    "    \n",
    "    # VISUALIZAION\n",
    "    FIGURE_SIZE = (12, 6)\n",
    "    FIGURE_DPI = 100\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.MODELS_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегментов: {len(config.SEGMENTS)}\")\n",
    "print(f\"  PSI порог (хорошо): {config.PSI_THRESHOLD_GOOD}\")\n",
    "print(f\"  PSI порог (предупреждение): {config.PSI_THRESHOLD_WARNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА РЕЗУЛЬТАТОВ ВСЕХ ЭКСПЕРИМЕНТОВ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА РЕЗУЛЬТАТОВ ЭКСПЕРИМЕНТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "experiments_file = config.OUTPUT_DIR / 'experiments_all.csv'\n",
    "\n",
    "if not experiments_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Файл не найден: {experiments_file}\\n\"\n",
    "        f\"Сначала запустите notebook 03_experiments_xgboost_rf.ipynb\"\n",
    "    )\n",
    "\n",
    "experiments_df = pd.read_csv(experiments_file)\n",
    "\n",
    "print(f\"\\n✓ Загружены результаты экспериментов\")\n",
    "print(f\"  Файл: {experiments_file}\")\n",
    "print(f\"  Всего экспериментов: {len(experiments_df)}\")\n",
    "print(f\"  Алгоритмы: {', '.join(sorted(experiments_df['algorithm'].unique()))}\")\n",
    "print(f\"  Методы балансировки: {len(experiments_df['balancing_method'].unique())}\")\n",
    "print(f\"  Сегменты: {len(experiments_df['segment_group'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ДАННЫХ ДЛЯ АНАЛИЗА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ ДЛЯ АНАЛИЗА\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for seg_id, seg_info in config.SEGMENTS.items():\n",
    "    print(f\"\\n{seg_id}: {seg_info['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    data[seg_id] = {}\n",
    "    \n",
    "    # Загрузка train и test данных\n",
    "    for split in ['train', 'test']:\n",
    "        file_path = config.OUTPUT_DIR / seg_info[split]\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Файл не найден: {file_path}\")\n",
    "        \n",
    "        df = pd.read_parquet(file_path)\n",
    "        data[seg_id][split] = df\n",
    "        \n",
    "        churn_rate = df[config.TARGET_COLUMN].mean()\n",
    "        print(f\"  {split.upper():5s}: {df.shape} | Churn: {churn_rate*100:.2f}%\")\n",
    "    \n",
    "    # Загрузка correlation данных\n",
    "    corr_path = config.OUTPUT_DIR / seg_info['correlation']\n",
    "    if corr_path.exists():\n",
    "        data[seg_id]['correlation'] = pd.read_csv(corr_path)\n",
    "        print(f\"  CORR : {len(data[seg_id]['correlation'])} признаков\")\n",
    "    else:\n",
    "        print(f\"  CORR : файл не найден (будет пропущен)\")\n",
    "        data[seg_id]['correlation'] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все данные загружены успешно\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ИТОГОВАЯ ТАБЛИЦА ВСЕХ ЭКСПЕРИМЕНТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all-experiments-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ПОЛНАЯ ТАБЛИЦА ВСЕХ ЭКСПЕРИМЕНТОВ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ИТОГОВАЯ ТАБЛИЦА ВСЕХ ЭКСПЕРИМЕНТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сортируем по segment, потом по ROC-AUC\n",
    "experiments_sorted = experiments_df.sort_values(\n",
    "    ['segment_group', 'roc_auc'],\n",
    "    ascending=[True, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Форматируем для вывода\n",
    "display_cols = [\n",
    "    'segment_group', 'algorithm', 'balancing_method',\n",
    "    'roc_auc', 'gini', 'f1', 'precision', 'recall',\n",
    "    'train_time_sec'\n",
    "]\n",
    "\n",
    "print(\"\\nВСЕ 40 ЭКСПЕРИМЕНТОВ (отсортировано по сегменту и ROC-AUC):\")\n",
    "print(\"=\"*80)\n",
    "print(experiments_sorted[display_cols].to_string(index=True))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top10-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: ТОП-10 МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ: ТОП-10 МОДЕЛЕЙ ПО ROC-AUC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Берем топ-10\n",
    "top10 = experiments_df.sort_values('roc_auc', ascending=False).head(10).copy()\n",
    "top10['model_label'] = (\n",
    "    top10['segment_group'] + ' | ' + \n",
    "    top10['algorithm'] + ' | ' + \n",
    "    top10['balancing_method'].str[:15]\n",
    ")\n",
    "\n",
    "# Создаем график\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "bars = ax.barh(\n",
    "    range(len(top10)),\n",
    "    top10['roc_auc'],\n",
    "    color=sns.color_palette(\"husl\", len(top10))\n",
    ")\n",
    "\n",
    "ax.set_yticks(range(len(top10)))\n",
    "ax.set_yticklabels(top10['model_label'])\n",
    "ax.set_xlabel('ROC-AUC', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ТОП-10 МОДЕЛЕЙ ПО ROC-AUC', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Добавляем значения на баре\n",
    "for i, (bar, value) in enumerate(zip(bars, top10['roc_auc'])):\n",
    "    ax.text(\n",
    "        value + 0.001, i, f'{value:.4f}',\n",
    "        va='center', fontsize=10, fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'top10_models_roc_auc.png', dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ График сохранен: {config.FIGURES_DIR / 'top10_models_roc_auc.png'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ВЫБОР ЛУЧШИХ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-best-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВЫБОР ЛУЧШИХ МОДЕЛЕЙ ДЛЯ КАЖДОГО СЕГМЕНТА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВЫБОР ЛУЧШИХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_models_info = {}\n",
    "\n",
    "for seg_id in config.SEGMENTS.keys():\n",
    "    # Находим лучшую модель для сегмента\n",
    "    seg_experiments = experiments_df[experiments_df['segment_group'] == seg_id]\n",
    "    best_exp = seg_experiments.sort_values('roc_auc', ascending=False).iloc[0]\n",
    "    \n",
    "    # Определяем файл модели\n",
    "    seg_num = seg_id.split()[1]  # '1' or '2'\n",
    "    algo_name = best_exp['algorithm'].lower().replace(' ', '_')\n",
    "    model_filename = f\"best_{algo_name}_seg{seg_num}.pkl\"\n",
    "    model_path = config.MODELS_DIR / model_filename\n",
    "    \n",
    "    # Загружаем модель\n",
    "    if model_path.exists():\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        best_models_info[seg_id] = {\n",
    "            'model': model,\n",
    "            'algorithm': best_exp['algorithm'],\n",
    "            'balancing_method': best_exp['balancing_method'],\n",
    "            'roc_auc': best_exp['roc_auc'],\n",
    "            'gini': best_exp['gini'],\n",
    "            'f1': best_exp['f1'],\n",
    "            'precision': best_exp['precision'],\n",
    "            'recall': best_exp['recall'],\n",
    "            'threshold': best_exp['threshold'],\n",
    "            'train_time_sec': best_exp['train_time_sec'],\n",
    "            'model_file': model_filename\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  Алгоритм: {best_exp['algorithm']}\")\n",
    "        print(f\"  Метод балансировки: {best_exp['balancing_method']}\")\n",
    "        print(f\"  ROC-AUC: {best_exp['roc_auc']:.4f}\")\n",
    "        print(f\"  Gini: {best_exp['gini']:.4f}\")\n",
    "        print(f\"  F1-Score: {best_exp['f1']:.4f}\")\n",
    "        print(f\"  Precision: {best_exp['precision']:.4f}\")\n",
    "        print(f\"  Recall: {best_exp['recall']:.4f}\")\n",
    "        print(f\"  Optimal Threshold: {best_exp['threshold']:.4f}\")\n",
    "        print(f\"  Время обучения: {best_exp['train_time_sec']:.2f} сек\")\n",
    "        print(f\"  Файл модели: {model_filename}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {seg_id}: Модель не найдена - {model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Загружено лучших моделей: {len(best_models_info)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. PSI ANALYSIS (Population Stability Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psi-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФУНКЦИИ ДЛЯ PSI АНАЛИЗА\n",
    "# ====================================================================================\n",
    "\n",
    "def calculate_psi(expected, actual, buckets=10):\n",
    "    \"\"\"\n",
    "    Рассчитать PSI (Population Stability Index).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    expected : array-like\n",
    "        Предсказания на train (baseline)\n",
    "    actual : array-like\n",
    "        Предсказания на test\n",
    "    buckets : int\n",
    "        Количество deciles\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    psi_value : float\n",
    "        Итоговый PSI score\n",
    "    psi_df : DataFrame\n",
    "        Детальная таблица по deciles\n",
    "    \"\"\"\n",
    "    \n",
    "    # Определяем границы deciles на основе expected (train)\n",
    "    breakpoints = np.percentile(expected, np.arange(0, 100 + 100/buckets, 100/buckets))\n",
    "    breakpoints = np.unique(breakpoints)  # Удаляем дубликаты\n",
    "    \n",
    "    # Считаем распределения\n",
    "    expected_percents = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "    \n",
    "    # Избегаем деления на ноль\n",
    "    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)\n",
    "    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)\n",
    "    \n",
    "    # Рассчитываем PSI для каждого decile\n",
    "    psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n",
    "    \n",
    "    # Итоговый PSI\n",
    "    psi_value = np.sum(psi_values)\n",
    "    \n",
    "    # Создаем детальную таблицу\n",
    "    psi_df = pd.DataFrame({\n",
    "        'Decile': range(1, len(psi_values) + 1),\n",
    "        'Expected_Pct': expected_percents,\n",
    "        'Actual_Pct': actual_percents,\n",
    "        'PSI': psi_values\n",
    "    })\n",
    "    \n",
    "    return psi_value, psi_df\n",
    "\n",
    "\n",
    "def interpret_psi(psi_value):\n",
    "    \"\"\"\n",
    "    Интерпретация PSI score.\n",
    "    \"\"\"\n",
    "    if psi_value < 0.1:\n",
    "        return \"✅ Отлично - модель стабильна\"\n",
    "    elif psi_value < 0.25:\n",
    "        return \"⚠️ Предупреждение - небольшие изменения\"\n",
    "    else:\n",
    "        return \"❌ Критично - значительный drift\"\n",
    "\n",
    "\n",
    "print(\"✓ Функции PSI анализа определены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psi-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# PSI АНАЛИЗ ДЛЯ ЛУЧШИХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PSI ANALYSIS (Population Stability Index)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "psi_results = {}\n",
    "\n",
    "for seg_id, model_info in best_models_info.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train = data[seg_id]['train'].drop(columns=[config.TARGET_COLUMN])\n",
    "    X_test = data[seg_id]['test'].drop(columns=[config.TARGET_COLUMN])\n",
    "    \n",
    "    # Предсказания\n",
    "    model = model_info['model']\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # PSI расчет\n",
    "    psi_value, psi_df = calculate_psi(y_train_proba, y_test_proba, buckets=10)\n",
    "    interpretation = interpret_psi(psi_value)\n",
    "    \n",
    "    psi_results[seg_id] = {\n",
    "        'psi_value': psi_value,\n",
    "        'psi_df': psi_df,\n",
    "        'interpretation': interpretation\n",
    "    }\n",
    "    \n",
    "    # Вывод результатов\n",
    "    print(f\"\\nPSI Score: {psi_value:.4f}\")\n",
    "    print(f\"Интерпретация: {interpretation}\")\n",
    "    print(\"\\nДетальная таблица по deciles:\")\n",
    "    print(psi_df.to_string(index=False))\n",
    "    \n",
    "    # Добавляем PSI в model_info\n",
    "    best_models_info[seg_id]['psi'] = psi_value\n",
    "    best_models_info[seg_id]['psi_interpretation'] = interpretation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ PSI анализ завершен\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. PERCENTILE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "percentile-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФУНКЦИИ ДЛЯ PERCENTILE АНАЛИЗА\n",
    "# ====================================================================================\n",
    "\n",
    "def percentile_analysis(y_true, y_pred_proba, n_deciles=10):\n",
    "    \"\"\"\n",
    "    Провести percentile/decile анализ.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Истинные значения target\n",
    "    y_pred_proba : array-like\n",
    "        Предсказанные вероятности\n",
    "    n_deciles : int\n",
    "        Количество deciles\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    percentile_df : DataFrame\n",
    "        Таблица с метриками по deciles\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    # Сортируем по вероятности (от высокой к низкой)\n",
    "    df = df.sort_values('y_pred_proba', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Разбиваем на deciles\n",
    "    df['decile'] = pd.qcut(df.index, q=n_deciles, labels=range(1, n_deciles + 1), duplicates='drop')\n",
    "    \n",
    "    # Общий churn rate\n",
    "    overall_churn_rate = df['y_true'].mean()\n",
    "    \n",
    "    # Группируем по deciles\n",
    "    results = []\n",
    "    \n",
    "    for decile in sorted(df['decile'].unique()):\n",
    "        decile_df = df[df['decile'] == decile]\n",
    "        \n",
    "        n_obs = len(decile_df)\n",
    "        n_target = decile_df['y_true'].sum()\n",
    "        churn_rate = n_target / n_obs if n_obs > 0 else 0\n",
    "        \n",
    "        # Precision для этого decile\n",
    "        precision = churn_rate\n",
    "        \n",
    "        # Lift\n",
    "        lift = churn_rate / overall_churn_rate if overall_churn_rate > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Decile': int(decile),\n",
    "            'N_Observations': n_obs,\n",
    "            'N_Target': int(n_target),\n",
    "            'Churn_Rate': churn_rate,\n",
    "            'Precision': precision,\n",
    "            'Lift': lift\n",
    "        })\n",
    "    \n",
    "    percentile_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Cumulative metrics\n",
    "    percentile_df['Cumulative_N_Target'] = percentile_df['N_Target'].cumsum()\n",
    "    percentile_df['Cumulative_N_Obs'] = percentile_df['N_Observations'].cumsum()\n",
    "    percentile_df['Cumulative_Precision'] = (\n",
    "        percentile_df['Cumulative_N_Target'] / percentile_df['Cumulative_N_Obs']\n",
    "    )\n",
    "    percentile_df['Cumulative_Recall'] = (\n",
    "        percentile_df['Cumulative_N_Target'] / df['y_true'].sum()\n",
    "    )\n",
    "    \n",
    "    return percentile_df\n",
    "\n",
    "\n",
    "print(\"✓ Функции percentile анализа определены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "percentile-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# PERCENTILE АНАЛИЗ ДЛЯ ЛУЧШИХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERCENTILE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "percentile_results = {}\n",
    "\n",
    "for seg_id, model_info in best_models_info.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_test = data[seg_id]['test'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_test = data[seg_id]['test'][config.TARGET_COLUMN]\n",
    "    \n",
    "    # Предсказания\n",
    "    model = model_info['model']\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Percentile анализ\n",
    "    percentile_df = percentile_analysis(y_test.values, y_test_proba, n_deciles=10)\n",
    "    \n",
    "    percentile_results[seg_id] = percentile_df\n",
    "    \n",
    "    # Вывод таблицы\n",
    "    print(\"\\nТаблица по deciles (от высокой вероятности к низкой):\")\n",
    "    print(percentile_df.to_string(index=False))\n",
    "    \n",
    "    # Сохраняем\n",
    "    seg_num = seg_id.split()[1]\n",
    "    output_file = config.OUTPUT_DIR / f'percentile_analysis_seg{seg_num}.csv'\n",
    "    percentile_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Сохранено: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Percentile анализ завершен\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. FEATURE IMPORTANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФУНКЦИИ ДЛЯ FEATURE IMPORTANCE\n",
    "# ====================================================================================\n",
    "\n",
    "def get_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"\n",
    "    Извлечь feature importance из модели.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained model\n",
    "        Обученная модель\n",
    "    feature_names : list\n",
    "        Имена признаков\n",
    "    top_n : int\n",
    "        Количество топ признаков\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    importance_df : DataFrame\n",
    "        Таблица с feature importance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Проверяем тип модели и извлекаем importance\n",
    "    model_type = type(model).__name__\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # RandomForest, XGBoost, LightGBM\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'get_feature_importance'):\n",
    "        # CatBoost\n",
    "        importances = model.get_feature_importance()\n",
    "    else:\n",
    "        raise ValueError(f\"Модель {model_type} не поддерживает feature importance\")\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Сортируем и берем топ\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "print(\"✓ Функции feature importance определены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# FEATURE IMPORTANCE АНАЛИЗ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_importance_results = {}\n",
    "\n",
    "for seg_id, model_info in best_models_info.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Получаем имена признаков\n",
    "    X_test = data[seg_id]['test'].drop(columns=[config.TARGET_COLUMN])\n",
    "    feature_names = X_test.columns.tolist()\n",
    "    \n",
    "    # Извлекаем feature importance\n",
    "    model = model_info['model']\n",
    "    importance_df = get_feature_importance(model, feature_names, top_n=20)\n",
    "    \n",
    "    feature_importance_results[seg_id] = importance_df\n",
    "    \n",
    "    # Вывод таблицы\n",
    "    print(\"\\nТОП-20 важных признаков:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "    \n",
    "    # Визуализация\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    ax.barh(\n",
    "        range(len(importance_df)),\n",
    "        importance_df['Importance'],\n",
    "        color=sns.color_palette(\"viridis\", len(importance_df))\n",
    "    )\n",
    "    \n",
    "    ax.set_yticks(range(len(importance_df)))\n",
    "    ax.set_yticklabels(importance_df['Feature'])\n",
    "    ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(\n",
    "        f'ТОП-20 ВАЖНЫХ ПРИЗНАКОВ\\n{seg_id}: {model_info[\"algorithm\"]}',\n",
    "        fontsize=14, fontweight='bold', pad=20\n",
    "    )\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    seg_num = seg_id.split()[1]\n",
    "    fig_path = config.FIGURES_DIR / f'feature_importance_seg{seg_num}.png'\n",
    "    plt.savefig(fig_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ График сохранен: {fig_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Feature importance анализ завершен\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. CORRELATION VS IMPORTANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-importance-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION VS IMPORTANCE ANALYSIS\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATION VS IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id in best_models_info.keys():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Проверяем наличие correlation данных\n",
    "    if data[seg_id]['correlation'] is None:\n",
    "        print(\"  ⚠️ Correlation данные не найдены, пропускаем анализ\")\n",
    "        continue\n",
    "    \n",
    "    # Получаем топ-20 по корреляции\n",
    "    corr_df = data[seg_id]['correlation'].copy()\n",
    "    \n",
    "    # Проверяем структуру файла\n",
    "    if 'feature' in corr_df.columns and 'abs_correlation' in corr_df.columns:\n",
    "        top_corr = set(corr_df.nlargest(20, 'abs_correlation')['feature'].values)\n",
    "    elif 'Feature' in corr_df.columns and 'Abs_Correlation' in corr_df.columns:\n",
    "        top_corr = set(corr_df.nlargest(20, 'Abs_Correlation')['Feature'].values)\n",
    "    else:\n",
    "        print(f\"  ⚠️ Неизвестная структура correlation файла: {corr_df.columns.tolist()}\")\n",
    "        continue\n",
    "    \n",
    "    # Получаем топ-20 по importance\n",
    "    importance_df = feature_importance_results[seg_id]\n",
    "    top_importance = set(importance_df['Feature'].values)\n",
    "    \n",
    "    # Находим пересечения\n",
    "    intersection = top_corr & top_importance\n",
    "    only_corr = top_corr - top_importance\n",
    "    only_importance = top_importance - top_corr\n",
    "    \n",
    "    # Вывод результатов\n",
    "    print(f\"\\nТОП-20 по корреляции: {len(top_corr)} признаков\")\n",
    "    print(f\"ТОП-20 по importance: {len(top_importance)} признаков\")\n",
    "    print(f\"\\nПересечение (в обоих топах): {len(intersection)} признаков\")\n",
    "    \n",
    "    if intersection:\n",
    "        print(\"\\nПризнаки в обоих топах:\")\n",
    "        for feat in sorted(intersection):\n",
    "            print(f\"  • {feat}\")\n",
    "    \n",
    "    print(f\"\\nТолько в ТОП-20 корреляции: {len(only_corr)} признаков\")\n",
    "    if only_corr and len(only_corr) <= 10:\n",
    "        for feat in sorted(only_corr):\n",
    "            print(f\"  • {feat}\")\n",
    "    \n",
    "    print(f\"\\nТолько в ТОП-20 importance: {len(only_importance)} признаков\")\n",
    "    if only_importance and len(only_importance) <= 10:\n",
    "        for feat in sorted(only_importance):\n",
    "            print(f\"  • {feat}\")\n",
    "    \n",
    "    # Insights\n",
    "    overlap_pct = len(intersection) / 20 * 100\n",
    "    print(f\"\\nINSIGHT: {overlap_pct:.1f}% перекрытия между корреляцией и importance\")\n",
    "    \n",
    "    if overlap_pct > 70:\n",
    "        print(\"  ✅ Высокое совпадение - модель полагается на линейно значимые признаки\")\n",
    "    elif overlap_pct > 40:\n",
    "        print(\"  ⚠️ Умеренное совпадение - модель находит нелинейные зависимости\")\n",
    "    else:\n",
    "        print(\"  ℹ️ Низкое совпадение - модель сильно опирается на нелинейные паттерны\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Correlation vs Importance анализ завершен\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. ROC-AUC CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ROC-AUC КРИВЫЕ ДЛЯ ЛУЧШИХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROC-AUC CURVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "for idx, (seg_id, model_info) in enumerate(best_models_info.items()):\n",
    "    # Подготовка данных\n",
    "    X_test = data[seg_id]['test'].drop(columns=[config.TARGET_COLUMN])\n",
    "    y_test = data[seg_id]['test'][config.TARGET_COLUMN]\n",
    "    \n",
    "    # Предсказания\n",
    "    model = model_info['model']\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # ROC кривая\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    roc_auc = model_info['roc_auc']\n",
    "    \n",
    "    # Рисуем\n",
    "    label = f\"{seg_id} | {model_info['algorithm']} (AUC = {roc_auc:.4f})\"\n",
    "    ax.plot(fpr, tpr, color=colors[idx], lw=2, label=label)\n",
    "\n",
    "# Diagonal reference line\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC CURVES - ЛУЧШИЕ МОДЕЛИ', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "roc_fig_path = config.FIGURES_DIR / 'roc_curves_best_models.png'\n",
    "plt.savefig(roc_fig_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ График сохранен: {roc_fig_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. ФИНАЛЬНЫЕ РЕКОМЕНДАЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФОРМИРОВАНИЕ ФИНАЛЬНЫХ РЕКОМЕНДАЦИЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ФОРМИРОВАНИЕ ФИНАЛЬНЫХ РЕКОМЕНДАЦИЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "recommendations.append(\"=\"*80)\n",
    "recommendations.append(\"РЕКОМЕНДАЦИИ ДЛЯ БАНКА\")\n",
    "recommendations.append(\"ПРОГНОЗИРОВАНИЕ ОТТОКА КЛИЕНТОВ (CHURN PREDICTION)\")\n",
    "recommendations.append(\"=\"*80)\n",
    "recommendations.append(\"\")\n",
    "\n",
    "# Для каждого сегмента\n",
    "for seg_id, model_info in best_models_info.items():\n",
    "    seg_name = config.SEGMENTS[seg_id]['name']\n",
    "    \n",
    "    recommendations.append(\"=\"*80)\n",
    "    recommendations.append(f\"{seg_id.upper()}: {seg_name.upper()}\")\n",
    "    recommendations.append(\"=\"*80)\n",
    "    recommendations.append(\"\")\n",
    "    \n",
    "    # Информация о модели\n",
    "    recommendations.append(\"ЛУЧШАЯ МОДЕЛЬ:\")\n",
    "    recommendations.append(\"-\" * 80)\n",
    "    recommendations.append(f\"  Алгоритм: {model_info['algorithm']}\")\n",
    "    recommendations.append(f\"  Метод балансировки: {model_info['balancing_method']}\")\n",
    "    recommendations.append(\"\")\n",
    "    \n",
    "    # Метрики\n",
    "    recommendations.append(\"МЕТРИКИ КАЧЕСТВА (на test set):\")\n",
    "    recommendations.append(\"-\" * 80)\n",
    "    recommendations.append(f\"  ROC-AUC:   {model_info['roc_auc']:.4f}\")\n",
    "    recommendations.append(f\"  Gini:      {model_info['gini']:.4f}\")\n",
    "    recommendations.append(f\"  F1-Score:  {model_info['f1']:.4f}\")\n",
    "    recommendations.append(f\"  Precision: {model_info['precision']:.4f}\")\n",
    "    recommendations.append(f\"  Recall:    {model_info['recall']:.4f}\")\n",
    "    recommendations.append(f\"  Optimal Threshold: {model_info['threshold']:.4f}\")\n",
    "    recommendations.append(\"\")\n",
    "    \n",
    "    # PSI\n",
    "    recommendations.append(\"СТАБИЛЬНОСТЬ МОДЕЛИ (PSI):\")\n",
    "    recommendations.append(\"-\" * 80)\n",
    "    recommendations.append(f\"  PSI Score: {model_info['psi']:.4f}\")\n",
    "    recommendations.append(f\"  Интерпретация: {model_info['psi_interpretation']}\")\n",
    "    recommendations.append(\"\")\n",
    "    \n",
    "    # Время обучения\n",
    "    recommendations.append(\"ПРАКТИЧНОСТЬ:\")\n",
    "    recommendations.append(\"-\" * 80)\n",
    "    recommendations.append(f\"  Время обучения: {model_info['train_time_sec']:.2f} сек\")\n",
    "    recommendations.append(f\"  Файл модели: {model_info['model_file']}\")\n",
    "    recommendations.append(\"\")\n",
    "    \n",
    "    # Топ-5 признаков\n",
    "    recommendations.append(\"ТОП-5 ВАЖНЫХ ПРИЗНАКОВ:\")\n",
    "    recommendations.append(\"-\" * 80)\n",
    "    importance_df = feature_importance_results[seg_id]\n",
    "    for idx, row in importance_df.head(5).iterrows():\n",
    "        rank = list(importance_df.head(5).index).index(idx) + 1\n",
    "        recommendations.append(f\"  {rank}. {row['Feature']:40s} - importance: {row['Importance']:.4f}\")\n",
    "    recommendations.append(\"\")\n",
    "    \n",
    "    # Обоснование\n",
    "    recommendations.append(\"ОБОСНОВАНИЕ ВЫБОРА:\")\n",
    "    recommendations.append(\"-\" * 80)\n",
    "    \n",
    "    # Метрики\n",
    "    if model_info['roc_auc'] >= 0.75:\n",
    "        recommendations.append(f\"  ✅ Отличное качество: ROC-AUC = {model_info['roc_auc']:.4f} (≥ 0.75)\")\n",
    "    elif model_info['roc_auc'] >= 0.70:\n",
    "        recommendations.append(f\"  ✓ Хорошее качество: ROC-AUC = {model_info['roc_auc']:.4f} (≥ 0.70)\")\n",
    "    else:\n",
    "        recommendations.append(f\"  ⚠️ Приемлемое качество: ROC-AUC = {model_info['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Стабильность\n",
    "    if model_info['psi'] < config.PSI_THRESHOLD_GOOD:\n",
    "        recommendations.append(f\"  ✅ Отличная стабильность: PSI = {model_info['psi']:.4f} (< 0.1)\")\n",
    "    elif model_info['psi'] < config.PSI_THRESHOLD_WARNING:\n",
    "        recommendations.append(f\"  ⚠️ Умеренная стабильность: PSI = {model_info['psi']:.4f} (< 0.25)\")\n",
    "    else:\n",
    "        recommendations.append(f\"  ❌ Низкая стабильность: PSI = {model_info['psi']:.4f} (≥ 0.25)\")\n",
    "    \n",
    "    # Время\n",
    "    if model_info['train_time_sec'] < 60:\n",
    "        recommendations.append(f\"  ✅ Быстрое обучение: {model_info['train_time_sec']:.1f}s (< 1 мин)\")\n",
    "    elif model_info['train_time_sec'] < 300:\n",
    "        recommendations.append(f\"  ✓ Приемлемое время: {model_info['train_time_sec']:.1f}s (< 5 мин)\")\n",
    "    else:\n",
    "        recommendations.append(f\"  ⚠️ Длительное обучение: {model_info['train_time_sec']:.1f}s\")\n",
    "    \n",
    "    recommendations.append(\"\")\n",
    "    recommendations.append(\"\")\n",
    "\n",
    "# Общие выводы\n",
    "recommendations.append(\"=\"*80)\n",
    "recommendations.append(\"ОБЩИЕ ВЫВОДЫ\")\n",
    "recommendations.append(\"=\"*80)\n",
    "recommendations.append(\"\")\n",
    "\n",
    "# Лучший алгоритм\n",
    "algorithm_performance = experiments_df.groupby('algorithm')['roc_auc'].agg(['mean', 'max']).sort_values('mean', ascending=False)\n",
    "best_algo = algorithm_performance.index[0]\n",
    "recommendations.append(\"1. ЛУЧШИЙ АЛГОРИТМ:\")\n",
    "recommendations.append(\"-\" * 80)\n",
    "recommendations.append(f\"   {best_algo} показал лучшие средние результаты\")\n",
    "recommendations.append(f\"   Средний ROC-AUC: {algorithm_performance.loc[best_algo, 'mean']:.4f}\")\n",
    "recommendations.append(f\"   Максимальный ROC-AUC: {algorithm_performance.loc[best_algo, 'max']:.4f}\")\n",
    "recommendations.append(\"\")\n",
    "\n",
    "# Лучший метод балансировки\n",
    "balancing_performance = experiments_df.groupby('balancing_method')['roc_auc'].agg(['mean', 'max']).sort_values('mean', ascending=False)\n",
    "best_balancing = balancing_performance.index[0]\n",
    "recommendations.append(\"2. ЛУЧШИЙ МЕТОД БАЛАНСИРОВКИ:\")\n",
    "recommendations.append(\"-\" * 80)\n",
    "recommendations.append(f\"   {best_balancing}\")\n",
    "recommendations.append(f\"   Средний ROC-AUC: {balancing_performance.loc[best_balancing, 'mean']:.4f}\")\n",
    "recommendations.append(\"\")\n",
    "\n",
    "# Практические рекомендации\n",
    "recommendations.append(\"3. ПРАКТИЧЕСКИЕ РЕКОМЕНДАЦИИ ДЛЯ ВНЕДРЕНИЯ:\")\n",
    "recommendations.append(\"-\" * 80)\n",
    "recommendations.append(\"   • Использовать разные модели для разных сегментов клиентов\")\n",
    "recommendations.append(\"   • Регулярно мониторить PSI для контроля стабильности моделей\")\n",
    "recommendations.append(\"   • Применять optimal threshold для каждой модели (указан выше)\")\n",
    "recommendations.append(\"   • Использовать percentile analysis для таргетирования клиентов\")\n",
    "recommendations.append(\"   • Переобучать модели при PSI > 0.25 (признак drift)\")\n",
    "recommendations.append(\"   • Фокусироваться на топ-5 важных признаков для каждого сегмента\")\n",
    "recommendations.append(\"\")\n",
    "\n",
    "# Связь корреляции и importance\n",
    "recommendations.append(\"4. СВЯЗЬ МЕЖДУ КОРРЕЛЯЦИЕЙ И ВАЖНОСТЬЮ ПРИЗНАКОВ:\")\n",
    "recommendations.append(\"-\" * 80)\n",
    "recommendations.append(\"   • Модели используют как линейные, так и нелинейные зависимости\")\n",
    "recommendations.append(\"   • Feature importance более надежна, чем простая корреляция\")\n",
    "recommendations.append(\"   • Некоторые важные признаки имеют низкую линейную корреляцию\")\n",
    "recommendations.append(\"\")\n",
    "\n",
    "recommendations.append(\"=\"*80)\n",
    "recommendations.append(f\"Дата создания рекомендаций: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "recommendations.append(\"=\"*80)\n",
    "\n",
    "# Вывод на экран\n",
    "for line in recommendations:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n✓ Рекомендации сформированы\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "---\n",
    "# 11. СОХРАНЕНИЕ ИТОГОВЫХ ФАЙЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ РЕКОМЕНДАЦИЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ИТОГОВЫХ ФАЙЛОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сохраняем текстовый отчет\n",
    "recommendations_file = config.OUTPUT_DIR / 'final_recommendations.txt'\n",
    "with open(recommendations_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(recommendations))\n",
    "\n",
    "print(f\"\\n✓ Рекомендации сохранены: {recommendations_file}\")\n",
    "print(f\"  Размер файла: {recommendations_file.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ КРАТКОЙ ТАБЛИЦЫ ЛУЧШИХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "best_models_summary = []\n",
    "\n",
    "for seg_id, model_info in best_models_info.items():\n",
    "    best_models_summary.append({\n",
    "        'segment_group': seg_id,\n",
    "        'segment_name': config.SEGMENTS[seg_id]['name'],\n",
    "        'algorithm': model_info['algorithm'],\n",
    "        'balancing_method': model_info['balancing_method'],\n",
    "        'roc_auc': model_info['roc_auc'],\n",
    "        'gini': model_info['gini'],\n",
    "        'f1': model_info['f1'],\n",
    "        'precision': model_info['precision'],\n",
    "        'recall': model_info['recall'],\n",
    "        'threshold': model_info['threshold'],\n",
    "        'psi': model_info['psi'],\n",
    "        'psi_interpretation': model_info['psi_interpretation'],\n",
    "        'train_time_sec': model_info['train_time_sec'],\n",
    "        'model_file': model_info['model_file']\n",
    "    })\n",
    "\n",
    "best_models_df = pd.DataFrame(best_models_summary)\n",
    "\n",
    "summary_file = config.OUTPUT_DIR / 'best_models_summary.csv'\n",
    "best_models_df.to_csv(summary_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Краткая таблица лучших моделей сохранена: {summary_file}\")\n",
    "print(f\"  Размер файла: {summary_file.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12",
   "metadata": {},
   "source": [
    "---\n",
    "# 12. ФИНАЛЬНАЯ СВОДКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФИНАЛЬНАЯ СВОДКА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"✓✓✓ ФИНАЛЬНЫЙ АНАЛИЗ ЗАВЕРШЕН УСПЕШНО ✓✓✓\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nДата: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ПРОВЕДЕННЫЙ АНАЛИЗ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n✓ Итоговая таблица всех {len(experiments_df)} экспериментов\")\n",
    "print(f\"✓ Визуализация топ-10 моделей\")\n",
    "print(f\"✓ PSI анализ для {len(best_models_info)} лучших моделей\")\n",
    "print(f\"✓ Percentile анализ (10 deciles)\")\n",
    "print(f\"✓ Feature Importance анализ (топ-20 признаков)\")\n",
    "print(f\"✓ Correlation vs Importance сравнение\")\n",
    "print(f\"✓ ROC-AUC кривые для лучших моделей\")\n",
    "print(f\"✓ Финальные рекомендации для банка\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ЛУЧШИЕ МОДЕЛИ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for seg_id, model_info in best_models_info.items():\n",
    "    print(f\"\\n{seg_id}: {model_info['algorithm']} + {model_info['balancing_method']}\")\n",
    "    print(f\"  ROC-AUC: {model_info['roc_auc']:.4f} | PSI: {model_info['psi']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СОХРАНЕННЫЕ ФАЙЛЫ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nРЕЗУЛЬТАТЫ (output/):\")\n",
    "print(f\"  • final_recommendations.txt - полный отчет с рекомендациями\")\n",
    "print(f\"  • best_models_summary.csv - таблица лучших моделей\")\n",
    "print(f\"  • percentile_analysis_seg1.csv - decile анализ для Segment 1\")\n",
    "print(f\"  • percentile_analysis_seg2.csv - decile анализ для Segment 2\")\n",
    "\n",
    "print(f\"\\nВИЗУАЛИЗАЦИИ (figures/):\")\n",
    "print(f\"  • top10_models_roc_auc.png - топ-10 моделей\")\n",
    "print(f\"  • feature_importance_seg1.png - важность признаков Segment 1\")\n",
    "print(f\"  • feature_importance_seg2.png - важность признаков Segment 2\")\n",
    "print(f\"  • roc_curves_best_models.png - ROC кривые\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ГОТОВО К ВНЕДРЕНИЮ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\nСледующие шаги:\")\n",
    "print(\"  1. Ознакомиться с рекомендациями в output/final_recommendations.txt\")\n",
    "print(\"  2. Использовать лучшие модели из models/ для production\")\n",
    "print(\"  3. Настроить мониторинг PSI для контроля drift\")\n",
    "print(\"  4. Применить optimal thresholds из best_models_summary.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ АНАЛИЗ ЗАВЕРШЕН\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
