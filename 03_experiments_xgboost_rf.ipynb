{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ===============================================================================\n",
    "# –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´: XGBOOST & RANDOMFOREST –° –ú–ï–¢–û–î–ê–ú–ò –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò\n",
    "# ===============================================================================\n",
    "\n",
    "**–¶–µ–ª—å:** –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (—á–∞—Å—Ç—å 2)\n",
    "\n",
    "**–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**\n",
    "- **–ê–ª–≥–æ—Ä–∏—Ç–º—ã:** XGBoost, RandomForest\n",
    "- **–ú–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:** No balancing, Class weights, SMOTE, Undersampling, SMOTE+Undersampling\n",
    "- **–°–µ–≥–º–µ–Ω—Ç—ã:** Segment 1, Segment 2\n",
    "- **–í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:** 2 √ó 5 √ó 2 = **20**\n",
    "\n",
    "**–ú–µ—Ç—Ä–∏–∫–∏:** ROC-AUC, Gini, PR-AUC, Precision, Recall, F1, Confusion Matrix\n",
    "\n",
    "**–î–∞—Ç–∞:** 2025-01-13  \n",
    "**Random seed:** 42\n",
    "\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# –î–∞–Ω–Ω—ã–µ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve,\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´: XGBOOST & RANDOMFOREST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "print(f\"  –î–∞—Ç–∞ –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\"\"\"\n",
    "    \n",
    "    # –í–û–°–ü–†–û–ò–ó–í–û–î–ò–ú–û–°–¢–¨\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # –ü–£–¢–ò\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    MODELS_DIR = Path(\"models\")\n",
    "    \n",
    "    # –ö–û–õ–û–ù–ö–ò\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    \n",
    "    # –°–ï–ì–ú–ï–ù–¢–´\n",
    "    SEGMENTS = {\n",
    "        'Segment 1': {\n",
    "            'name': 'Small Business',\n",
    "            'train': 'seg1_train.parquet',\n",
    "            'val': 'seg1_val.parquet',\n",
    "            'test': 'seg1_test.parquet'\n",
    "        },\n",
    "        'Segment 2': {\n",
    "            'name': 'Middle + Large Business',\n",
    "            'train': 'seg2_train.parquet',\n",
    "            'val': 'seg2_val.parquet',\n",
    "            'test': 'seg2_test.parquet'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # –ê–õ–ì–û–†–ò–¢–ú–´\n",
    "    ALGORITHMS = ['XGBoost', 'RandomForest']\n",
    "    \n",
    "    # –ú–ï–¢–û–î–´ –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò\n",
    "    BALANCING_METHODS = [\n",
    "        'No balancing',\n",
    "        'Class weights',\n",
    "        'SMOTE',\n",
    "        'Random Undersampling',\n",
    "        'SMOTE + Undersampling'\n",
    "    ]\n",
    "    \n",
    "    # XGBOOST –ü–ê–†–ê–ú–ï–¢–†–´\n",
    "    XGBOOST_PARAMS = {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'early_stopping_rounds': 50,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # RANDOMFOREST –ü–ê–†–ê–ú–ï–¢–†–´\n",
    "    RANDOMFOREST_PARAMS = {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 100,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    # BALANCING –ü–ê–†–ê–ú–ï–¢–†–´\n",
    "    SMOTE_PARAMS = {\n",
    "        'random_state': 42,\n",
    "        'k_neighbors': 5\n",
    "    }\n",
    "    \n",
    "    UNDERSAMPLING_PARAMS = {\n",
    "        'random_state': 42,\n",
    "        'sampling_strategy': 'auto'\n",
    "    }\n",
    "    \n",
    "    # THRESHOLD TUNING\n",
    "    THRESHOLD_METRIC = 'f1'\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.MODELS_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  –ê–ª–≥–æ—Ä–∏—Ç–º—ã: {', '.join(config.ALGORITHMS)}\")\n",
    "print(f\"  –ú–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(config.BALANCING_METHODS)}\")\n",
    "print(f\"  –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(config.SEGMENTS)}\")\n",
    "print(f\"  –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(config.ALGORITHMS) * len(config.BALANCING_METHODS) * len(config.SEGMENTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–ó–ê–ì–†–£–ó–ö–ê –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for seg_id, seg_info in config.SEGMENTS.items():\n",
    "    print(f\"\\n{seg_id}: {seg_info['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    data[seg_id] = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = config.OUTPUT_DIR / seg_info[split]\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}\\n\"\n",
    "                f\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ notebook 01_data_preparation_eda.ipynb\"\n",
    "            )\n",
    "        \n",
    "        df = pd.read_parquet(file_path)\n",
    "        data[seg_id][split] = df\n",
    "        \n",
    "        churn_rate = df[config.TARGET_COLUMN].mean()\n",
    "        print(f\"  {split.upper():5s}: {df.shape} | Churn: {churn_rate*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-previous-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –ó–ê–ì–†–£–ó–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–†–ï–î–´–î–£–©–ï–ì–û –≠–¢–ê–ü–ê\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–ó–ê–ì–†–£–ó–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–†–ï–î–´–î–£–©–ï–ì–û –≠–¢–ê–ü–ê\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "previous_results_file = config.OUTPUT_DIR / 'experiments_part1.csv'\n",
    "\n",
    "if not previous_results_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {previous_results_file}\\n\"\n",
    "        f\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ notebook 02_experiments_catboost_lightgbm.ipynb\"\n",
    "    )\n",
    "\n",
    "previous_results_df = pd.read_csv(previous_results_file)\n",
    "\n",
    "print(f\"\\n‚úì –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞\")\n",
    "print(f\"  –§–∞–π–ª: {previous_results_file}\")\n",
    "print(f\"  –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(previous_results_df)}\")\n",
    "print(f\"  –ê–ª–≥–æ—Ä–∏—Ç–º—ã: {', '.join(previous_results_df['algorithm'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò\n",
    "# ====================================================================================\n",
    "\n",
    "def prepare_data(df, target_col):\n",
    "    \"\"\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ X –∏ y\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def apply_balancing(X_train, y_train, method):\n",
    "    \"\"\"\n",
    "    –ü—Ä–∏–º–µ–Ω–∏—Ç—å –º–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫ train –¥–∞–Ω–Ω—ã–º.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_balanced, y_balanced, sample_weights (or None)\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'No balancing':\n",
    "        return X_train.copy(), y_train.copy(), None\n",
    "    \n",
    "    elif method == 'Class weights':\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        sample_weights = np.array([class_weights[int(y)] for y in y_train])\n",
    "        return X_train.copy(), y_train.copy(), sample_weights\n",
    "    \n",
    "    elif method == 'SMOTE':\n",
    "        smote = SMOTE(**config.SMOTE_PARAMS)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    elif method == 'Random Undersampling':\n",
    "        rus = RandomUnderSampler(**config.UNDERSAMPLING_PARAMS)\n",
    "        X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    elif method == 'SMOTE + Undersampling':\n",
    "        # –°–Ω–∞—á–∞–ª–∞ SMOTE, –ø–æ—Ç–æ–º undersampling\n",
    "        smote = SMOTE(**config.SMOTE_PARAMS)\n",
    "        X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        rus = RandomUnderSampler(**config.UNDERSAMPLING_PARAMS)\n",
    "        X_res, y_res = rus.fit_resample(X_smote, y_smote)\n",
    "        \n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown balancing method: {method}\")\n",
    "\n",
    "\n",
    "def train_model(algorithm, X_train, y_train, X_val, y_val, sample_weights=None):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    trained_model\n",
    "    \"\"\"\n",
    "    \n",
    "    if algorithm == 'XGBoost':\n",
    "        model = XGBClassifier(**config.XGBOOST_PARAMS)\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "    elif algorithm == 'RandomForest':\n",
    "        model = RandomForestClassifier(**config.RANDOMFOREST_PARAMS)\n",
    "        \n",
    "        # RandomForest –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç eval_set, –æ–±—É—á–∞–µ–º –±–µ–∑ early stopping\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    \"\"\"\n",
    "    –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_threshold, best_score\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred, zero_division=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    return thresholds[optimal_idx], scores[optimal_idx]\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, y_pred, threshold):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'] = cm[0, 0]\n",
    "    metrics['fp'] = cm[0, 1]\n",
    "    metrics['fn'] = cm[1, 0]\n",
    "    metrics['tp'] = cm[1, 1]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"‚úì –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í\n",
    "\n",
    "–¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏:  \n",
    "**2 –∞–ª–≥–æ—Ä–∏—Ç–º–∞ √ó 5 –º–µ—Ç–æ–¥–æ–≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ √ó 2 —Å–µ–≥–º–µ–Ω—Ç–∞ = 20 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –ó–ê–ü–£–°–ö –í–°–ï–• –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "models_storage = {}  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "total_experiments = len(config.ALGORITHMS) * len(config.BALANCING_METHODS) * len(config.SEGMENTS)\n",
    "\n",
    "print(f\"\\n–í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {total_experiments}\")\n",
    "print(f\"–ù–∞—á–∞–ª–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º progress bar\n",
    "pbar = tqdm(total=total_experiments, desc=\"–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å\", position=0)\n",
    "\n",
    "experiment_num = 0\n",
    "\n",
    "# –¶–∏–∫–ª –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º\n",
    "for seg_id, seg_info in config.SEGMENTS.items():\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞\n",
    "    X_train, y_train = prepare_data(data[seg_id]['train'], config.TARGET_COLUMN)\n",
    "    X_val, y_val = prepare_data(data[seg_id]['val'], config.TARGET_COLUMN)\n",
    "    X_test, y_test = prepare_data(data[seg_id]['test'], config.TARGET_COLUMN)\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞\n",
    "    models_storage[seg_id] = {}\n",
    "    \n",
    "    # –¶–∏–∫–ª –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º\n",
    "    for algorithm in config.ALGORITHMS:\n",
    "        \n",
    "        models_storage[seg_id][algorithm] = {\n",
    "            'best_model': None,\n",
    "            'best_roc_auc': 0,\n",
    "            'best_method': None\n",
    "        }\n",
    "        \n",
    "        # –¶–∏–∫–ª –ø–æ –º–µ—Ç–æ–¥–∞–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏\n",
    "        for balancing_method in config.BALANCING_METHODS:\n",
    "            \n",
    "            experiment_num += 1\n",
    "            \n",
    "            try:\n",
    "                # ===== 1. –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê =====\n",
    "                X_train_balanced, y_train_balanced, sample_weights = apply_balancing(\n",
    "                    X_train, y_train, balancing_method\n",
    "                )\n",
    "                \n",
    "                # ===== 2. –û–ë–£–ß–ï–ù–ò–ï =====\n",
    "                start_time = time.time()\n",
    "                \n",
    "                model = train_model(\n",
    "                    algorithm,\n",
    "                    X_train_balanced, y_train_balanced,\n",
    "                    X_val, y_val,\n",
    "                    sample_weights\n",
    "                )\n",
    "                \n",
    "                train_time = time.time() - start_time\n",
    "                \n",
    "                # ===== 3. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø =====\n",
    "                y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # ===== 4. OPTIMAL THRESHOLD =====\n",
    "                optimal_threshold, _ = find_optimal_threshold(\n",
    "                    y_val, y_val_proba, config.THRESHOLD_METRIC\n",
    "                )\n",
    "                \n",
    "                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å optimal threshold –Ω–∞ test\n",
    "                y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n",
    "                \n",
    "                # ===== 5. –ú–ï–¢–†–ò–ö–ò –ù–ê TEST =====\n",
    "                metrics = calculate_metrics(\n",
    "                    y_test, y_test_proba, y_test_pred, optimal_threshold\n",
    "                )\n",
    "                \n",
    "                # ===== 6. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í =====\n",
    "                result = {\n",
    "                    'segment_group': seg_id,\n",
    "                    'segment_name': seg_info['name'],\n",
    "                    'algorithm': algorithm,\n",
    "                    'balancing_method': balancing_method,\n",
    "                    'train_samples': len(X_train_balanced),\n",
    "                    'train_time_sec': train_time,\n",
    "                    **metrics\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # ===== 7. –û–ë–ù–û–í–õ–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò =====\n",
    "                if metrics['roc_auc'] > models_storage[seg_id][algorithm]['best_roc_auc']:\n",
    "                    models_storage[seg_id][algorithm]['best_model'] = model\n",
    "                    models_storage[seg_id][algorithm]['best_roc_auc'] = metrics['roc_auc']\n",
    "                    models_storage[seg_id][algorithm]['best_method'] = balancing_method\n",
    "                \n",
    "                # ===== 8. –í–´–í–û–î –ü–†–û–ì–†–ï–°–°–ê =====\n",
    "                pbar.set_postfix_str(\n",
    "                    f\"{seg_id} | {algorithm} | {balancing_method[:15]:15s} | \"\n",
    "                    f\"ROC-AUC: {metrics['roc_auc']:.4f} | Time: {train_time:.1f}s\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏\n",
    "                del X_train_balanced, y_train_balanced, sample_weights\n",
    "                if balancing_method != 'Class weights' and balancing_method != 'No balancing':\n",
    "                    gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {experiment_num}: {seg_id} | {algorithm} | {balancing_method}\")\n",
    "                print(f\"   –û—à–∏–±–∫–∞: {str(e)}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã\")\n",
    "print(f\"  –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"  –£—Å–ø–µ—à–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(results)}/{total_experiments}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –í–°–ï–• –≠–¢–ê–ü–û–í\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame —Å –Ω–æ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "new_results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n–ù–æ–≤—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (XGBoost, RandomForest): {len(new_results_df)}\")\n",
    "print(f\"–ü—Ä–µ–¥—ã–¥—É—â–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (CatBoost, LightGBM): {len(previous_results_df)}\")\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "all_results_df = pd.concat([previous_results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ROC-AUC\n",
    "all_results_df = all_results_df.sort_values('roc_auc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã\")\n",
    "print(f\"  –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(all_results_df)}\")\n",
    "print(f\"  –ê–ª–≥–æ—Ä–∏—Ç–º—ã: {', '.join(all_results_df['algorithm'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –ê–ù–ê–õ–ò–ó –í–°–ï–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–ê–ù–ê–õ–ò–ó –í–°–ï–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n–í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_results_df)}\")\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "print(f\"\\n–û–°–ù–û–í–ù–´–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò:\")\n",
    "print(f\"  ROC-AUC: min={all_results_df['roc_auc'].min():.4f}, \"\n",
    "      f\"max={all_results_df['roc_auc'].max():.4f}, \"\n",
    "      f\"mean={all_results_df['roc_auc'].mean():.4f}\")\n",
    "print(f\"  Gini: min={all_results_df['gini'].min():.4f}, \"\n",
    "      f\"max={all_results_df['gini'].max():.4f}, \"\n",
    "      f\"mean={all_results_df['gini'].mean():.4f}\")\n",
    "print(f\"  F1: min={all_results_df['f1'].min():.4f}, \"\n",
    "      f\"max={all_results_df['f1'].max():.4f}, \"\n",
    "      f\"mean={all_results_df['f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –¢–û–ü-10 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–û –°–ï–ì–ú–ï–ù–¢–ê–ú\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–¢–û–ü-10 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ü–û ROC-AUC (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id in config.SEGMENTS.keys():\n",
    "    seg_results = all_results_df[all_results_df['segment_group'] == seg_id].head(10)\n",
    "    \n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    display_cols = ['algorithm', 'balancing_method', 'roc_auc', 'gini', 'f1', \n",
    "                   'precision', 'recall', 'train_time_sec']\n",
    "    \n",
    "    print(seg_results[display_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "algorithm-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –°–†–ê–í–ù–ï–ù–ò–ï –ê–õ–ì–û–†–ò–¢–ú–û–í\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–°–†–ê–í–ù–ï–ù–ò–ï –ê–õ–ì–û–†–ò–¢–ú–û–í (—Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "algorithm_stats = all_results_df.groupby('algorithm').agg({\n",
    "    'roc_auc': ['mean', 'std', 'max'],\n",
    "    'gini': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'train_time_sec': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "algorithm_stats = algorithm_stats.sort_values(('roc_auc', 'mean'), ascending=False)\n",
    "\n",
    "print(\"\\n–°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º:\")\n",
    "print(algorithm_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-by-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ê–õ–ì–û–†–ò–¢–ú–ê–ú\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ê–õ–ì–û–†–ò–¢–ú–ê–ú\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for algorithm in sorted(all_results_df['algorithm'].unique()):\n",
    "    print(f\"\\n{algorithm}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    algo_results = all_results_df[all_results_df['algorithm'] == algorithm]\n",
    "    \n",
    "    for seg_id in config.SEGMENTS.keys():\n",
    "        seg_algo = algo_results[algo_results['segment_group'] == seg_id]\n",
    "        \n",
    "        if len(seg_algo) > 0:\n",
    "            best = seg_algo.iloc[0]\n",
    "            print(f\"  {seg_id}: {best['balancing_method']:25s} | \"\n",
    "                  f\"ROC-AUC: {best['roc_auc']:.4f} | \"\n",
    "                  f\"Gini: {best['gini']:.4f} | \"\n",
    "                  f\"F1: {best['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –ú–û–î–ï–õ–ï–ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-all-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –°–û–•–†–ê–ù–ï–ù–ò–ï –í–°–ï–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "all_results_file = config.OUTPUT_DIR / 'experiments_all.csv'\n",
    "all_results_df.to_csv(all_results_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úì –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {all_results_file}\")\n",
    "print(f\"  –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(all_results_df)}\")\n",
    "print(f\"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {all_results_file.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ò–• –ú–û–î–ï–õ–ï–ô (XGBOOST & RANDOMFOREST)\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ò–• –ú–û–î–ï–õ–ï–ô\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "for seg_id in config.SEGMENTS.keys():\n",
    "    seg_num = seg_id.split()[1]  # '1' or '2'\n",
    "    \n",
    "    for algorithm in config.ALGORITHMS:\n",
    "        \n",
    "        model_info = models_storage[seg_id][algorithm]\n",
    "        \n",
    "        if model_info['best_model'] is not None:\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "            algo_name = algorithm.lower().replace(' ', '_')\n",
    "            model_filename = f\"best_{algo_name}_seg{seg_num}.pkl\"\n",
    "            model_path = config.MODELS_DIR / model_filename\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model_info['best_model'], f)\n",
    "            \n",
    "            saved_models.append({\n",
    "                'file': model_filename,\n",
    "                'segment': seg_id,\n",
    "                'algorithm': algorithm,\n",
    "                'balancing_method': model_info['best_method'],\n",
    "                'roc_auc': model_info['best_roc_auc']\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n‚úì {model_filename}\")\n",
    "            print(f\"  –°–µ–≥–º–µ–Ω—Ç: {seg_id}\")\n",
    "            print(f\"  –ê–ª–≥–æ—Ä–∏—Ç–º: {algorithm}\")\n",
    "            print(f\"  –ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {model_info['best_method']}\")\n",
    "            print(f\"  ROC-AUC (test): {model_info['best_roc_auc']:.4f}\")\n",
    "            print(f\"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(saved_models)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–ï–¢–ê–î–ê–ù–ù–´–•\n",
    "# ====================================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'random_seed': config.RANDOM_SEED,\n",
    "    'total_experiments': len(all_results_df),\n",
    "    'part1_experiments': len(previous_results_df),\n",
    "    'part2_experiments': len(new_results_df),\n",
    "    'algorithms': sorted(all_results_df['algorithm'].unique().tolist()),\n",
    "    'balancing_methods': config.BALANCING_METHODS,\n",
    "    'segments': list(config.SEGMENTS.keys()),\n",
    "    'xgboost_params': config.XGBOOST_PARAMS,\n",
    "    'randomforest_params': config.RANDOMFOREST_PARAMS,\n",
    "    'saved_models_part2': saved_models,\n",
    "    'best_overall': {\n",
    "        'segment': all_results_df.iloc[0]['segment_group'],\n",
    "        'algorithm': all_results_df.iloc[0]['algorithm'],\n",
    "        'balancing_method': all_results_df.iloc[0]['balancing_method'],\n",
    "        'roc_auc': float(all_results_df.iloc[0]['roc_auc']),\n",
    "        'gini': float(all_results_df.iloc[0]['gini']),\n",
    "        'f1': float(all_results_df.iloc[0]['f1'])\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = config.OUTPUT_DIR / 'experiments_all_metadata.json'\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"‚úì‚úì‚úì –í–°–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û ‚úì‚úì‚úì\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Random seed: {config.RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–°–ï–• –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n–í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(all_results_df)}\")\n",
    "print(f\"  ‚Ä¢ –ß–∞—Å—Ç—å 1 (CatBoost, LightGBM): {len(previous_results_df)}\")\n",
    "print(f\"  ‚Ä¢ –ß–∞—Å—Ç—å 2 (XGBoost, RandomForest): {len(new_results_df)}\")\n",
    "print(f\"\\n–ê–ª–≥–æ—Ä–∏—Ç–º–æ–≤: {len(all_results_df['algorithm'].unique())}\")\n",
    "print(f\"  {', '.join(sorted(all_results_df['algorithm'].unique()))}\")\n",
    "print(f\"–ú–µ—Ç–æ–¥–æ–≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(config.BALANCING_METHODS)}\")\n",
    "print(f\"–°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(config.SEGMENTS)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"–õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ (–í–°–ï –ê–õ–ì–û–†–ò–¢–ú–´)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "best_overall = all_results_df.iloc[0]\n",
    "print(f\"\\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–ø–æ ROC-AUC):\")\n",
    "print(f\"  –°–µ–≥–º–µ–Ω—Ç: {best_overall['segment_group']} ({best_overall['segment_name']})\")\n",
    "print(f\"  –ê–ª–≥–æ—Ä–∏—Ç–º: {best_overall['algorithm']}\")\n",
    "print(f\"  –ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {best_overall['balancing_method']}\")\n",
    "print(f\"  ROC-AUC: {best_overall['roc_auc']:.4f}\")\n",
    "print(f\"  Gini: {best_overall['gini']:.4f}\")\n",
    "print(f\"  F1-Score: {best_overall['f1']:.4f}\")\n",
    "print(f\"  Precision: {best_overall['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_overall['recall']:.4f}\")\n",
    "\n",
    "# –¢–û–ü-3 –ø–æ –∫–∞–∂–¥–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"–¢–û–ü-3 –ü–û –°–ï–ì–ú–ï–ù–¢–ê–ú\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for seg_id in config.SEGMENTS.keys():\n",
    "    print(f\"\\n{seg_id} ({config.SEGMENTS[seg_id]['name']}):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    seg_top = all_results_df[all_results_df['segment_group'] == seg_id].head(3)\n",
    "    \n",
    "    for idx, row in seg_top.iterrows():\n",
    "        rank = list(seg_top.index).index(idx) + 1\n",
    "        print(f\"  {rank}. {row['algorithm']:15s} | {row['balancing_method']:25s} | \"\n",
    "              f\"ROC-AUC: {row['roc_auc']:.4f} | F1: {row['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"–°–û–•–†–ê–ù–ï–ù–ù–´–ï –§–ê–ô–õ–´\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n–†–ï–ó–£–õ–¨–¢–ê–¢–´ (output/):\")\n",
    "print(f\"  ‚Ä¢ experiments_all.csv - —Ç–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö {len(all_results_df)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\")\n",
    "print(f\"  ‚Ä¢ experiments_all_metadata.json - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\")\n",
    "\n",
    "print(f\"\\n–ú–û–î–ï–õ–ò (models/):\")\n",
    "print(f\"  XGBoost & RandomForest:\")\n",
    "for model in saved_models:\n",
    "    print(f\"    ‚Ä¢ {model['file']} - {model['segment']} | {model['algorithm']} | \"\n",
    "          f\"{model['balancing_method']} | ROC-AUC: {model['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úì –í–°–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ó–ê–í–ï–†–®–ï–ù–´\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
