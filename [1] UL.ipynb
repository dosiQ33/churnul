{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12386d2-f0a2-4773-9f95-4cf99515c5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"✓ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c8a8f0-2421-4def-926c-f81938cc7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration initialized\n",
      "  - Train data: data\\churn_train_ul.csv\n",
      "  - Prod data: data\\churn_prod_ul.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Class\n",
    "class ChurnConfig:\n",
    "    \"\"\"Centralized configuration for churn model preprocessing\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    LOG_DIR = Path(\"logs\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # Data files\n",
    "    TRAIN_INPUT_FILE = \"churn_train_ul.csv\"\n",
    "    PROD_INPUT_FILE = \"churn_prod_ul.csv\"\n",
    "    \n",
    "    OUTPUT_FILE = \"churn_data_preprocessed.csv\"\n",
    "    TRAIN_OUTPUT_FILE = \"train_processed.csv\"\n",
    "    TEST_OUTPUT_FILE = \"test_processed.csv\"\n",
    "    VAL_OUTPUT_FILE = \"val_processed.csv\"\n",
    "    METADATA_FILE = \"preprocessing_metadata.json\"\n",
    "    \n",
    "    # Data loading\n",
    "    DELIMITER = '|'\n",
    "    ENCODING = 'windows-1251'\n",
    "    \n",
    "    # Preprocessing parameters\n",
    "    CORRELATION_THRESHOLD = 0.85\n",
    "    OUTLIER_IQR_MULTIPLIER = 1.5\n",
    "    REMOVE_GAPS = True\n",
    "    HANDLE_OUTLIERS = True\n",
    "    REMOVE_HIGH_CORRELATIONS = True\n",
    "    \n",
    "    # Train/Val/Test split - TEMPORAL SPLIT\n",
    "    TRAIN_SIZE = 0.70\n",
    "    VAL_SIZE = 0.15\n",
    "    TEST_SIZE = 0.15\n",
    "    \n",
    "    # Model parameters\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # Columns to exclude from preprocessing\n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    \n",
    "    # CatBoost categorical features\n",
    "    CATEGORICAL_FEATURES = [\n",
    "        'segment_group',\n",
    "        'obs_month',\n",
    "        'obs_quarter'\n",
    "    ]\n",
    "    \n",
    "    # Feature groups\n",
    "    FINANCIAL_KEYWORDS = ['profit', 'income', 'expense', 'margin', 'provision']\n",
    "    BALANCE_KEYWORDS = ['balance', 'assets', 'liabilities']\n",
    "    ACTIVITY_KEYWORDS = ['activity', 'active', 'months_with']\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        for dir_path in [cls.DATA_DIR, cls.OUTPUT_DIR, cls.LOG_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_train_input_path(cls):\n",
    "        return cls.DATA_DIR / cls.TRAIN_INPUT_FILE\n",
    "    \n",
    "    @classmethod\n",
    "    def get_prod_input_path(cls):\n",
    "        return cls.DATA_DIR / cls.PROD_INPUT_FILE\n",
    "\n",
    "# Initialize configuration\n",
    "config = ChurnConfig()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Configuration initialized\")\n",
    "print(f\"  - Train data: {config.get_train_input_path()}\")\n",
    "print(f\"  - Prod data: {config.get_prod_input_path()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a950c8a-d5e4-49a6-86ad-2e177de5548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - ============================================================\n",
      "INFO - CHURN MODEL PREPROCESSING PIPELINE STARTED\n",
      "INFO - ============================================================\n",
      "✓ Logging configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Logging Setup\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    log_file = config.LOG_DIR / f\"preprocessing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    \n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    # Setup root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers = []\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"CHURN MODEL PREPROCESSING PIPELINE STARTED\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"✓ Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460b39b0-1ec0-4e76-9291-6cfa1013f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoader class defined with memory optimization\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: DataLoader with Memory Optimization\n",
    "class DataLoader:\n",
    "    \"\"\"Handle data loading with validation and memory optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, delimiter='|', encoding='utf-8'):\n",
    "        self.file_path = Path(file_path)\n",
    "        self.delimiter = delimiter\n",
    "        self.encoding = encoding\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def load(self, optimize_memory=True):\n",
    "        \"\"\"Load data with memory optimization\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Loading data from: {self.file_path}\")\n",
    "            \n",
    "            if not self.file_path.exists():\n",
    "                raise FileNotFoundError(f\"Data file not found: {self.file_path}\")\n",
    "            \n",
    "            # Load with optimized dtypes\n",
    "            df = pd.read_csv(\n",
    "                self.file_path,\n",
    "                delimiter=self.delimiter,\n",
    "                encoding=self.encoding,\n",
    "                thousands=',',\n",
    "                low_memory=False\n",
    "            )\n",
    "            \n",
    "            memory_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "            self.logger.info(f\"  Memory before optimization: {memory_before:.2f} MB\")\n",
    "            \n",
    "            # Standardize column names\n",
    "            df.columns = df.columns.str.lower().str.strip()\n",
    "            \n",
    "            # OPTIMIZE MEMORY\n",
    "            if optimize_memory:\n",
    "                df = self._optimize_dtypes(df)\n",
    "                memory_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "                savings = (1 - memory_after/memory_before) * 100\n",
    "                self.logger.info(f\"  Memory after optimization: {memory_after:.2f} MB ({savings:.1f}% saved)\")\n",
    "            \n",
    "            self.logger.info(f\"✓ Data loaded: {df.shape}\")\n",
    "            self._validate_data(df)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"✗ Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _optimize_dtypes(self, df):\n",
    "        \"\"\"Optimize DataFrame dtypes to reduce memory\"\"\"\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            \n",
    "            # Skip ID columns, target, and categorical features\n",
    "            if col in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES:\n",
    "                continue\n",
    "            \n",
    "            # Optimize numeric columns\n",
    "            if col_type != 'object':\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                \n",
    "                # Integer types\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                \n",
    "                # Float types\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _validate_data(self, df):\n",
    "        \"\"\"Validate loaded data\"\"\"\n",
    "        missing_id_cols = [col for col in config.ID_COLUMNS if col not in df.columns]\n",
    "        if missing_id_cols:\n",
    "            self.logger.warning(f\"⚠ Missing ID columns: {missing_id_cols}\")\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"Loaded dataframe is empty\")\n",
    "        \n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            unique_targets = df[config.TARGET_COLUMN].unique()\n",
    "            if not set(unique_targets).issubset({0, 1, np.nan}):\n",
    "                self.logger.warning(f\"⚠ Unexpected target values: {unique_targets}\")\n",
    "            self.logger.info(\"✓ Data validation passed (with target)\")\n",
    "        else:\n",
    "            self.logger.info(\"✓ Data validation passed (no target - production data)\")\n",
    "\n",
    "print(\"✓ DataLoader class defined with memory optimization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca5577c-1954-4750-b89f-5a544f61d572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - LOADING TRAINING DATA\n",
      "INFO - ============================================================\n",
      "INFO - Loading data from: data\\churn_train_ul.csv\n",
      "INFO -   Memory before optimization: 5194.33 MB\n",
      "INFO -   Memory after optimization: 2286.35 MB (56.0% saved)\n",
      "INFO - ✓ Data loaded: (3243871, 195)\n",
      "INFO - ✓ Data validation passed (with target)\n",
      "\n",
      "✓ Training data loaded: (3243871, 195)\n",
      "  Columns: ['cli_code', 'client_id', 'observation_point', 'target_churn_3m', 'segment_group', 'obs_months_count', 'avg_activity_6m', 'active_months_6m', 'avg_products_6m', 'max_products_6m']...\n",
      "  Target distribution:\n",
      "target_churn_3m\n",
      "0    3195163\n",
      "1      48708\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Training Data\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"LOADING TRAINING DATA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "loader = DataLoader(config.get_train_input_path(), delimiter=config.DELIMITER)\n",
    "train_full_df = loader.load(optimize_memory=True)\n",
    "\n",
    "print(f\"\\n✓ Training data loaded: {train_full_df.shape}\")\n",
    "print(f\"  Columns: {list(train_full_df.columns[:10])}...\")\n",
    "print(f\"  Target distribution:\\n{train_full_df[config.TARGET_COLUMN].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fd3545-81d0-440c-a414-d0c8ddd85017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - ============================================================\n",
      "INFO - DATA QUALITY ANALYSIS\n",
      "INFO - ============================================================\n",
      "INFO - Shape: (3243871, 195), Memory: 2286.35 MB\n",
      "INFO - ✓ No missing values\n",
      "INFO - ℹ 9 constant columns\n",
      "INFO - ⚠ 7 near-constant columns\n",
      "INFO - ✓ No duplicates\n",
      "INFO - Target: Churn rate = 0.0150\n",
      "INFO - Period: 2023-06-30 to 2025-06-30\n",
      "\n",
      "✓ Data quality analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Data Quality Analysis Class & Execution\n",
    "class DataQualityAnalyzer:\n",
    "    \"\"\"Analyze data quality and generate reports\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.quality_report = {}\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Run comprehensive data quality analysis\"\"\"\n",
    "        self.logger.info(\"=\"*60)\n",
    "        self.logger.info(\"DATA QUALITY ANALYSIS\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "        \n",
    "        self._basic_info()\n",
    "        self._check_missing_values()\n",
    "        self._check_constant_columns()\n",
    "        self._check_duplicates()\n",
    "        self._analyze_target_distribution()\n",
    "        self._analyze_temporal_distribution()\n",
    "        \n",
    "        return self.quality_report\n",
    "    \n",
    "    def _basic_info(self):\n",
    "        info = {\n",
    "            'shape': self.df.shape,\n",
    "            'n_rows': len(self.df),\n",
    "            'n_columns': len(self.df.columns),\n",
    "            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'n_numeric': len(self.df.select_dtypes(include=[np.number]).columns),\n",
    "            'n_categorical': len(self.df.select_dtypes(include=['object']).columns)\n",
    "        }\n",
    "        self.quality_report['basic_info'] = info\n",
    "        self.logger.info(f\"Shape: {info['shape']}, Memory: {info['memory_usage_mb']:.2f} MB\")\n",
    "    \n",
    "    def _check_missing_values(self):\n",
    "        missing = self.df.isnull().sum()\n",
    "        missing_pct = (missing / len(self.df) * 100).round(2)\n",
    "        missing_df = pd.DataFrame({\n",
    "            'column': missing.index,\n",
    "            'missing_count': missing.values,\n",
    "            'missing_pct': missing_pct.values\n",
    "        })\n",
    "        missing_df = missing_df[missing_df['missing_count'] > 0].sort_values('missing_pct', ascending=False)\n",
    "        self.quality_report['missing_values'] = missing_df\n",
    "        \n",
    "        if len(missing_df) > 0:\n",
    "            self.logger.info(f\"⚠ {len(missing_df)} columns with missing values\")\n",
    "        else:\n",
    "            self.logger.info(\"✓ No missing values\")\n",
    "    \n",
    "    def _check_constant_columns(self):\n",
    "        constant_cols = []\n",
    "        near_constant_cols = []\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            n_unique = self.df[col].nunique(dropna=False)\n",
    "            if n_unique == 1:\n",
    "                constant_cols.append(col)\n",
    "            elif n_unique == 2 and self.df[col].value_counts(normalize=True).iloc[0] > 0.99:\n",
    "                near_constant_cols.append(col)\n",
    "        \n",
    "        self.quality_report['constant_columns'] = constant_cols\n",
    "        self.quality_report['near_constant_columns'] = near_constant_cols\n",
    "        \n",
    "        if constant_cols:\n",
    "            self.logger.info(f\"ℹ {len(constant_cols)} constant columns\")\n",
    "        if near_constant_cols:\n",
    "            self.logger.info(f\"⚠ {len(near_constant_cols)} near-constant columns\")\n",
    "    \n",
    "    def _check_duplicates(self):\n",
    "        n_duplicates = self.df.duplicated().sum()\n",
    "        self.quality_report['duplicates'] = {'count': n_duplicates}\n",
    "        \n",
    "        if n_duplicates > 0:\n",
    "            self.logger.warning(f\"⚠ {n_duplicates} duplicate rows\")\n",
    "        else:\n",
    "            self.logger.info(\"✓ No duplicates\")\n",
    "    \n",
    "    def _analyze_target_distribution(self):\n",
    "        if config.TARGET_COLUMN not in self.df.columns:\n",
    "            self.logger.info(\"ℹ No target column (production data)\")\n",
    "            self.quality_report['target_distribution'] = None\n",
    "            return\n",
    "        \n",
    "        target_dist = self.df[config.TARGET_COLUMN].value_counts()\n",
    "        churn_rate = self.df[config.TARGET_COLUMN].mean()\n",
    "        \n",
    "        self.quality_report['target_distribution'] = {\n",
    "            'churn_rate': churn_rate,\n",
    "            'n_churned': int(target_dist.get(1, 0)),\n",
    "            'n_not_churned': int(target_dist.get(0, 0))\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"Target: Churn rate = {churn_rate:.4f}\")\n",
    "    \n",
    "    def _analyze_temporal_distribution(self):\n",
    "        if 'observation_point' in self.df.columns:\n",
    "            self.df['observation_point'] = pd.to_datetime(self.df['observation_point'])\n",
    "            temporal_info = {\n",
    "                'min_date': self.df['observation_point'].min(),\n",
    "                'max_date': self.df['observation_point'].max(),\n",
    "                'n_unique_dates': self.df['observation_point'].nunique(),\n",
    "                'n_unique_clients': self.df['cli_code'].nunique()\n",
    "            }\n",
    "            self.quality_report['temporal_info'] = temporal_info\n",
    "            self.logger.info(f\"Period: {temporal_info['min_date'].date()} to {temporal_info['max_date'].date()}\")\n",
    "\n",
    "# Run quality analysis\n",
    "quality_analyzer = DataQualityAnalyzer(train_full_df)\n",
    "quality_report = quality_analyzer.analyze()\n",
    "\n",
    "print(\"\\n✓ Data quality analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537e9fa8-55ee-4d00-beea-c54e6c0cb442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - CREATING TEMPORAL TRAIN/VAL/TEST SPLIT\n",
      "INFO - ============================================================\n",
      "INFO - ============================================================\n",
      "INFO - TEMPORAL TRAIN/VALIDATION/TEST SPLIT\n",
      "INFO - ============================================================\n",
      "INFO - Total unique dates: 25\n",
      "INFO - Date range: 2023-06-30 to 2025-06-30\n",
      "INFO - \n",
      "Split cutoff dates:\n",
      "INFO -   Train: up to 2024-10-31 (17 dates)\n",
      "INFO -   Val: 2024-11-30 to 2025-02-28 (4 dates)\n",
      "INFO -   Test: 2025-03-31 onwards (4 dates)\n",
      "INFO - \n",
      "TRAIN:\n",
      "INFO -   Records: 2162862\n",
      "INFO -   Clients: 173448\n",
      "INFO -   Dates: 2023-06-30 to 2024-10-31\n",
      "INFO - \n",
      "VAL:\n",
      "INFO -   Records: 535263\n",
      "INFO -   Clients: 144022\n",
      "INFO -   Dates: 2024-11-30 to 2025-02-28\n",
      "INFO - \n",
      "TEST:\n",
      "INFO -   Records: 545746\n",
      "INFO -   Clients: 146264\n",
      "INFO -   Dates: 2025-03-31 to 2025-06-30\n",
      "INFO - \n",
      "✓ Temporal ordering verified - no data leakage\n",
      "\n",
      "✓ Temporal split complete\n",
      "  Train: (2162862, 195)\n",
      "  Val: (535263, 195)\n",
      "  Test: (545746, 195)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: TEMPORAL SPLIT (Key Change - by time, not random!)\n",
    "class TemporalSplitter:\n",
    "    \"\"\"Split data by TIME to prevent data leakage\"\"\"\n",
    "    \n",
    "    def __init__(self, df, date_col='observation_point'):\n",
    "        self.df = df\n",
    "        self.date_col = date_col\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def split(self, train_size=0.70, val_size=0.15, test_size=0.15):\n",
    "        \"\"\"Split data temporally based on observation_point\"\"\"\n",
    "        self.logger.info(\"=\"*60)\n",
    "        self.logger.info(\"TEMPORAL TRAIN/VALIDATION/TEST SPLIT\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "        \n",
    "        # Ensure date column is datetime\n",
    "        if self.df[self.date_col].dtype != 'datetime64[ns]':\n",
    "            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col])\n",
    "        \n",
    "        # Sort by date\n",
    "        df_sorted = self.df.sort_values(self.date_col).reset_index(drop=True)\n",
    "        \n",
    "        # Get unique dates\n",
    "        unique_dates = sorted(df_sorted[self.date_col].unique())\n",
    "        n_dates = len(unique_dates)\n",
    "        \n",
    "        self.logger.info(f\"Total unique dates: {n_dates}\")\n",
    "        self.logger.info(f\"Date range: {unique_dates[0].date()} to {unique_dates[-1].date()}\")\n",
    "        \n",
    "        # Calculate cutoff indices based on proportions\n",
    "        train_cutoff_idx = int(n_dates * train_size)\n",
    "        val_cutoff_idx = int(n_dates * (train_size + val_size))\n",
    "        \n",
    "        # Get cutoff dates\n",
    "        train_end_date = unique_dates[train_cutoff_idx - 1]\n",
    "        val_end_date = unique_dates[val_cutoff_idx - 1]\n",
    "        \n",
    "        self.logger.info(f\"\\nSplit cutoff dates:\")\n",
    "        self.logger.info(f\"  Train: up to {train_end_date.date()} ({train_cutoff_idx} dates)\")\n",
    "        self.logger.info(f\"  Val: {unique_dates[train_cutoff_idx].date()} to {val_end_date.date()} ({val_cutoff_idx - train_cutoff_idx} dates)\")\n",
    "        self.logger.info(f\"  Test: {unique_dates[val_cutoff_idx].date()} onwards ({n_dates - val_cutoff_idx} dates)\")\n",
    "        \n",
    "        # Create splits\n",
    "        train = df_sorted[df_sorted[self.date_col] <= train_end_date].copy()\n",
    "        val = df_sorted[(df_sorted[self.date_col] > train_end_date) & \n",
    "                       (df_sorted[self.date_col] <= val_end_date)].copy()\n",
    "        test = df_sorted[df_sorted[self.date_col] > val_end_date].copy()\n",
    "        \n",
    "        # Log statistics\n",
    "        split_info = {\n",
    "            'train': {\n",
    "                'n_records': len(train),\n",
    "                'n_clients': train['cli_code'].nunique(),\n",
    "                'date_range': (train[self.date_col].min(), train[self.date_col].max())\n",
    "            },\n",
    "            'val': {\n",
    "                'n_records': len(val),\n",
    "                'n_clients': val['cli_code'].nunique(),\n",
    "                'date_range': (val[self.date_col].min(), val[self.date_col].max())\n",
    "            },\n",
    "            'test': {\n",
    "                'n_records': len(test),\n",
    "                'n_clients': test['cli_code'].nunique(),\n",
    "                'date_range': (test[self.date_col].min(), test[self.date_col].max())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, info in split_info.items():\n",
    "            self.logger.info(f\"\\n{name.upper()}:\")\n",
    "            self.logger.info(f\"  Records: {info['n_records']}\")\n",
    "            self.logger.info(f\"  Clients: {info['n_clients']}\")\n",
    "            self.logger.info(f\"  Dates: {info['date_range'][0].date()} to {info['date_range'][1].date()}\")\n",
    "        \n",
    "        # Verify no data leakage\n",
    "        assert train[self.date_col].max() < val[self.date_col].min(), \"Data leakage: train overlaps val!\"\n",
    "        assert val[self.date_col].max() < test[self.date_col].min(), \"Data leakage: val overlaps test!\"\n",
    "        self.logger.info(\"\\n✓ Temporal ordering verified - no data leakage\")\n",
    "        \n",
    "        return train, val, test, split_info\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"CREATING TEMPORAL TRAIN/VAL/TEST SPLIT\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "splitter = TemporalSplitter(train_full_df)\n",
    "train_df, val_df, test_df, split_info = splitter.split(\n",
    "    train_size=config.TRAIN_SIZE,\n",
    "    val_size=config.VAL_SIZE,\n",
    "    test_size=config.TEST_SIZE\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Temporal split complete\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val: {val_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c97833c1-cf75-4002-aa9a-5aa99ed347fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - REMOVING TEMPORAL GAPS (TRAIN ONLY)\n",
      "INFO - ============================================================\n",
      "INFO - Analyzing temporal gaps (chunked)...\n",
      "INFO -   Processed 50000/173448 clients\n",
      "INFO -   Processed 100000/173448 clients\n",
      "INFO -   Processed 150000/173448 clients\n",
      "INFO - Gaps: 1131 clients (0.7%)\n",
      "INFO - ✓ Removed 1131 clients with gaps\n",
      "INFO -   Records: 2162862 → 2153407\n",
      "INFO - \n",
      "Removing 1131 clients from val/test as well...\n",
      "INFO -   Val: 531880 records remaining\n",
      "INFO -   Test: 542902 records remaining\n",
      "\n",
      "✓ Gap removal complete\n",
      "  Train: 2153407 records\n",
      "  Val: 531880 records\n",
      "  Test: 542902 records\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Gap Detection (AFTER split - only on train!)\n",
    "class GapDetector:\n",
    "    \"\"\"Detect and handle temporal gaps - AFTER SPLIT to avoid leakage\"\"\"\n",
    "    \n",
    "    def __init__(self, df, client_col='cli_code', date_col='observation_point'):\n",
    "        self.df = df\n",
    "        self.client_col = client_col\n",
    "        self.date_col = date_col\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def detect_gaps(self):\n",
    "        \"\"\"Detect clients with gaps - CHUNKED VERSION\"\"\"\n",
    "        self.logger.info(\"Analyzing temporal gaps (chunked)...\")\n",
    "        \n",
    "        # Convert date once\n",
    "        if self.df[self.date_col].dtype != 'datetime64[ns]':\n",
    "            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col])\n",
    "        \n",
    "        # Process in chunks by client\n",
    "        unique_clients = self.df[self.client_col].unique()\n",
    "        chunk_size = 10000\n",
    "        clients_with_gaps_list = []\n",
    "        \n",
    "        for i in range(0, len(unique_clients), chunk_size):\n",
    "            chunk_clients = unique_clients[i:i+chunk_size]\n",
    "            \n",
    "            chunk_df = self.df[self.df[self.client_col].isin(chunk_clients)].copy()\n",
    "            chunk_df = chunk_df.sort_values([self.client_col, self.date_col])\n",
    "            \n",
    "            chunk_df['month_num'] = chunk_df[self.date_col].dt.to_period('M').apply(lambda x: x.ordinal)\n",
    "            chunk_df['month_diff'] = chunk_df.groupby(self.client_col)['month_num'].diff()\n",
    "            \n",
    "            gaps_summary = chunk_df.groupby(self.client_col)['month_diff'].agg([\n",
    "                ('max_gap', 'max'),\n",
    "                ('total_gaps', lambda x: (x > 1).sum())\n",
    "            ]).reset_index()\n",
    "            \n",
    "            chunk_clients_with_gaps = gaps_summary[gaps_summary['max_gap'] > 1]\n",
    "            clients_with_gaps_list.append(chunk_clients_with_gaps)\n",
    "            \n",
    "            del chunk_df, gaps_summary\n",
    "            \n",
    "            if (i // chunk_size + 1) % 5 == 0:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "                self.logger.info(f\"  Processed {i+chunk_size}/{len(unique_clients)} clients\")\n",
    "        \n",
    "        clients_with_gaps = pd.concat(clients_with_gaps_list, ignore_index=True)\n",
    "        \n",
    "        gap_info = {\n",
    "            'total_clients': len(unique_clients),\n",
    "            'clients_with_gaps': len(clients_with_gaps),\n",
    "            'pct_with_gaps': len(clients_with_gaps) / len(unique_clients) * 100\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"Gaps: {gap_info['clients_with_gaps']} clients ({gap_info['pct_with_gaps']:.1f}%)\")\n",
    "        \n",
    "        return clients_with_gaps, gap_info\n",
    "    \n",
    "    def remove_clients_with_gaps(self):\n",
    "        \"\"\"Remove clients with gaps\"\"\"\n",
    "        clients_with_gaps, gap_info = self.detect_gaps()\n",
    "        \n",
    "        if len(clients_with_gaps) == 0:\n",
    "            self.logger.info(\"✓ No gaps found\")\n",
    "            return self.df, gap_info\n",
    "        \n",
    "        bad_clients = set(clients_with_gaps[self.client_col].unique())\n",
    "        \n",
    "        original_len = len(self.df)\n",
    "        mask = ~self.df[self.client_col].isin(bad_clients)\n",
    "        df_clean = self.df[mask].copy()\n",
    "        \n",
    "        self.logger.info(f\"✓ Removed {len(bad_clients)} clients with gaps\")\n",
    "        self.logger.info(f\"  Records: {original_len} → {len(df_clean)}\")\n",
    "        \n",
    "        del clients_with_gaps, bad_clients, mask\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        return df_clean, gap_info\n",
    "\n",
    "\n",
    "if config.REMOVE_GAPS:\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"REMOVING TEMPORAL GAPS (TRAIN ONLY)\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    gap_detector = GapDetector(train_df)\n",
    "    train_df_new, gap_info = gap_detector.remove_clients_with_gaps()\n",
    "    \n",
    "    # Store list of clients to remove\n",
    "    clients_to_remove = set(train_df['cli_code']) - set(train_df_new['cli_code'])\n",
    "    \n",
    "    # Also remove these clients from val and test\n",
    "    if len(clients_to_remove) > 0:\n",
    "        logger.info(f\"\\nRemoving {len(clients_to_remove)} clients from val/test as well...\")\n",
    "        val_df = val_df[~val_df['cli_code'].isin(clients_to_remove)].copy()\n",
    "        test_df = test_df[~test_df['cli_code'].isin(clients_to_remove)].copy()\n",
    "        logger.info(f\"  Val: {len(val_df)} records remaining\")\n",
    "        logger.info(f\"  Test: {len(test_df)} records remaining\")\n",
    "    \n",
    "    del train_df\n",
    "    train_df = train_df_new\n",
    "    del train_df_new, clients_to_remove\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ Gap removal complete\")\n",
    "    print(f\"  Train: {len(train_df)} records\")\n",
    "    print(f\"  Val: {len(val_df)} records\")\n",
    "    print(f\"  Test: {len(test_df)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347b01f9-6ab1-4456-918d-3904e95262ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ PreprocessingPipeline class defined (optimized for CatBoost)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Preprocessing Pipeline - Core Methods (NO ENCODING for CatBoost!)\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"Main preprocessing pipeline - optimized for CatBoost\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.metadata = {\n",
    "            'config': vars(config),\n",
    "            'steps': [],\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        self.fitted_columns = None\n",
    "    \n",
    "    def fit_transform(self, train_df):\n",
    "        \"\"\"Fit preprocessing on training data and transform it\"\"\"\n",
    "        self.logger.info(\"=\"*60)\n",
    "        self.logger.info(\"FITTING PREPROCESSING PIPELINE\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "        \n",
    "        df_processed = train_df\n",
    "        original_shape = df_processed.shape\n",
    "        self.metadata['original_train_shape'] = original_shape\n",
    "        \n",
    "        # Store original column order (before any transformations)\n",
    "        self.fitted_columns = [col for col in df_processed.columns \n",
    "                              if col not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "        \n",
    "        # Step 1: Remove constant columns\n",
    "        df_processed, _ = self._remove_constant_columns(df_processed, fit=True)\n",
    "        \n",
    "        # Step 2: Handle outliers\n",
    "        df_processed, _ = self._handle_outliers(df_processed, fit=True)\n",
    "        \n",
    "        # Step 3: Handle missing values (NO ENCODING - CatBoost handles categoricals!)\n",
    "        df_processed, _ = self._handle_missing_values(df_processed, fit=True)\n",
    "        \n",
    "        # Step 4: Remove high correlations\n",
    "        df_processed, _ = self._remove_correlations(df_processed, fit=True)\n",
    "        \n",
    "        df_final = df_processed.copy()\n",
    "        \n",
    "        # Store final feature list after all transformations\n",
    "        self.final_features = [col for col in df_final.columns \n",
    "                              if col not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "        \n",
    "        self._log_summary(original_shape, df_final.shape, 'TRAIN')\n",
    "        \n",
    "        if df_processed is not train_df and df_processed is not df_final:\n",
    "            del df_processed\n",
    "            import gc\n",
    "            gc.collect()\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    def transform(self, df, dataset_name='test'):\n",
    "        \"\"\"Transform new data using fitted preprocessing\"\"\"\n",
    "        self.logger.info(f\"=\"*60)\n",
    "        self.logger.info(f\"TRANSFORMING {dataset_name.upper()} DATA\")\n",
    "        self.logger.info(f\"=\"*60)\n",
    "        \n",
    "        df_processed = df\n",
    "        original_shape = df_processed.shape\n",
    "        \n",
    "        df_processed, _ = self._remove_constant_columns(df_processed, fit=False)\n",
    "        df_processed, _ = self._handle_outliers(df_processed, fit=False)\n",
    "        df_processed, _ = self._handle_missing_values(df_processed, fit=False)\n",
    "        df_processed, _ = self._remove_correlations(df_processed, fit=False)\n",
    "        \n",
    "        # FINAL: Align columns with training data\n",
    "        df_processed = self._align_columns_with_training(df_processed, dataset_name)\n",
    "        \n",
    "        df_final = df_processed.copy()\n",
    "        \n",
    "        self._log_summary(original_shape, df_final.shape, dataset_name)\n",
    "        \n",
    "        if df_processed is not df and df_processed is not df_final:\n",
    "            del df_processed\n",
    "            import gc\n",
    "            gc.collect()\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    def _align_columns_with_training(self, df, dataset_name):\n",
    "        \"\"\"Ensure DataFrame has same columns as training data\"\"\"\n",
    "        if self.final_features is None:\n",
    "            self.logger.warning(\"⚠ No fitted features found, skipping alignment\")\n",
    "            return df\n",
    "        \n",
    "        preserve_cols = [col for col in config.ID_COLUMNS if col in df.columns]\n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            preserve_cols.append(config.TARGET_COLUMN)\n",
    "        \n",
    "        current_features = [col for col in df.columns if col not in preserve_cols]\n",
    "        \n",
    "        missing_cols = [col for col in self.final_features if col not in current_features]\n",
    "        extra_cols = [col for col in current_features if col not in self.final_features]\n",
    "        \n",
    "        needs_modification = len(missing_cols) > 0 or len(extra_cols) > 0\n",
    "        \n",
    "        if needs_modification:\n",
    "            df = df.copy()\n",
    "            \n",
    "            if missing_cols:\n",
    "                self.logger.warning(f\"⚠ Adding {len(missing_cols)} missing columns with zeros\")\n",
    "                for col in missing_cols:\n",
    "                    df[col] = 0\n",
    "            \n",
    "            if extra_cols:\n",
    "                self.logger.warning(f\"⚠ Removing {len(extra_cols)} extra columns\")\n",
    "                df = df.drop(columns=extra_cols)\n",
    "        \n",
    "        final_column_order = preserve_cols + self.final_features\n",
    "        df = df[[col for col in final_column_order if col in df.columns]]\n",
    "        \n",
    "        self.logger.info(f\"✓ Final columns aligned: {len(df.columns)} total\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _log_summary(self, original_shape, final_shape, name):\n",
    "        \"\"\"Log preprocessing summary\"\"\"\n",
    "        self.logger.info(f\"\\n{'='*60}\")\n",
    "        self.logger.info(f\"SUMMARY - {name}\")\n",
    "        self.logger.info(f\"{'='*60}\")\n",
    "        self.logger.info(f\"Original: {original_shape}\")\n",
    "        self.logger.info(f\"Final: {final_shape}\")\n",
    "        self.logger.info(f\"Records: {(1-final_shape[0]/original_shape[0])*100:.1f}% reduction\")\n",
    "        self.logger.info(f\"Features: {(1-final_shape[1]/original_shape[1])*100:.1f}% reduction\")\n",
    "\n",
    "print(\"\\n✓ PreprocessingPipeline class defined (optimized for CatBoost)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88e8afd-d466-46f5-9611-6232438527d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ PreprocessingPipeline helper methods attached\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Preprocessing Pipeline - Helper Methods\n",
    "def _remove_constant_columns(self, df, fit=True):\n",
    "    \"\"\"Remove constant columns\"\"\"\n",
    "    if fit:\n",
    "        self.logger.info(\"Step: Removing constant columns...\")\n",
    "        self.constant_cols = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "                continue\n",
    "            n_unique = df[col].nunique(dropna=False)\n",
    "            if n_unique == 1:\n",
    "                self.constant_cols.append(col)\n",
    "        \n",
    "        if self.constant_cols:\n",
    "            self.logger.info(f\"  Removing {len(self.constant_cols)} constant columns\")\n",
    "            df = df.drop(columns=self.constant_cols)\n",
    "        else:\n",
    "            self.logger.info(\"  No constant columns found\")\n",
    "        \n",
    "        self.metadata['steps'].append({\n",
    "            'step': 'remove_constants',\n",
    "            'n_removed': len(self.constant_cols)\n",
    "        })\n",
    "    else:\n",
    "        self.logger.info(f\"  Skipping constant column removal (will align at end)\")\n",
    "    \n",
    "    return df, {}\n",
    "\n",
    "def _handle_outliers(self, df, fit=True):\n",
    "    \"\"\"Handle outliers in numerical columns\"\"\"\n",
    "    if not self.config.HANDLE_OUTLIERS:\n",
    "        return df, {}\n",
    "    \n",
    "    self.logger.info(\"Step: Handling outliers...\")\n",
    "    \n",
    "    if fit:\n",
    "        target_cols = [col for col in df.columns \n",
    "                      if any(kw in col.lower() for kw in \n",
    "                            self.config.FINANCIAL_KEYWORDS + self.config.BALANCE_KEYWORDS)\n",
    "                      and col not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "        \n",
    "        self.outlier_bounds = {}\n",
    "        for col in target_cols:\n",
    "            if col in df.columns and df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                self.outlier_bounds[col] = {\n",
    "                    'lower': Q1 - self.config.OUTLIER_IQR_MULTIPLIER * IQR,\n",
    "                    'upper': Q3 + self.config.OUTLIER_IQR_MULTIPLIER * IQR\n",
    "                }\n",
    "        \n",
    "        for col, bounds in self.outlier_bounds.items():\n",
    "            df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "        \n",
    "        self.logger.info(f\"  Treated outliers in {len(self.outlier_bounds)} columns\")\n",
    "        \n",
    "        self.metadata['steps'].append({\n",
    "            'step': 'handle_outliers',\n",
    "            'n_columns': len(self.outlier_bounds)\n",
    "        })\n",
    "    else:\n",
    "        n_clipped = 0\n",
    "        for col, bounds in self.outlier_bounds.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "                n_clipped += 1\n",
    "        \n",
    "        self.logger.info(f\"  Applied outlier clipping to {n_clipped}/{len(self.outlier_bounds)} columns\")\n",
    "    \n",
    "    return df, {}\n",
    "\n",
    "def _handle_missing_values(self, df, fit=True):\n",
    "    \"\"\"Handle missing values - NO ENCODING for CatBoost!\"\"\"\n",
    "    self.logger.info(\"Step: Handling missing values...\")\n",
    "    \n",
    "    if fit:\n",
    "        # Separate numeric and categorical (but don't encode categorical!)\n",
    "        self.numeric_cols_for_imputation = [col for col in df.select_dtypes(include=[np.number]).columns\n",
    "                                           if col not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "        \n",
    "        # CatBoost handles categorical features natively - just keep them as strings\n",
    "        self.categorical_cols_for_imputation = [col for col in config.CATEGORICAL_FEATURES \n",
    "                                               if col in df.columns]\n",
    "        \n",
    "        # Fit imputers\n",
    "        self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "        self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        \n",
    "        if len(self.numeric_cols_for_imputation) > 0:\n",
    "            df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                df[self.numeric_cols_for_imputation]\n",
    "            )\n",
    "        \n",
    "        if len(self.categorical_cols_for_imputation) > 0:\n",
    "            df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                df[self.categorical_cols_for_imputation]\n",
    "            )\n",
    "        \n",
    "        self.logger.info(f\"  Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
    "                        f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        self.logger.info(f\"  ℹ Categorical features kept as-is for CatBoost\")\n",
    "        \n",
    "        self.metadata['steps'].append({\n",
    "            'step': 'handle_missing',\n",
    "            'n_numeric': len(self.numeric_cols_for_imputation),\n",
    "            'n_categorical': len(self.categorical_cols_for_imputation),\n",
    "            'note': 'No encoding - CatBoost handles categoricals natively'\n",
    "        })\n",
    "    else:\n",
    "        numeric_cols_present = [col for col in self.numeric_cols_for_imputation if col in df.columns]\n",
    "        categorical_cols_present = [col for col in self.categorical_cols_for_imputation if col in df.columns]\n",
    "        \n",
    "        missing_numeric = [col for col in self.numeric_cols_for_imputation if col not in df.columns]\n",
    "        if missing_numeric:\n",
    "            self.logger.warning(f\"⚠ Adding {len(missing_numeric)} missing numeric columns with fill values\")\n",
    "            for i, col in enumerate(missing_numeric):\n",
    "                fill_value = self.numeric_imputer.statistics_[\n",
    "                    self.numeric_cols_for_imputation.index(col)\n",
    "                ]\n",
    "                df[col] = fill_value\n",
    "        \n",
    "        missing_categorical = [col for col in self.categorical_cols_for_imputation if col not in df.columns]\n",
    "        if missing_categorical:\n",
    "            self.logger.warning(f\"⚠ Adding {len(missing_categorical)} missing categorical columns with fill values\")\n",
    "            for i, col in enumerate(missing_categorical):\n",
    "                fill_value = self.categorical_imputer.statistics_[\n",
    "                    self.categorical_cols_for_imputation.index(col)\n",
    "                ]\n",
    "                df[col] = fill_value\n",
    "        \n",
    "        if len(self.numeric_cols_for_imputation) > 0 and hasattr(self, 'numeric_imputer'):\n",
    "            df[self.numeric_cols_for_imputation] = self.numeric_imputer.transform(\n",
    "                df[self.numeric_cols_for_imputation]\n",
    "            )\n",
    "        \n",
    "        if len(self.categorical_cols_for_imputation) > 0 and hasattr(self, 'categorical_imputer'):\n",
    "            df[self.categorical_cols_for_imputation] = self.categorical_imputer.transform(\n",
    "                df[self.categorical_cols_for_imputation]\n",
    "            )\n",
    "    \n",
    "    return df, {}\n",
    "\n",
    "def _remove_correlations(self, df, fit=True):\n",
    "    \"\"\"Remove highly correlated features\"\"\"\n",
    "    if not self.config.REMOVE_HIGH_CORRELATIONS:\n",
    "        return df, {}\n",
    "    \n",
    "    self.logger.info(\"Step: Removing high correlations...\")\n",
    "    \n",
    "    if fit:\n",
    "        # Only numeric, non-categorical columns\n",
    "        numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns\n",
    "                       if col not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "        \n",
    "        if len(numeric_cols) > 1:\n",
    "            corr_matrix = df[numeric_cols].corr().abs()\n",
    "            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "            \n",
    "            self.features_to_drop_corr = [col for col in upper.columns \n",
    "                                         if any(upper[col] > self.config.CORRELATION_THRESHOLD)]\n",
    "            \n",
    "            if self.features_to_drop_corr:\n",
    "                df = df.drop(columns=self.features_to_drop_corr)\n",
    "                self.logger.info(f\"  Removed {len(self.features_to_drop_corr)} correlated features\")\n",
    "            else:\n",
    "                self.logger.info(\"  No highly correlated features found\")\n",
    "        else:\n",
    "            self.features_to_drop_corr = []\n",
    "        \n",
    "        self.metadata['steps'].append({\n",
    "            'step': 'remove_correlations',\n",
    "            'n_removed': len(self.features_to_drop_corr)\n",
    "        })\n",
    "    else:\n",
    "        self.logger.info(f\"  Skipping correlation removal (will align at end)\")\n",
    "    \n",
    "    return df, {}\n",
    "\n",
    "# Attach helper methods to PreprocessingPipeline class\n",
    "PreprocessingPipeline._remove_constant_columns = _remove_constant_columns\n",
    "PreprocessingPipeline._handle_outliers = _handle_outliers\n",
    "PreprocessingPipeline._handle_missing_values = _handle_missing_values\n",
    "PreprocessingPipeline._remove_correlations = _remove_correlations\n",
    "\n",
    "print(\"\\n✓ PreprocessingPipeline helper methods attached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f76e5c1-b87d-4912-a988-e6bb35d65f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - FITTING PREPROCESSING ON TRAINING DATA\n",
      "INFO - ============================================================\n",
      "INFO - ============================================================\n",
      "INFO - FITTING PREPROCESSING PIPELINE\n",
      "INFO - ============================================================\n",
      "INFO - Step: Removing constant columns...\n",
      "INFO -   Removing 9 constant columns\n",
      "INFO - Step: Handling outliers...\n",
      "INFO -   Treated outliers in 91 columns\n",
      "INFO - Step: Handling missing values...\n",
      "INFO -   Imputed: 181 numeric, 3 categorical\n",
      "INFO -   ℹ Categorical features kept as-is for CatBoost\n",
      "INFO - Step: Removing high correlations...\n",
      "INFO -   Removed 61 correlated features\n",
      "INFO - \n",
      "============================================================\n",
      "INFO - SUMMARY - TRAIN\n",
      "INFO - ============================================================\n",
      "INFO - Original: (2153407, 195)\n",
      "INFO - Final: (2153407, 125)\n",
      "INFO - Records: 0.0% reduction\n",
      "INFO - Features: 35.9% reduction\n",
      "\n",
      "✓ Preprocessing fitted on training data\n",
      "  Original shape: (2153407, 195)\n",
      "  Processed shape: (2153407, 125)\n",
      "  Features stored: 121\n",
      "  Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Fit Preprocessing Pipeline on Training Data\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"FITTING PREPROCESSING ON TRAINING DATA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = PreprocessingPipeline(config)\n",
    "\n",
    "# Fit and transform training data\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "\n",
    "print(f\"\\n✓ Preprocessing fitted on training data\")\n",
    "print(f\"  Original shape: {train_df.shape}\")\n",
    "print(f\"  Processed shape: {train_processed.shape}\")\n",
    "print(f\"  Features stored: {len(pipeline.final_features)}\")\n",
    "\n",
    "# Clean up\n",
    "del train_df\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfc7503-9bce-4fe1-8ec5-1143ac17a8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - TRANSFORMING VALIDATION AND TEST DATA\n",
      "INFO - ============================================================\n",
      "INFO - ============================================================\n",
      "INFO - TRANSFORMING VALIDATION DATA\n",
      "INFO - ============================================================\n",
      "INFO -   Skipping constant column removal (will align at end)\n",
      "INFO - Step: Handling outliers...\n",
      "INFO -   Applied outlier clipping to 91/91 columns\n",
      "INFO - Step: Handling missing values...\n",
      "INFO - Step: Removing high correlations...\n",
      "INFO -   Skipping correlation removal (will align at end)\n",
      "WARNING - ⚠ Removing 70 extra columns\n",
      "INFO - ✓ Final columns aligned: 125 total\n",
      "INFO - \n",
      "============================================================\n",
      "INFO - SUMMARY - validation\n",
      "INFO - ============================================================\n",
      "INFO - Original: (531880, 195)\n",
      "INFO - Final: (531880, 125)\n",
      "INFO - Records: 0.0% reduction\n",
      "INFO - Features: 35.9% reduction\n",
      "INFO - ============================================================\n",
      "INFO - TRANSFORMING TEST DATA\n",
      "INFO - ============================================================\n",
      "INFO -   Skipping constant column removal (will align at end)\n",
      "INFO - Step: Handling outliers...\n",
      "INFO -   Applied outlier clipping to 91/91 columns\n",
      "INFO - Step: Handling missing values...\n",
      "INFO - Step: Removing high correlations...\n",
      "INFO -   Skipping correlation removal (will align at end)\n",
      "WARNING - ⚠ Removing 70 extra columns\n",
      "INFO - ✓ Final columns aligned: 125 total\n",
      "INFO - \n",
      "============================================================\n",
      "INFO - SUMMARY - test\n",
      "INFO - ============================================================\n",
      "INFO - Original: (542902, 195)\n",
      "INFO - Final: (542902, 125)\n",
      "INFO - Records: 0.0% reduction\n",
      "INFO - Features: 35.9% reduction\n",
      "\n",
      "✓ Validation and test data transformed\n",
      "  Validation: (531880, 195) → (531880, 125)\n",
      "  Test: (542902, 195) → (542902, 125)\n",
      "  Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Transform Validation and Test Data\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"TRANSFORMING VALIDATION AND TEST DATA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Transform validation data\n",
    "val_processed = pipeline.transform(val_df, dataset_name='validation')\n",
    "\n",
    "# Transform test data\n",
    "test_processed = pipeline.transform(test_df, dataset_name='test')\n",
    "\n",
    "print(f\"\\n✓ Validation and test data transformed\")\n",
    "print(f\"  Validation: {val_df.shape} → {val_processed.shape}\")\n",
    "print(f\"  Test: {test_df.shape} → {test_processed.shape}\")\n",
    "\n",
    "# Clean up\n",
    "del val_df, test_df\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8963b3aa-e8d8-44db-adb8-bb46b46ed0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - LOADING PRODUCTION DATA\n",
      "INFO - ============================================================\n",
      "INFO - Loading data from: data\\churn_prod_ul.csv\n",
      "INFO -   Memory before optimization: 329.52 MB\n",
      "INFO -   Memory after optimization: 144.16 MB (56.3% saved)\n",
      "INFO - ✓ Data loaded: (206770, 194)\n",
      "INFO - ✓ Data validation passed (no target - production data)\n",
      "INFO - ✓ Production data loaded\n",
      "INFO - ℹ Production data is for scoring (no target)\n",
      "\n",
      "✓ Production data loaded: (206770, 194)\n",
      "INFO - \n",
      "============================================================\n",
      "INFO - TRANSFORMING PRODUCTION DATA\n",
      "INFO - ============================================================\n",
      "INFO - ============================================================\n",
      "INFO - TRANSFORMING PRODUCTION DATA\n",
      "INFO - ============================================================\n",
      "INFO -   Skipping constant column removal (will align at end)\n",
      "INFO - Step: Handling outliers...\n",
      "INFO -   Applied outlier clipping to 91/91 columns\n",
      "INFO - Step: Handling missing values...\n",
      "INFO - Step: Removing high correlations...\n",
      "INFO -   Skipping correlation removal (will align at end)\n",
      "WARNING - ⚠ Removing 70 extra columns\n",
      "INFO - ✓ Final columns aligned: 124 total\n",
      "INFO - \n",
      "============================================================\n",
      "INFO - SUMMARY - production\n",
      "INFO - ============================================================\n",
      "INFO - Original: (206770, 194)\n",
      "INFO - Final: (206770, 124)\n",
      "INFO - Records: 0.0% reduction\n",
      "INFO - Features: 36.1% reduction\n",
      "✓ Production data transformed\n",
      "  Original: (206770, 194)\n",
      "  Processed: (206770, 124)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Load and Transform Production Data\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"LOADING PRODUCTION DATA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "prod_df = None\n",
    "prod_path = config.get_prod_input_path()\n",
    "\n",
    "if prod_path.exists():\n",
    "    prod_loader = DataLoader(prod_path, delimiter=config.DELIMITER)\n",
    "    prod_df = prod_loader.load()\n",
    "    \n",
    "    logger.info(\"✓ Production data loaded\")\n",
    "    logger.info(\"ℹ Production data is for scoring (no target)\")\n",
    "    \n",
    "    print(f\"\\n✓ Production data loaded: {prod_df.shape}\")\n",
    "    \n",
    "    # Transform production data\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"TRANSFORMING PRODUCTION DATA\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        prod_processed = pipeline.transform(prod_df, dataset_name='production')\n",
    "        \n",
    "        print(f\"✓ Production data transformed\")\n",
    "        print(f\"  Original: {prod_df.shape}\")\n",
    "        print(f\"  Processed: {prod_processed.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Error transforming production data: {e}\")\n",
    "        print(f\"\\n✗ Error transforming production data: {e}\")\n",
    "        print(\"\\nℹ Check that production data has compatible features\")\n",
    "        prod_processed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1648b09-dcd4-4ab4-965e-030a613e0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - SAVING PROCESSED DATA\n",
      "INFO - ============================================================\n",
      "INFO - ✓ Saved training data: output\\train_processed.csv\n",
      "INFO - ✓ Saved validation data: output\\val_processed.csv\n",
      "INFO - ✓ Saved test data: output\\test_processed.csv\n",
      "\n",
      "✓ Processed data saved:\n",
      "  Train: output\\train_processed.csv\n",
      "  Val: output\\val_processed.csv\n",
      "  Test: output\\test_processed.csv\n",
      "INFO - ✓ Saved production data: output\\prod_processed.csv\n",
      "  Prod: output\\prod_processed.csv\n",
      "INFO - ✓ Saved metadata: output\\preprocessing_metadata.json\n",
      "\n",
      "✓ Metadata saved: output\\preprocessing_metadata.json\n",
      "INFO - ✓ Saved categorical features list: output\\categorical_features.json\n",
      "✓ Categorical features list saved: output\\categorical_features.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Save All Processed Data\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"SAVING PROCESSED DATA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Save datasets\n",
    "train_path = config.OUTPUT_DIR / \"train_processed.csv\"\n",
    "val_path = config.OUTPUT_DIR / \"val_processed.csv\"\n",
    "test_path = config.OUTPUT_DIR / \"test_processed.csv\"\n",
    "\n",
    "train_processed.to_csv(train_path, index=False, sep='|')\n",
    "val_processed.to_csv(val_path, index=False, sep='|')\n",
    "test_processed.to_csv(test_path, index=False, sep='|')\n",
    "\n",
    "logger.info(f\"✓ Saved training data: {train_path}\")\n",
    "logger.info(f\"✓ Saved validation data: {val_path}\")\n",
    "logger.info(f\"✓ Saved test data: {test_path}\")\n",
    "\n",
    "print(f\"\\n✓ Processed data saved:\")\n",
    "print(f\"  Train: {train_path}\")\n",
    "print(f\"  Val: {val_path}\")\n",
    "print(f\"  Test: {test_path}\")\n",
    "\n",
    "if prod_processed is not None:\n",
    "    prod_path = config.OUTPUT_DIR / \"prod_processed.csv\"\n",
    "    prod_processed.to_csv(prod_path, index=False, sep='|')\n",
    "    logger.info(f\"✓ Saved production data: {prod_path}\")\n",
    "    print(f\"  Prod: {prod_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = config.OUTPUT_DIR / config.METADATA_FILE\n",
    "pipeline.metadata['split_info'] = split_info\n",
    "pipeline.metadata['categorical_features'] = config.CATEGORICAL_FEATURES\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(pipeline.metadata, f, indent=2, default=str)\n",
    "\n",
    "logger.info(f\"✓ Saved metadata: {metadata_path}\")\n",
    "print(f\"\\n✓ Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Save categorical features list for model training\n",
    "cat_features_path = config.OUTPUT_DIR / \"categorical_features.json\"\n",
    "with open(cat_features_path, 'w') as f:\n",
    "    json.dump(config.CATEGORICAL_FEATURES, f, indent=2)\n",
    "logger.info(f\"✓ Saved categorical features list: {cat_features_path}\")\n",
    "print(f\"✓ Categorical features list saved: {cat_features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb12324-1a67-466f-8854-8189b59a9848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - PREPROCESSING PIPELINE COMPLETED\n",
      "INFO - ============================================================\n",
      "\n",
      "============================================================\n",
      "✓ PREPROCESSING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "\n",
      "Final Dataset Shapes:\n",
      "  Training:   (2153407, 125)\n",
      "  Validation: (531880, 125)\n",
      "  Test:       (542902, 125)\n",
      "  Production: (206770, 124)\n",
      "\n",
      "Churn Rates:\n",
      "  Training:   0.0144\n",
      "  Validation: 0.0177\n",
      "  Test:       0.0145\n",
      "\n",
      "Output Files:\n",
      "  Data: output\n",
      "  Logs: logs\n",
      "\n",
      "============================================================\n",
      "✓ READY FOR MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Review logs in the 'logs' directory\n",
      "2. Check processed data in 'output' directory\n",
      "3. Train model on train_processed\n",
      "4. Tune on val_processed\n",
      "5. Evaluate on test_processed\n",
      "6. Score on prod_processed (if available)\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Final Summary and Verification\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"PREPROCESSING PIPELINE COMPLETED\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ PREPROCESSING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFinal Dataset Shapes:\")\n",
    "print(f\"  Training:   {train_processed.shape}\")\n",
    "print(f\"  Validation: {val_processed.shape}\")\n",
    "print(f\"  Test:       {test_processed.shape}\")\n",
    "if prod_processed is not None:\n",
    "    print(f\"  Production: {prod_processed.shape}\")\n",
    "\n",
    "if config.TARGET_COLUMN in train_processed.columns:\n",
    "    print(f\"\\nChurn Rates:\")\n",
    "    print(f\"  Training:   {train_processed[config.TARGET_COLUMN].mean():.4f}\")\n",
    "    print(f\"  Validation: {val_processed[config.TARGET_COLUMN].mean():.4f}\")\n",
    "    print(f\"  Test:       {test_processed[config.TARGET_COLUMN].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Data: {config.OUTPUT_DIR}\")\n",
    "print(f\"  Logs: {config.LOG_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ READY FOR MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Review logs in the 'logs' directory\")\n",
    "print(\"2. Check processed data in 'output' directory\")\n",
    "print(\"3. Train model on train_processed\")\n",
    "print(\"4. Tune on val_processed\")\n",
    "print(\"5. Evaluate on test_processed\")\n",
    "print(\"6. Score on prod_processed (if available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b9b209e-df80-4962-864a-d328b37e7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - MULTI-MODEL MULTI-SEGMENT CONFIGURATION\n",
      "INFO - ============================================================\n",
      "INFO - Segments: ['SMALL_BUSINESS', 'MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
      "INFO - Models: ['catboost', 'xgboost', 'lightgbm']\n",
      "\n",
      "✓ Multi-model configuration initialized\n",
      "  Segments: ['SMALL_BUSINESS', 'MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
      "  Models: ['catboost', 'xgboost', 'lightgbm']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 18: Multi-Model & Multi-Segment Configuration\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "class MultiModelConfig:\n",
    "    \"\"\"Configuration for multi-model, multi-segment training\"\"\"\n",
    "    \n",
    "    # Directories\n",
    "    SEGMENT_DIR = config.OUTPUT_DIR / \"segments\"\n",
    "    MODEL_SPECIFIC_DIR = config.OUTPUT_DIR / \"model_specific\"\n",
    "    ENCODERS_DIR = config.OUTPUT_DIR / \"encoders\"\n",
    "    \n",
    "    # Segments\n",
    "    SEGMENTS = ['SMALL_BUSINESS', 'MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "    SEGMENT_COLUMN = 'segment_group'\n",
    "    \n",
    "    # Models\n",
    "    MODELS = ['catboost', 'xgboost', 'lightgbm']\n",
    "    \n",
    "    # Encoding strategies\n",
    "    ENCODING_STRATEGY = {\n",
    "        'catboost': 'none',      # CatBoost handles categoricals natively\n",
    "        'xgboost': 'onehot',     # XGBoost needs one-hot encoding\n",
    "        'lightgbm': 'label'      # LightGBM works best with label encoding\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        \"\"\"Create all necessary directories\"\"\"\n",
    "        for dir_path in [cls.SEGMENT_DIR, cls.MODEL_SPECIFIC_DIR, cls.ENCODERS_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for model in cls.MODELS:\n",
    "            (cls.MODEL_SPECIFIC_DIR / model).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize\n",
    "multi_config = MultiModelConfig()\n",
    "multi_config.create_directories()\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"MULTI-MODEL MULTI-SEGMENT CONFIGURATION\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Segments: {multi_config.SEGMENTS}\")\n",
    "logger.info(f\"Models: {multi_config.MODELS}\")\n",
    "\n",
    "print(\"\\n✓ Multi-model configuration initialized\")\n",
    "print(f\"  Segments: {multi_config.SEGMENTS}\")\n",
    "print(f\"  Models: {multi_config.MODELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caf49877-544c-4903-8156-fd972aaa0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MultiModelEncoder class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 19: Multi-Model Encoder Class\n",
    "# ============================================================\n",
    "\n",
    "class MultiModelEncoder:\n",
    "    \"\"\"Handle encoding for different ML frameworks\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.label_encoders = {}\n",
    "        self.onehot_encoder = None\n",
    "        self.onehot_feature_names = []\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def fit_label_encoding(self, df):\n",
    "        \"\"\"Fit label encoders for LightGBM\"\"\"\n",
    "        self.logger.info(\"  Fitting Label Encoders for LightGBM...\")\n",
    "        \n",
    "        for col in self.categorical_features:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(df[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        self.logger.info(f\"    ✓ Fitted {len(self.label_encoders)} label encoders\")\n",
    "        return self\n",
    "    \n",
    "    def transform_label_encoding(self, df):\n",
    "        \"\"\"Apply label encoding for LightGBM\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col, encoder in self.label_encoders.items():\n",
    "            if col in df_encoded.columns:\n",
    "                df_encoded[col] = df_encoded[col].astype(str).apply(\n",
    "                    lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1\n",
    "                )\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def fit_onehot_encoding(self, df):\n",
    "        \"\"\"Fit one-hot encoder for XGBoost\"\"\"\n",
    "        self.logger.info(\"  Fitting One-Hot Encoder for XGBoost...\")\n",
    "        \n",
    "        categorical_data = df[self.categorical_features].astype(str)\n",
    "        \n",
    "        self.onehot_encoder = OneHotEncoder(\n",
    "            drop='first',\n",
    "            sparse_output=False,\n",
    "            handle_unknown='ignore'\n",
    "        )\n",
    "        \n",
    "        self.onehot_encoder.fit(categorical_data)\n",
    "        self.onehot_feature_names = self.onehot_encoder.get_feature_names_out(\n",
    "            self.categorical_features\n",
    "        ).tolist()\n",
    "        \n",
    "        self.logger.info(f\"    ✓ One-hot encoding creates {len(self.onehot_feature_names)} features\")\n",
    "        return self\n",
    "    \n",
    "    def transform_onehot_encoding(self, df):\n",
    "        \"\"\"Apply one-hot encoding for XGBoost\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        categorical_data = df_encoded[self.categorical_features].astype(str)\n",
    "        encoded_array = self.onehot_encoder.transform(categorical_data)\n",
    "        \n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded_array,\n",
    "            columns=self.onehot_feature_names,\n",
    "            index=df_encoded.index\n",
    "        )\n",
    "        \n",
    "        df_encoded = df_encoded.drop(columns=self.categorical_features)\n",
    "        df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def prepare_for_model(self, df, model_type, fit=False):\n",
    "        \"\"\"Prepare data for specific model type\"\"\"\n",
    "        \n",
    "        if model_type == 'catboost':\n",
    "            return df.copy()\n",
    "        \n",
    "        elif model_type == 'lightgbm':\n",
    "            if fit:\n",
    "                self.fit_label_encoding(df)\n",
    "            return self.transform_label_encoding(df)\n",
    "        \n",
    "        elif model_type == 'xgboost':\n",
    "            if fit:\n",
    "                self.fit_onehot_encoding(df)\n",
    "            return self.transform_onehot_encoding(df)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save encoders to file\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'label_encoders': self.label_encoders,\n",
    "                'onehot_encoder': self.onehot_encoder,\n",
    "                'onehot_feature_names': self.onehot_feature_names,\n",
    "                'categorical_features': self.categorical_features\n",
    "            }, f)\n",
    "\n",
    "print(\"✓ MultiModelEncoder class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a306300f-a260-4db8-9d6f-9fe85530283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - SEGMENT DISTRIBUTION ANALYSIS\n",
      "INFO - ============================================================\n",
      "\n",
      "============================================================\n",
      "SEGMENT DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Train:\n",
      "  SMALL_BUSINESS      : 2,024,010 records | Churn: 30,472 ( 1.51%) | Imbalance: 1:65.4\n",
      "INFO - Train - SMALL_BUSINESS: 2,024,010 records, churn rate: 0.0151\n",
      "  MIDDLE_BUSINESS     : 115,095 records | Churn:   550 ( 0.48%) | Imbalance: 1:208.3\n",
      "INFO - Train - MIDDLE_BUSINESS: 115,095 records, churn rate: 0.0048\n",
      "  LARGE_BUSINESS      :  14,302 records | Churn:    92 ( 0.64%) | Imbalance: 1:154.5\n",
      "INFO - Train - LARGE_BUSINESS: 14,302 records, churn rate: 0.0064\n",
      "\n",
      "Val:\n",
      "  SMALL_BUSINESS      : 493,087 records | Churn: 9,157 ( 1.86%) | Imbalance: 1:52.8\n",
      "INFO - Val - SMALL_BUSINESS: 493,087 records, churn rate: 0.0186\n",
      "  MIDDLE_BUSINESS     :  35,200 records | Churn:   255 ( 0.72%) | Imbalance: 1:137.0\n",
      "INFO - Val - MIDDLE_BUSINESS: 35,200 records, churn rate: 0.0072\n",
      "  LARGE_BUSINESS      :   3,593 records | Churn:    21 ( 0.58%) | Imbalance: 1:170.1\n",
      "INFO - Val - LARGE_BUSINESS: 3,593 records, churn rate: 0.0058\n",
      "\n",
      "Test:\n",
      "  SMALL_BUSINESS      : 500,370 records | Churn: 7,526 ( 1.50%) | Imbalance: 1:65.5\n",
      "INFO - Test - SMALL_BUSINESS: 500,370 records, churn rate: 0.0150\n",
      "  MIDDLE_BUSINESS     :  39,032 records | Churn:   300 ( 0.77%) | Imbalance: 1:129.1\n",
      "INFO - Test - MIDDLE_BUSINESS: 39,032 records, churn rate: 0.0077\n",
      "  LARGE_BUSINESS      :   3,500 records | Churn:    32 ( 0.91%) | Imbalance: 1:108.4\n",
      "INFO - Test - LARGE_BUSINESS: 3,500 records, churn rate: 0.0091\n",
      "\n",
      "⚠ Note: Severe class imbalance - will handle in training notebook\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 20: Segment Distribution Analysis\n",
    "# ============================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"SEGMENT DISTRIBUTION ANALYSIS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEGMENT DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze each dataset\n",
    "for dataset_name, df in [('Train', train_processed), ('Val', val_processed), ('Test', test_processed)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    \n",
    "    for segment in multi_config.SEGMENTS:\n",
    "        segment_df = df[df[multi_config.SEGMENT_COLUMN] == segment]\n",
    "        churn_count = segment_df[config.TARGET_COLUMN].sum()\n",
    "        churn_rate = segment_df[config.TARGET_COLUMN].mean()\n",
    "        imbalance = (1 - churn_rate) / churn_rate if churn_rate > 0 else 0\n",
    "        \n",
    "        print(f\"  {segment:20s}: {len(segment_df):7,} records | \"\n",
    "              f\"Churn: {churn_count:5,} ({churn_rate*100:5.2f}%) | \"\n",
    "              f\"Imbalance: 1:{imbalance:.1f}\")\n",
    "        \n",
    "        logger.info(f\"{dataset_name} - {segment}: {len(segment_df):,} records, \"\n",
    "                   f\"churn rate: {churn_rate:.4f}\")\n",
    "\n",
    "print(\"\\n⚠ Note: Severe class imbalance - will handle in training notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e64104-b79c-4ddd-a613-86ea90fa8c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - \n",
      "============================================================\n",
      "INFO - SPLITTING DATA BY SEGMENTS\n",
      "INFO - ============================================================\n",
      "INFO - \n",
      "Processing segment: SMALL_BUSINESS\n",
      "INFO -   Train: 2,024,010 records\n",
      "INFO -   Val: 493,087 records\n",
      "INFO -   Test: 500,370 records\n",
      "INFO - \n",
      "Processing segment: MIDDLE_BUSINESS\n",
      "INFO -   Train: 115,095 records\n",
      "INFO -   Val: 35,200 records\n",
      "INFO -   Test: 39,032 records\n",
      "INFO - \n",
      "Processing segment: LARGE_BUSINESS\n",
      "INFO -   Train: 14,302 records\n",
      "INFO -   Val: 3,593 records\n",
      "INFO -   Test: 3,500 records\n",
      "INFO - \n",
      "Splitting production data...\n",
      "INFO -   SMALL_BUSINESS: 191,107 prod records\n",
      "INFO -   MIDDLE_BUSINESS: 14,508 prod records\n",
      "INFO -   LARGE_BUSINESS: 1,155 prod records\n",
      "\n",
      "✓ Data split by segments: output\\segments\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 21: Split Data by Segments\n",
    "# ============================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"SPLITTING DATA BY SEGMENTS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "segment_datasets = {}\n",
    "\n",
    "for segment in multi_config.SEGMENTS:\n",
    "    logger.info(f\"\\nProcessing segment: {segment}\")\n",
    "    \n",
    "    # Filter each dataset by segment\n",
    "    segment_train = train_processed[train_processed[multi_config.SEGMENT_COLUMN] == segment].copy()\n",
    "    segment_val = val_processed[val_processed[multi_config.SEGMENT_COLUMN] == segment].copy()\n",
    "    segment_test = test_processed[test_processed[multi_config.SEGMENT_COLUMN] == segment].copy()\n",
    "    \n",
    "    segment_datasets[segment] = {\n",
    "        'train': segment_train,\n",
    "        'val': segment_val,\n",
    "        'test': segment_test\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"  Train: {len(segment_train):,} records\")\n",
    "    logger.info(f\"  Val: {len(segment_val):,} records\")\n",
    "    logger.info(f\"  Test: {len(segment_test):,} records\")\n",
    "    \n",
    "    # Save raw segment splits\n",
    "    segment_dir = multi_config.SEGMENT_DIR / segment\n",
    "    segment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    segment_train.to_csv(segment_dir / \"train_raw.csv\", index=False, sep='|')\n",
    "    segment_val.to_csv(segment_dir / \"val_raw.csv\", index=False, sep='|')\n",
    "    segment_test.to_csv(segment_dir / \"test_raw.csv\", index=False, sep='|')\n",
    "\n",
    "# Production data by segment\n",
    "if prod_processed is not None:\n",
    "    logger.info(\"\\nSplitting production data...\")\n",
    "    for segment in multi_config.SEGMENTS:\n",
    "        segment_prod = prod_processed[prod_processed[multi_config.SEGMENT_COLUMN] == segment].copy()\n",
    "        segment_datasets[segment]['prod'] = segment_prod\n",
    "        \n",
    "        segment_dir = multi_config.SEGMENT_DIR / segment\n",
    "        segment_prod.to_csv(segment_dir / \"prod_raw.csv\", index=False, sep='|')\n",
    "        logger.info(f\"  {segment}: {len(segment_prod):,} prod records\")\n",
    "\n",
    "print(f\"\\n✓ Data split by segments: {multi_config.SEGMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ff48d-cb70-48bb-a033-65ef106b9c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenvc)",
   "language": "python",
   "name": "myenvc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
