"""
Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ CSV Ð² Parquet Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸
Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð²ÑÐµ Ð½ÑŽÐ°Ð½ÑÑ‹ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…
"""
import pandas as pd
import numpy as np
from pathlib import Path
import time


def optimize_dtypes(df, categorical_features, id_columns, target_column):
    """
    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ñ‚Ð¸Ð¿Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… DataFrame (ÐºÐ°Ðº Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼ DataLoader)
    """
    print("\n  ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
    memory_before = df.memory_usage(deep=True).sum() / (1024**2)
    print(f"    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð´Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {memory_before:.2f} MB")

    # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
    for col in categorical_features:
        if col in df.columns:
            df[col] = df[col].astype('category')
            print(f"    âœ“ {col}: category ({df[col].nunique()} ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…)")

    # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
    optimized_count = 0
    for col in df.columns:
        # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ID ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸, target Ð¸ ÑƒÐ¶Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ
        if col in id_columns + [target_column] + categorical_features:
            continue

        col_type = df[col].dtype

        # Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()

            # Ð¦ÐµÐ»Ð¾Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ‹Ðµ Ñ‚Ð¸Ð¿Ñ‹
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                    optimized_count += 1
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                    optimized_count += 1
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                    optimized_count += 1

            # Ð’ÐµÑ‰ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ñ‚Ð¸Ð¿Ñ‹
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                    optimized_count += 1

    memory_after = df.memory_usage(deep=True).sum() / (1024**2)
    savings = (1 - memory_after/memory_before) * 100

    print(f"    ÐŸÐ°Ð¼ÑÑ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {memory_after:.2f} MB")
    print(f"    Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ: {savings:.1f}%")
    print(f"    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº: {optimized_count}")

    return df


def load_csv_with_original_settings(csv_path, delimiter='|', encoding='windows-1251'):
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ CSV Ñ Ñ‚ÐµÐ¼Ð¸ Ð¶Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸, Ñ‡Ñ‚Ð¾ Ð¸ Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼ DataLoader
    """
    print(f"\n{'='*60}")
    print(f"Ð—ÐÐ“Ð Ð£Ð—ÐšÐ CSV: {csv_path.name}")
    print(f"{'='*60}")

    if not csv_path.exists():
        raise FileNotFoundError(f"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {csv_path}")

    file_size = csv_path.stat().st_size / (1024**2)
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {file_size:.2f} MB")

    print(f"\n  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° CSV...")
    start_time = time.time()

    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ CSV Ñ ÐžÐ Ð˜Ð“Ð˜ÐÐÐ›Ð¬ÐÐ«ÐœÐ˜ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸
    df = pd.read_csv(
        csv_path,
        delimiter=delimiter,
        encoding=encoding,
        thousands=',',      # Ð’Ð°Ð¶Ð½Ð¾! Ð”Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸Ñ‚ÐµÐ»Ñ Ñ‚Ñ‹ÑÑÑ‡
        low_memory=False
    )

    load_time = time.time() - start_time

    # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð¼ÐµÐ½ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº (ÐºÐ°Ðº Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»Ðµ)
    df.columns = df.columns.str.lower().str.strip()

    print(f"  âœ“ CSV Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° {load_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {df.shape}")
    print(f"  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸: {len(df.columns)}")

    return df, load_time


def save_to_parquet(df, parquet_path, compression='snappy'):
    """
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ DataFrame Ð² Parquet Ñ ÑÐ¶Ð°Ñ‚Ð¸ÐµÐ¼

    ÐžÐ¿Ñ†Ð¸Ð¸ ÑÐ¶Ð°Ñ‚Ð¸Ñ:
    - 'snappy': Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
    - 'gzip': Ð»ÑƒÑ‡ÑˆÐµÐµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ, Ð½Ð¾ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½ÐµÐµ
    - 'brotli': Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ, Ð¼ÐµÐ´Ð»ÐµÐ½Ð½ÐµÐµ
    - 'zstd': Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹ Ð±Ð°Ð»Ð°Ð½Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð¸ ÑÐ¶Ð°Ñ‚Ð¸Ñ
    """
    print(f"\n{'='*60}")
    print(f"Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð’ PARQUET")
    print(f"{'='*60}")
    print(f"  Ð¤Ð°Ð¹Ð»: {parquet_path.name}")
    print(f"  Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ: {compression}")

    start_time = time.time()

    df.to_parquet(
        parquet_path,
        engine='pyarrow',
        compression=compression,
        index=False
    )

    save_time = time.time() - start_time
    file_size = parquet_path.stat().st_size / (1024**2)

    print(f"  âœ“ Parquet ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð·Ð° {save_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {file_size:.2f} MB")

    return file_size, save_time


def load_parquet(parquet_path):
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Parquet Ñ„Ð°Ð¹Ð»
    """
    print(f"\n{'='*60}")
    print(f"Ð—ÐÐ“Ð Ð£Ð—ÐšÐ PARQUET: {parquet_path.name}")
    print(f"{'='*60}")

    if not parquet_path.exists():
        raise FileNotFoundError(f"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {parquet_path}")

    file_size = parquet_path.stat().st_size / (1024**2)
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð°: {file_size:.2f} MB")

    print(f"\n  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Parquet...")
    start_time = time.time()

    df = pd.read_parquet(parquet_path)

    load_time = time.time() - start_time

    print(f"  âœ“ Parquet Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð·Ð° {load_time:.2f} ÑÐµÐº")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {df.shape}")
    print(f"  ÐŸÐ°Ð¼ÑÑ‚ÑŒ: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

    return df, load_time


def convert_and_compare(csv_path, parquet_path,
                       categorical_features, id_columns, target_column,
                       delimiter='|', encoding='windows-1251',
                       compression='snappy'):
    """
    ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ»: CSV â†’ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ â†’ Parquet â†’ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾
    """
    print("\n" + "="*60)
    print("ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET Ð¡ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð•Ð™")
    print("="*60)

    # Ð¨ÐÐ“ 1: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° CSV
    df_csv, csv_load_time = load_csv_with_original_settings(
        csv_path, delimiter, encoding
    )
    csv_size = csv_path.stat().st_size / (1024**2)

    # Ð¨ÐÐ“ 2: ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…
    df_optimized = optimize_dtypes(
        df_csv, categorical_features, id_columns, target_column
    )

    # Ð¨ÐÐ“ 3: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Parquet
    parquet_size, save_time = save_to_parquet(
        df_optimized, parquet_path, compression
    )

    # Ð¨ÐÐ“ 4: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Parquet Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
    df_parquet, parquet_load_time = load_parquet(parquet_path)

    # Ð¨ÐÐ“ 5: Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ðµ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ
    print(f"\n{'='*60}")
    print("ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’ÐžÐ• Ð¡Ð ÐÐ’ÐÐ•ÐÐ˜Ð•")
    print(f"{'='*60}")

    print(f"\n{'ÐœÐµÑ‚Ñ€Ð¸ÐºÐ°':<30} {'CSV':<15} {'Parquet':<15} {'Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ':<15}")
    print("-" * 75)

    print(f"{'Ð Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð° (MB):':<30} {csv_size:<15.2f} {parquet_size:<15.2f} {csv_size/parquet_size:.1f}x Ð¼ÐµÐ½ÑŒÑˆÐµ")
    print(f"{'Ð’Ñ€ÐµÐ¼Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ (ÑÐµÐº):':<30} {csv_load_time:<15.2f} {parquet_load_time:<15.2f} {csv_load_time/parquet_load_time:.1f}x Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ")

    csv_memory = df_csv.memory_usage(deep=True).sum() / (1024**2)
    parquet_memory = df_parquet.memory_usage(deep=True).sum() / (1024**2)
    print(f"{'ÐŸÐ°Ð¼ÑÑ‚ÑŒ (MB):':<30} {csv_memory:<15.2f} {parquet_memory:<15.2f} {csv_memory/parquet_memory:.1f}x ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    print(f"\n{'ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸:':<30}")
    print(f"  Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚: {df_csv.shape == df_parquet.shape}")
    print(f"  ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÑŽÑ‚: {list(df_csv.columns) == list(df_parquet.columns)}")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚Ð¸Ð¿Ð¾Ð² ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
    print(f"\n{'ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸:':<30}")
    for col in categorical_features:
        if col in df_parquet.columns:
            print(f"  {col}: {df_parquet[col].dtype} (unique={df_parquet[col].nunique()})")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° target
    if target_column in df_parquet.columns:
        print(f"\n{'Target Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ:':<30}")
        print(f"  {df_parquet[target_column].value_counts().to_dict()}")

    return df_parquet


def main():
    """
    ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ - ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð²
    """
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¸Ð· Ð²Ð°ÑˆÐµÐ³Ð¾ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
    CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']
    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']
    TARGET_COLUMN = 'target_churn_3m'
    DELIMITER = '|'
    ENCODING = 'windows-1251'

    # Ð¤Ð°Ð¹Ð»Ñ‹ Ð´Ð»Ñ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸
    data_dir = Path("data")

    datasets = [
        # (CSV Ð¿ÑƒÑ‚ÑŒ, Parquet Ð¿ÑƒÑ‚ÑŒ, Ð¸Ð¼Ñ)
        (data_dir / "churn_train_ul.csv", data_dir / "churn_train_ul.parquet", "Training"),
        (data_dir / "churn_prod_ul.csv", data_dir / "churn_prod_ul.parquet", "Production"),
    ]

    print("\n" + "="*60)
    print("ðŸš€ ÐœÐÐ¡Ð¡ÐžÐ’ÐÐ¯ ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ CSV â†’ PARQUET")
    print("="*60)
    print(f"\nÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸:")
    print(f"  Delimiter: '{DELIMITER}'")
    print(f"  Encoding: {ENCODING}")
    print(f"  Categorical features: {CATEGORICAL_FEATURES}")
    print(f"  Compression: snappy")

    results = {}

    for csv_path, parquet_path, name in datasets:
        if not csv_path.exists():
            print(f"\nâš  ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½ {name}: Ñ„Ð°Ð¹Ð» {csv_path} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
            continue

        print(f"\n\n{'#'*60}")
        print(f"# {name.upper()} DATASET")
        print(f"{'#'*60}")

        df = convert_and_compare(
            csv_path=csv_path,
            parquet_path=parquet_path,
            categorical_features=CATEGORICAL_FEATURES,
            id_columns=ID_COLUMNS,
            target_column=TARGET_COLUMN,
            delimiter=DELIMITER,
            encoding=ENCODING,
            compression='snappy'  # ÐœÐ¾Ð¶Ð½Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð° 'gzip', 'zstd', 'brotli'
        )

        results[name] = {
            'csv': csv_path,
            'parquet': parquet_path,
            'shape': df.shape,
            'memory': df.memory_usage(deep=True).sum() / (1024**2)
        }

    # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚
    print("\n\n" + "="*60)
    print("âœ… ÐšÐžÐÐ’Ð•Ð Ð¢ÐÐ¦Ð˜Ð¯ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ")
    print("="*60)

    print(f"\n{'Dataset':<20} {'Shape':<20} {'Memory (MB)':<15} {'Parquet Ð¿ÑƒÑ‚ÑŒ':<30}")
    print("-" * 90)

    for name, info in results.items():
        print(f"{name:<20} {str(info['shape']):<20} {info['memory']:<15.2f} {info['parquet'].name:<30}")

    print("\nðŸ’¡ Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ pd.read_parquet() Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…!")
    print("   ÐŸÑ€Ð¸Ð¼ÐµÑ€: df = pd.read_parquet('data/churn_train_ul.parquet')")


if __name__ == "__main__":
    main()
