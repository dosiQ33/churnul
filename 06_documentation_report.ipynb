{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОТЧЕТ ДЛЯ ДОКУМЕНТАЦИИ МОДЕЛИ CHURN PREDICTION\n",
    "\n",
    "**Назначение**: Генерация всех таблиц, графиков и метрик для заполнения документации.\n",
    "\n",
    "**Структура**:\n",
    "1. Общая подготовка данных (preprocessing для всех сегментов)\n",
    "2. **SEGMENT 1 (Small Business)** - все разделы документации\n",
    "3. **SEGMENT 2 (Middle + Large Business)** - все разделы документации\n",
    "\n",
    "**Результаты сохраняются в**:\n",
    "- `output/` - CSV таблицы\n",
    "- `figures/` - графики\n",
    "- `models/` - обученные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ЧАСТЬ 0: ИМПОРТЫ И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "# Config\n",
    "from config import Config\n",
    "config = Config()\n",
    "\n",
    "# Создаем директории\n",
    "Path('output').mkdir(exist_ok=True)\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "\n",
    "# Стиль графиков\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Импорты и конфигурация загружены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ЧАСТЬ 1: ЗАГРУЗКА И ОБЩИЙ PREPROCESSING\n",
    "\n",
    "Этот preprocessing применяется ко ВСЕМ данным до разделения по сегментам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ЗАГРУЗКА ИСХОДНЫХ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_full = pd.read_parquet('data/churn_train_ul.parquet')\n",
    "\n",
    "print(f\"\\nИсходные данные:\")\n",
    "print(f\"  Строк: {df_full.shape[0]:,}\")\n",
    "print(f\"  Колонок: {df_full.shape[1]}\")\n",
    "print(f\"  Период: {df_full['observation_point'].min()} - {df_full['observation_point'].max()}\")\n",
    "print(f\"  Churn rate: {df_full[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Temporal Split (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEMPORAL SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_full['observation_point'] = pd.to_datetime(df_full['observation_point'])\n",
    "df_sorted = df_full.sort_values('observation_point').reset_index(drop=True)\n",
    "\n",
    "unique_dates = sorted(df_sorted['observation_point'].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "train_cutoff_idx = int(n_dates * 0.70)\n",
    "val_cutoff_idx = int(n_dates * 0.85)\n",
    "\n",
    "train_cutoff = unique_dates[train_cutoff_idx - 1]\n",
    "val_cutoff = unique_dates[val_cutoff_idx - 1]\n",
    "\n",
    "train_df = df_sorted[df_sorted['observation_point'] <= train_cutoff].copy()\n",
    "val_df = df_sorted[(df_sorted['observation_point'] > train_cutoff) & \n",
    "                    (df_sorted['observation_point'] <= val_cutoff)].copy()\n",
    "test_df = df_sorted[df_sorted['observation_point'] > val_cutoff].copy()\n",
    "\n",
    "print(f\"\\nTRAIN:\")\n",
    "print(f\"  Период: {train_df['observation_point'].min()} - {train_df['observation_point'].max()}\")\n",
    "print(f\"  Записей: {len(train_df):,}\")\n",
    "print(f\"  Churn rate: {train_df[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nVALIDATION:\")\n",
    "print(f\"  Период: {val_df['observation_point'].min()} - {val_df['observation_point'].max()}\")\n",
    "print(f\"  Записей: {len(val_df):,}\")\n",
    "print(f\"  Churn rate: {val_df[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTEST (Out-of-Time):\")\n",
    "print(f\"  Период: {test_df['observation_point'].min()} - {test_df['observation_point'].max()}\")\n",
    "print(f\"  Записей: {len(test_df):,}\")\n",
    "print(f\"  Churn rate: {test_df[config.TARGET_COLUMN].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Gap Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GAP DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def remove_gaps(df):\n",
    "    client_counts = df.groupby('cli_code')['observation_point'].count()\n",
    "    expected_months = 6\n",
    "    valid_clients = client_counts[client_counts == expected_months].index\n",
    "    df_clean = df[df['cli_code'].isin(valid_clients)].copy()\n",
    "    removed = len(df) - len(df_clean)\n",
    "    print(f\"  Удалено записей с пропусками: {removed:,}\")\n",
    "    return df_clean\n",
    "\n",
    "train_df = remove_gaps(train_df)\n",
    "val_df = remove_gaps(val_df)\n",
    "test_df = remove_gaps(test_df)\n",
    "\n",
    "print(f\"\\nПосле gap detection:\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Val: {len(val_df):,}\")\n",
    "print(f\"  Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Preprocessing Pipeline\n",
    "\n",
    "Единый preprocessing для обеспечения:\n",
    "- Статистической стабильности\n",
    "- Консистентности в продакшене\n",
    "- Сравнимости моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PreprocessingPipeline:\n",
    "    \"\"\"Полный preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.fitted_columns = None\n",
    "        self.final_features = None\n",
    "        self.constant_cols = []\n",
    "        self.outlier_bounds = {}\n",
    "        self.numeric_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numeric_cols_for_imputation = []\n",
    "        self.categorical_cols_for_imputation = []\n",
    "        self.features_to_drop_corr = []\n",
    "    \n",
    "    def fit_transform(self, train_df):\n",
    "        \"\"\"Fit and transform training data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = train_df.copy()\n",
    "        \n",
    "        self.fitted_columns = [c for c in df.columns \n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "        \n",
    "        df = self._remove_constants(df, fit=True)\n",
    "        df = self._handle_outliers(df, fit=True)\n",
    "        df = self._handle_missing(df, fit=True)\n",
    "        df = self._remove_correlations(df, fit=True)\n",
    "        \n",
    "        self.final_features = [c for c in df.columns \n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "        \n",
    "        print(f\"\\n✓ Preprocessing complete\")\n",
    "        print(f\"  Final features: {len(self.final_features)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, name=''):\n",
    "        \"\"\"Transform validation/test data\"\"\"\n",
    "        df = df.copy()\n",
    "        df = self._remove_constants(df, fit=False)\n",
    "        df = self._handle_outliers(df, fit=False)\n",
    "        df = self._handle_missing(df, fit=False)\n",
    "        df = self._remove_correlations(df, fit=False)\n",
    "        df = self._align_columns(df, name)\n",
    "        return df\n",
    "    \n",
    "    def _remove_constants(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n1. Removing constant columns...\")\n",
    "            for col in df.columns:\n",
    "                if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "                    continue\n",
    "                if df[col].nunique(dropna=False) == 1:\n",
    "                    self.constant_cols.append(col)\n",
    "            \n",
    "            if self.constant_cols:\n",
    "                df = df.drop(columns=self.constant_cols)\n",
    "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
    "            else:\n",
    "                print(f\"   ✓ No constant columns found\")\n",
    "        else:\n",
    "            df = df.drop(columns=[c for c in self.constant_cols if c in df.columns])\n",
    "        return df\n",
    "    \n",
    "    def _handle_outliers(self, df, fit):\n",
    "        if fit:\n",
    "            print(f\"\\n2. Handling outliers (IQR clipping)...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "            \n",
    "            for col in numeric:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower = Q1 - 1.5 * IQR\n",
    "                upper = Q3 + 1.5 * IQR\n",
    "                self.outlier_bounds[col] = (lower, upper)\n",
    "            \n",
    "            print(f\"   Clipped: {len(self.outlier_bounds)} columns (IQR × 1.5)\")\n",
    "        \n",
    "        for col, (lower, upper) in self.outlier_bounds.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].clip(lower, upper)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _handle_missing(self, df, fit):\n",
    "        if fit:\n",
    "            print(f\"\\n3. Handling missing values...\")\n",
    "            \n",
    "            self.numeric_cols_for_imputation = [\n",
    "                c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
    "            ]\n",
    "            \n",
    "            self.categorical_cols_for_imputation = [\n",
    "                c for c in config.CATEGORICAL_FEATURES if c in df.columns\n",
    "            ]\n",
    "            \n",
    "            if self.numeric_cols_for_imputation:\n",
    "                self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                    df[self.numeric_cols_for_imputation]\n",
    "                )\n",
    "            \n",
    "            if self.categorical_cols_for_imputation:\n",
    "                self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                    df[self.categorical_cols_for_imputation]\n",
    "                )\n",
    "            \n",
    "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, {len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        else:\n",
    "            if self.numeric_cols_for_imputation and self.numeric_imputer:\n",
    "                cols = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
    "                if cols:\n",
    "                    df[cols] = self.numeric_imputer.transform(df[cols])\n",
    "            \n",
    "            if self.categorical_cols_for_imputation and self.categorical_imputer:\n",
    "                cols = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
    "                if cols:\n",
    "                    df[cols] = self.categorical_imputer.transform(df[cols])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _remove_correlations(self, df, fit):\n",
    "        if not config.REMOVE_HIGH_CORRELATIONS:\n",
    "            return df\n",
    "        \n",
    "        if fit:\n",
    "            print(f\"\\n4. Removing high correlations (threshold={config.CORRELATION_THRESHOLD})...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "            \n",
    "            if len(numeric) > 1:\n",
    "                corr = df[numeric].corr().abs()\n",
    "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                self.features_to_drop_corr = [c for c in upper.columns \n",
    "                                             if any(upper[c] > config.CORRELATION_THRESHOLD)]\n",
    "                \n",
    "                if self.features_to_drop_corr:\n",
    "                    df = df.drop(columns=self.features_to_drop_corr)\n",
    "                    print(f\"   Removed: {len(self.features_to_drop_corr)} features\")\n",
    "                else:\n",
    "                    print(f\"   ✓ No highly correlated features found\")\n",
    "        else:\n",
    "            df = df.drop(columns=[c for c in self.features_to_drop_corr if c in df.columns])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _align_columns(self, df, name):\n",
    "        preserve = [c for c in config.ID_COLUMNS if c in df.columns]\n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            preserve.append(config.TARGET_COLUMN)\n",
    "        \n",
    "        current = [c for c in df.columns if c not in preserve]\n",
    "        missing = [c for c in self.final_features if c not in current]\n",
    "        extra = [c for c in current if c not in self.final_features]\n",
    "        \n",
    "        if missing:\n",
    "            for col in missing:\n",
    "                df[col] = 0\n",
    "        \n",
    "        if extra:\n",
    "            df = df.drop(columns=extra)\n",
    "        \n",
    "        order = preserve + self.final_features\n",
    "        df = df[[c for c in order if c in df.columns]]\n",
    "        \n",
    "        print(f\"  ✓ {name}: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"✓ PreprocessingPipeline класс определен\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Применение Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pipeline = PreprocessingPipeline(config)\n",
    "\n",
    "# Fit на train, transform на val и test\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "\n",
    "print(\"\\nPreprocessing: validation\")\n",
    "val_processed = pipeline.transform(val_df, name='validation')\n",
    "\n",
    "print(\"\\nPreprocessing: test (OOT)\")\n",
    "test_processed = pipeline.transform(test_df, name='test (OOT)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nШаги preprocessing:\")\n",
    "print(f\"  1. Константные колонки удалено: {len(pipeline.constant_cols)}\")\n",
    "print(f\"  2. Выбросы обработано (IQR clipping): {len(pipeline.outlier_bounds)} колонок\")\n",
    "print(f\"  3. Пропуски заполнено:\")\n",
    "print(f\"     - Числовых: {len(pipeline.numeric_cols_for_imputation)}\")\n",
    "print(f\"     - Категориальных: {len(pipeline.categorical_cols_for_imputation)}\")\n",
    "print(f\"  4. Коррелирующих признаков удалено: {len(pipeline.features_to_drop_corr)}\")\n",
    "print(f\"\\nИтоговое количество признаков: {len(pipeline.final_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Разделение по сегментам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SEGMENT 1: Small Business\n",
    "seg1_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "# Удаляем segment_group (одно значение) + ID + temporal\n",
    "temporal_features = ['obs_year', 'obs_month', 'obs_quarter']\n",
    "cols_to_drop_seg1 = [config.SEGMENT_COLUMN] + [c for c in config.ID_COLUMNS if c in seg1_train.columns] + temporal_features\n",
    "seg1_train = seg1_train.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_train.columns])\n",
    "seg1_val = seg1_val.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_val.columns])\n",
    "seg1_test = seg1_test.drop(columns=[c for c in cols_to_drop_seg1 if c in seg1_test.columns])\n",
    "\n",
    "print(f\"\\nSEGMENT 1: {config.SEGMENT_1_NAME}\")\n",
    "print(f\"  Train: {seg1_train.shape} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg1_val.shape} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg1_test.shape} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "# SEGMENT 2: Middle + Large Business\n",
    "seg2_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "# Оставляем segment_group + удаляем ID + temporal\n",
    "cols_to_drop_seg2 = [c for c in config.ID_COLUMNS if c in seg2_train.columns] + temporal_features\n",
    "seg2_train = seg2_train.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_train.columns])\n",
    "seg2_val = seg2_val.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_val.columns])\n",
    "seg2_test = seg2_test.drop(columns=[c for c in cols_to_drop_seg2 if c in seg2_test.columns])\n",
    "\n",
    "# Label Encoding для segment_group\n",
    "segment_mapping = {'MIDDLE_BUSINESS': 0, 'LARGE_BUSINESS': 1}\n",
    "seg2_train[config.SEGMENT_COLUMN] = seg2_train[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "seg2_val[config.SEGMENT_COLUMN] = seg2_val[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "seg2_test[config.SEGMENT_COLUMN] = seg2_test[config.SEGMENT_COLUMN].map(segment_mapping)\n",
    "\n",
    "print(f\"\\nSEGMENT 2: {config.SEGMENT_2_NAME}\")\n",
    "print(f\"  Train: {seg2_train.shape} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg2_val.shape} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg2_test.shape} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Данные готовы для моделирования\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ЧАСТЬ 2: SEGMENT 1 - SMALL BUSINESS\n",
    "\n",
    "**Все разделы документации для Segment 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Раздел 2.3 - Анализ потока (Статистика по выборке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: АНАЛИЗ ПОТОКА (Section 2.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Объединяем все splits для seg1\n",
    "seg1_full = pd.concat([seg1_train, seg1_val, seg1_test], axis=0)\n",
    "\n",
    "# Получаем периоды из исходных данных\n",
    "train_period = f\"{train_df['observation_point'].min().date()} - {train_df['observation_point'].max().date()}\"\n",
    "test_period = f\"{test_df['observation_point'].min().date()} - {test_df['observation_point'].max().date()}\"\n",
    "\n",
    "stats_seg1 = {\n",
    "    'Период выборки': f\"{df_full['observation_point'].min().date()} - {df_full['observation_point'].max().date()}\",\n",
    "    'Количество наблюдений в контрактах': f\"{len(seg1_full):,}\",\n",
    "    'Количество дефолтов в контрактах': f\"{int(seg1_full[config.TARGET_COLUMN].sum()):,}\",\n",
    "    'Уровень фактической целевой переменной': f\"{seg1_full[config.TARGET_COLUMN].mean()*100:.2f}%\"\n",
    "}\n",
    "\n",
    "print(\"\\nТаблица для документации:\")\n",
    "print(\"\\n| Наименование показателя | Значение |\")\n",
    "print(\"| :---- | :---- |\")\n",
    "for key, value in stats_seg1.items():\n",
    "    print(f\"| {key} | {value} |\")\n",
    "\n",
    "# Сохраняем в CSV\n",
    "pd.DataFrame([stats_seg1]).T.to_csv('output/seg1_flow_analysis.csv', header=['Значение'])\n",
    "print(\"\\n✓ Сохранено: output/seg1_flow_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Раздел 3.3 - Результаты сбора ABT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: СТАТИСТИКА ABT (Section 3.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Разделяем на числовые и не числовые\n",
    "numeric_cols = seg1_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_numeric_cols = seg1_full.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Убираем target из предикторов\n",
    "if config.TARGET_COLUMN in numeric_cols:\n",
    "    numeric_cols.remove(config.TARGET_COLUMN)\n",
    "\n",
    "abt_stats_seg1 = {\n",
    "    'Количество наблюдений': f\"{len(seg1_full):,}\",\n",
    "    'Количество событий': f\"{int(seg1_full[config.TARGET_COLUMN].sum()):,}\",\n",
    "    'Количество целевых переменных': '1 (target_churn_3m)',\n",
    "    'Количество числовых предикторов': len(numeric_cols),\n",
    "    'Количество не числовых предикторов': len(non_numeric_cols)\n",
    "}\n",
    "\n",
    "print(\"\\nТаблица для документации:\")\n",
    "print(\"\\n| Наименование показателя | Значение |\")\n",
    "print(\"| :---- | :---- |\")\n",
    "for key, value in abt_stats_seg1.items():\n",
    "    print(f\"| {key} | {value} |\")\n",
    "\n",
    "# Сохраняем\n",
    "pd.DataFrame([abt_stats_seg1]).T.to_csv('output/seg1_abt_statistics.csv', header=['Значение'])\n",
    "print(\"\\n✓ Сохранено: output/seg1_abt_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Раздел 3.5.1-3.5.2 - Обработка данных\n",
    "\n",
    "Информация уже представлена в общем preprocessing выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: ОБРАБОТКА ДАННЫХ (Section 3.5.1-3.5.2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n3.5.1. Удаление и замена пропущенных значений:\")\n",
    "print(\"  - Метод: Median Imputation для числовых признаков\")\n",
    "print(\"  - Метод: Most Frequent для категориальных\")\n",
    "print(f\"  - Обработано числовых: {len(pipeline.numeric_cols_for_imputation)}\")\n",
    "print(f\"  - Обработано категориальных: {len(pipeline.categorical_cols_for_imputation)}\")\n",
    "\n",
    "print(\"\\n3.5.2. Обработка категориальных значений:\")\n",
    "print(\"  - segment_group: УДАЛЕНА (только одно значение SMALL_BUSINESS)\")\n",
    "print(\"  - Временные признаки (obs_year, obs_month, obs_quarter): УДАЛЕНЫ (высокий PSI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Раздел 3.5.3 - Индекс PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_psi(expected, actual, bins=10):\n",
    "    \"\"\"Calculate Population Stability Index\"\"\"\n",
    "    combined = np.concatenate([expected, actual])\n",
    "    min_val = combined.min()\n",
    "    max_val = combined.max()\n",
    "    \n",
    "    breakpoints = np.linspace(min_val, max_val, bins + 1)\n",
    "    breakpoints[0] = -np.inf\n",
    "    breakpoints[-1] = np.inf\n",
    "    \n",
    "    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "    \n",
    "    expected_percents = expected_counts / len(expected)\n",
    "    actual_percents = actual_counts / len(actual)\n",
    "    \n",
    "    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)\n",
    "    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)\n",
    "    \n",
    "    psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n",
    "    psi = np.sum(psi_values)\n",
    "    \n",
    "    return psi\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: PSI ANALYSIS (Section 3.5.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Только числовые признаки\n",
    "numeric_features = [c for c in seg1_train.columns \n",
    "                   if c != config.TARGET_COLUMN and seg1_train[c].dtype in [np.number]]\n",
    "\n",
    "psi_results_seg1 = []\n",
    "for col in numeric_features:\n",
    "    psi = calculate_psi(seg1_train[col].values, seg1_test[col].values)\n",
    "    psi_results_seg1.append({'feature': col, 'PSI': psi})\n",
    "\n",
    "psi_df_seg1 = pd.DataFrame(psi_results_seg1).sort_values('PSI', ascending=False)\n",
    "\n",
    "# Категории PSI\n",
    "stable = (psi_df_seg1['PSI'] < 0.1).sum()\n",
    "moderate = ((psi_df_seg1['PSI'] >= 0.1) & (psi_df_seg1['PSI'] < 0.2)).sum()\n",
    "high = (psi_df_seg1['PSI'] >= 0.2).sum()\n",
    "\n",
    "print(f\"\\nОбщая статистика PSI:\")\n",
    "print(f\"  Всего признаков: {len(psi_df_seg1)}\")\n",
    "print(f\"  Стабильных (PSI < 0.1): {stable} ({stable/len(psi_df_seg1)*100:.1f}%)\")\n",
    "print(f\"  Умеренный drift (0.1-0.2): {moderate} ({moderate/len(psi_df_seg1)*100:.1f}%)\")\n",
    "print(f\"  Высокий drift (PSI > 0.2): {high} ({high/len(psi_df_seg1)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nТОП-10 признаков с наибольшим PSI:\")\n",
    "print(psi_df_seg1.head(10).to_string(index=False))\n",
    "\n",
    "# Сохраняем\n",
    "psi_df_seg1.to_csv('output/seg1_psi_analysis.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg1_psi_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Раздел 3.5.4 - Корреляционный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: КОРРЕЛЯЦИОННЫЙ АНАЛИЗ (Section 3.5.4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Point-biserial корреляция с target\n",
    "numeric_features = [c for c in seg1_train.select_dtypes(include=[np.number]).columns \n",
    "                   if c != config.TARGET_COLUMN]\n",
    "\n",
    "correlations_seg1 = []\n",
    "for col in numeric_features:\n",
    "    corr, pval = pointbiserialr(seg1_train[config.TARGET_COLUMN], seg1_train[col])\n",
    "    correlations_seg1.append({\n",
    "        'feature': col,\n",
    "        'correlation': corr,\n",
    "        'p_value': pval,\n",
    "        'significant': pval < 0.05\n",
    "    })\n",
    "\n",
    "corr_df_seg1 = pd.DataFrame(correlations_seg1)\n",
    "corr_df_seg1['abs_correlation'] = corr_df_seg1['correlation'].abs()\n",
    "corr_df_seg1 = corr_df_seg1.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "significant = corr_df_seg1['significant'].sum()\n",
    "avg_corr = corr_df_seg1['abs_correlation'].mean()\n",
    "max_corr = corr_df_seg1['abs_correlation'].max()\n",
    "\n",
    "print(f\"\\nОбщая статистика:\")\n",
    "print(f\"  Всего признаков: {len(corr_df_seg1)}\")\n",
    "print(f\"  Значимых (p<0.05): {significant}\")\n",
    "print(f\"  Средняя |корреляция|: {avg_corr:.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {max_corr:.4f}\")\n",
    "\n",
    "print(f\"\\nТОП-20 признаков по корреляции с target:\")\n",
    "print(corr_df_seg1[['feature', 'correlation', 'p_value', 'significant']].head(20).to_string(index=False))\n",
    "\n",
    "# Сохраняем\n",
    "corr_df_seg1.to_csv('output/seg1_target_correlation.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg1_target_correlation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Раздел 4.1 - Разбиение выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: РАЗБИЕНИЕ ВЫБОРКИ (Section 4.1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "split_table_seg1 = {\n",
    "    'Роль данных': ['Train', 'Val', 'Test'],\n",
    "    'Количество наблюдений': [\n",
    "        f\"{len(seg1_train):,}\",\n",
    "        f\"{len(seg1_val):,}\",\n",
    "        f\"{len(seg1_test):,}\"\n",
    "    ],\n",
    "    'Количество событий': [\n",
    "        f\"{int(seg1_train[config.TARGET_COLUMN].sum()):,}\",\n",
    "        f\"{int(seg1_val[config.TARGET_COLUMN].sum()):,}\",\n",
    "        f\"{int(seg1_test[config.TARGET_COLUMN].sum()):,}\"\n",
    "    ],\n",
    "    'Churn Rate': [\n",
    "        f\"{seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\",\n",
    "        f\"{seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\",\n",
    "        f\"{seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "split_df_seg1 = pd.DataFrame(split_table_seg1)\n",
    "print(\"\\nТаблица для документации:\")\n",
    "print(split_df_seg1.to_markdown(index=False))\n",
    "\n",
    "# Сохраняем\n",
    "split_df_seg1.to_csv('output/seg1_split_table.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg1_split_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Раздел 4.2-4.3 - Обучение модели и результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: ОБУЧЕНИЕ МОДЕЛИ (Section 4.2-4.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Подготовка данных\n",
    "X_train_seg1 = seg1_train.drop(columns=[config.TARGET_COLUMN])\n",
    "y_train_seg1 = seg1_train[config.TARGET_COLUMN]\n",
    "\n",
    "X_val_seg1 = seg1_val.drop(columns=[config.TARGET_COLUMN])\n",
    "y_val_seg1 = seg1_val[config.TARGET_COLUMN]\n",
    "\n",
    "X_test_seg1 = seg1_test.drop(columns=[config.TARGET_COLUMN])\n",
    "y_test_seg1 = seg1_test[config.TARGET_COLUMN]\n",
    "\n",
    "# XGBoost для Segment 1\n",
    "print(\"\\nОбучение XGBoost...\")\n",
    "model_seg1 = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "model_seg1.fit(\n",
    "    X_train_seg1, y_train_seg1,\n",
    "    eval_set=[(X_val_seg1, y_val_seg1)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Предсказания\n",
    "y_pred_proba_train_seg1 = model_seg1.predict_proba(X_train_seg1)[:, 1]\n",
    "y_pred_proba_val_seg1 = model_seg1.predict_proba(X_val_seg1)[:, 1]\n",
    "y_pred_proba_test_seg1 = model_seg1.predict_proba(X_test_seg1)[:, 1]\n",
    "\n",
    "# Метрики\n",
    "threshold_seg1 = 0.12\n",
    "\n",
    "roc_auc_train_seg1 = roc_auc_score(y_train_seg1, y_pred_proba_train_seg1)\n",
    "roc_auc_val_seg1 = roc_auc_score(y_val_seg1, y_pred_proba_val_seg1)\n",
    "roc_auc_test_seg1 = roc_auc_score(y_test_seg1, y_pred_proba_test_seg1)\n",
    "\n",
    "gini_train_seg1 = 2 * roc_auc_train_seg1 - 1\n",
    "gini_val_seg1 = 2 * roc_auc_val_seg1 - 1\n",
    "gini_test_seg1 = 2 * roc_auc_test_seg1 - 1\n",
    "\n",
    "print(f\"\\n✓ Модель обучена\")\n",
    "print(f\"\\nМетрики (XGBoost, threshold={threshold_seg1}):\")\n",
    "print(f\"  Train - ROC-AUC: {roc_auc_train_seg1:.4f}, Gini: {gini_train_seg1:.4f}\")\n",
    "print(f\"  Val   - ROC-AUC: {roc_auc_val_seg1:.4f}, Gini: {gini_val_seg1:.4f}\")\n",
    "print(f\"  Test  - ROC-AUC: {roc_auc_test_seg1:.4f}, Gini: {gini_test_seg1:.4f}\")\n",
    "\n",
    "# Сохранение модели\n",
    "with open('models/seg1_xgboost_final.pkl', 'wb') as f:\n",
    "    pickle.dump(model_seg1, f)\n",
    "print(\"\\n✓ Модель сохранена: models/seg1_xgboost_final.pkl\")\n",
    "\n",
    "# Сохраняем метрики\n",
    "metrics_seg1 = pd.DataFrame({\n",
    "    'Dataset': ['Train', 'Validation', 'Test'],\n",
    "    'ROC-AUC': [roc_auc_train_seg1, roc_auc_val_seg1, roc_auc_test_seg1],\n",
    "    'Gini': [gini_train_seg1, gini_val_seg1, gini_test_seg1]\n",
    "})\n",
    "metrics_seg1.to_csv('output/seg1_model_metrics.csv', index=False)\n",
    "print(\"✓ Метрики сохранены: output/seg1_model_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Раздел 5.2 - Важность признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: FEATURE IMPORTANCE (Section 5.2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_seg1 = pd.DataFrame({\n",
    "    'feature': X_train_seg1.columns,\n",
    "    'importance': model_seg1.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nТОП-20 признаков:\")\n",
    "print(feature_importance_seg1.head(20).to_string(index=False))\n",
    "\n",
    "# График\n",
    "plt.figure(figsize=(10, 8))\n",
    "top20 = feature_importance_seg1.head(20)\n",
    "plt.barh(range(len(top20)), top20['importance'])\n",
    "plt.yticks(range(len(top20)), top20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('SEGMENT 1: Top 20 Feature Importance (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg1_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Сохраняем\n",
    "feature_importance_seg1.to_csv('output/seg1_feature_importance.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg1_feature_importance.csv\")\n",
    "print(\"✓ График сохранен: figures/seg1_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Раздел 5.3 - SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: SHAP ANALYSIS (Section 5.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SHAP values (на валидационной выборке для скорости)\n",
    "print(\"\\nРасчет SHAP values...\")\n",
    "explainer_seg1 = shap.TreeExplainer(model_seg1)\n",
    "shap_values_seg1 = explainer_seg1.shap_values(X_val_seg1)\n",
    "\n",
    "# SHAP summary plot (importance)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_seg1, X_val_seg1, plot_type='bar', show=False, max_display=20)\n",
    "plt.title('SEGMENT 1: SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg1_shap_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# SHAP beeswarm plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_seg1, X_val_seg1, show=False, max_display=20)\n",
    "plt.title('SEGMENT 1: SHAP Summary (Beeswarm)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg1_shap_beeswarm.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n✓ SHAP графики сохранены:\")\n",
    "print(\"  - figures/seg1_shap_importance.png\")\n",
    "print(\"  - figures/seg1_shap_beeswarm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. Раздел 5.4 - Decile Analysis и Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_deciles(y_true, y_pred_proba, n_bins=10):\n",
    "    \"\"\"Calculate decile analysis with lift\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    df['decile'] = pd.qcut(df['y_pred_proba'], q=n_bins, labels=False, duplicates='drop') + 1\n",
    "    \n",
    "    decile_stats = df.groupby('decile').agg({\n",
    "        'y_true': ['count', 'sum', 'mean'],\n",
    "        'y_pred_proba': ['min', 'max', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    decile_stats.columns = ['Decile', 'Count', 'Events', 'Event_Rate', \n",
    "                            'Min_Score', 'Max_Score', 'Avg_Score']\n",
    "    \n",
    "    # Cumulative metrics\n",
    "    decile_stats = decile_stats.sort_values('Decile', ascending=False)\n",
    "    decile_stats['Cum_Events'] = decile_stats['Events'].cumsum()\n",
    "    decile_stats['Cum_Count'] = decile_stats['Count'].cumsum()\n",
    "    decile_stats['Cum_Event_Rate'] = decile_stats['Cum_Events'] / decile_stats['Cum_Count']\n",
    "    \n",
    "    # Lift\n",
    "    overall_event_rate = df['y_true'].mean()\n",
    "    decile_stats['Lift'] = decile_stats['Event_Rate'] / overall_event_rate\n",
    "    decile_stats['Cum_Lift'] = decile_stats['Cum_Event_Rate'] / overall_event_rate\n",
    "    \n",
    "    # Cumulative Gain %\n",
    "    total_events = df['y_true'].sum()\n",
    "    decile_stats['Cum_Gain_%'] = (decile_stats['Cum_Events'] / total_events) * 100\n",
    "    \n",
    "    return decile_stats.sort_values('Decile')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: DECILE ANALYSIS + LIFT (Section 5.4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train\n",
    "decile_train_seg1 = calculate_deciles(y_train_seg1, y_pred_proba_train_seg1)\n",
    "print(\"\\nDECILE ANALYSIS - TRAIN:\")\n",
    "print(decile_train_seg1.to_string(index=False))\n",
    "\n",
    "# Test\n",
    "decile_test_seg1 = calculate_deciles(y_test_seg1, y_pred_proba_test_seg1)\n",
    "print(\"\\n\\nDECILE ANALYSIS - TEST:\")\n",
    "print(decile_test_seg1.to_string(index=False))\n",
    "\n",
    "# Сохраняем\n",
    "decile_train_seg1.to_csv('output/seg1_decile_analysis_train.csv', index=False)\n",
    "decile_test_seg1.to_csv('output/seg1_decile_analysis_test.csv', index=False)\n",
    "\n",
    "# График Lift\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Lift chart\n",
    "axes[0].plot(decile_train_seg1['Decile'], decile_train_seg1['Lift'], marker='o', label='Train')\n",
    "axes[0].plot(decile_test_seg1['Decile'], decile_test_seg1['Lift'], marker='s', label='Test')\n",
    "axes[0].axhline(y=1, color='r', linestyle='--', label='Baseline')\n",
    "axes[0].set_xlabel('Decile')\n",
    "axes[0].set_ylabel('Lift')\n",
    "axes[0].set_title('SEGMENT 1: Lift by Decile')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative Gain\n",
    "axes[1].plot(decile_train_seg1['Decile'], decile_train_seg1['Cum_Gain_%'], marker='o', label='Train')\n",
    "axes[1].plot(decile_test_seg1['Decile'], decile_test_seg1['Cum_Gain_%'], marker='s', label='Test')\n",
    "axes[1].plot([1, 10], [10, 100], 'r--', label='Random')\n",
    "axes[1].set_xlabel('Decile')\n",
    "axes[1].set_ylabel('Cumulative Gain (%)')\n",
    "axes[1].set_title('SEGMENT 1: Cumulative Gain')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg1_decile_lift.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n✓ Сохранено:\")\n",
    "print(\"  - output/seg1_decile_analysis_train.csv\")\n",
    "print(\"  - output/seg1_decile_analysis_test.csv\")\n",
    "print(\"  - figures/seg1_decile_lift.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. ROC Curve для Segment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: ROC CURVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ROC curves\n",
    "fpr_train_seg1, tpr_train_seg1, _ = roc_curve(y_train_seg1, y_pred_proba_train_seg1)\n",
    "fpr_val_seg1, tpr_val_seg1, _ = roc_curve(y_val_seg1, y_pred_proba_val_seg1)\n",
    "fpr_test_seg1, tpr_test_seg1, _ = roc_curve(y_test_seg1, y_pred_proba_test_seg1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train_seg1, tpr_train_seg1, label=f'Train (AUC={roc_auc_train_seg1:.4f})', linewidth=2)\n",
    "plt.plot(fpr_val_seg1, tpr_val_seg1, label=f'Val (AUC={roc_auc_val_seg1:.4f})', linewidth=2)\n",
    "plt.plot(fpr_test_seg1, tpr_test_seg1, label=f'Test (AUC={roc_auc_test_seg1:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SEGMENT 1 (Small Business): ROC Curve - XGBoost')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg1_roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"✓ ROC кривая сохранена: figures/seg1_roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ЧАСТЬ 3: SEGMENT 2 - MIDDLE + LARGE BUSINESS\n",
    "\n",
    "**Все разделы документации для Segment 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Раздел 2.3 - Анализ потока (Статистика по выборке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: АНАЛИЗ ПОТОКА (Section 2.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "seg2_full = pd.concat([seg2_train, seg2_val, seg2_test], axis=0)\n",
    "\n",
    "stats_seg2 = {\n",
    "    'Период выборки': f\"{df_full['observation_point'].min().date()} - {df_full['observation_point'].max().date()}\",\n",
    "    'Количество наблюдений в контрактах': f\"{len(seg2_full):,}\",\n",
    "    'Количество дефолтов в контрактах': f\"{int(seg2_full[config.TARGET_COLUMN].sum()):,}\",\n",
    "    'Уровень фактической целевой переменной': f\"{seg2_full[config.TARGET_COLUMN].mean()*100:.2f}%\"\n",
    "}\n",
    "\n",
    "print(\"\\nТаблица для документации:\")\n",
    "print(\"\\n| Наименование показателя | Значение |\")\n",
    "print(\"| :---- | :---- |\")\n",
    "for key, value in stats_seg2.items():\n",
    "    print(f\"| {key} | {value} |\")\n",
    "\n",
    "pd.DataFrame([stats_seg2]).T.to_csv('output/seg2_flow_analysis.csv', header=['Значение'])\n",
    "print(\"\\n✓ Сохранено: output/seg2_flow_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Раздел 3.3 - Результаты сбора ABT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: СТАТИСТИКА ABT (Section 3.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numeric_cols = seg2_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_numeric_cols = seg2_full.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "if config.TARGET_COLUMN in numeric_cols:\n",
    "    numeric_cols.remove(config.TARGET_COLUMN)\n",
    "\n",
    "abt_stats_seg2 = {\n",
    "    'Количество наблюдений': f\"{len(seg2_full):,}\",\n",
    "    'Количество событий': f\"{int(seg2_full[config.TARGET_COLUMN].sum()):,}\",\n",
    "    'Количество целевых переменных': '1 (target_churn_3m)',\n",
    "    'Количество числовых предикторов': len(numeric_cols),\n",
    "    'Количество не числовых предикторов': len(non_numeric_cols)\n",
    "}\n",
    "\n",
    "print(\"\\nТаблица для документации:\")\n",
    "print(\"\\n| Наименование показателя | Значение |\")\n",
    "print(\"| :---- | :---- |\")\n",
    "for key, value in abt_stats_seg2.items():\n",
    "    print(f\"| {key} | {value} |\")\n",
    "\n",
    "pd.DataFrame([abt_stats_seg2]).T.to_csv('output/seg2_abt_statistics.csv', header=['Значение'])\n",
    "print(\"\\n✓ Сохранено: output/seg2_abt_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Раздел 3.5.1-3.5.2 - Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: ОБРАБОТКА ДАННЫХ (Section 3.5.1-3.5.2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n3.5.1. Удаление и замена пропущенных значений:\")\n",
    "print(\"  - Метод: Median Imputation для числовых признаков\")\n",
    "print(\"  - Метод: Most Frequent для категориальных\")\n",
    "print(f\"  - Обработано числовых: {len(pipeline.numeric_cols_for_imputation)}\")\n",
    "print(f\"  - Обработано категориальных: {len(pipeline.categorical_cols_for_imputation)}\")\n",
    "\n",
    "print(\"\\n3.5.2. Обработка категориальных значений:\")\n",
    "print(\"  - segment_group: Label Encoding (MIDDLE_BUSINESS → 0, LARGE_BUSINESS → 1)\")\n",
    "print(\"  - Временные признаки (obs_year, obs_month, obs_quarter): УДАЛЕНЫ (высокий PSI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Раздел 3.5.3 - Индекс PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: PSI ANALYSIS (Section 3.5.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numeric_features = [c for c in seg2_train.columns \n",
    "                   if c != config.TARGET_COLUMN and seg2_train[c].dtype in [np.number]]\n",
    "\n",
    "psi_results_seg2 = []\n",
    "for col in numeric_features:\n",
    "    psi = calculate_psi(seg2_train[col].values, seg2_test[col].values)\n",
    "    psi_results_seg2.append({'feature': col, 'PSI': psi})\n",
    "\n",
    "psi_df_seg2 = pd.DataFrame(psi_results_seg2).sort_values('PSI', ascending=False)\n",
    "\n",
    "stable = (psi_df_seg2['PSI'] < 0.1).sum()\n",
    "moderate = ((psi_df_seg2['PSI'] >= 0.1) & (psi_df_seg2['PSI'] < 0.2)).sum()\n",
    "high = (psi_df_seg2['PSI'] >= 0.2).sum()\n",
    "\n",
    "print(f\"\\nОбщая статистика PSI:\")\n",
    "print(f\"  Всего признаков: {len(psi_df_seg2)}\")\n",
    "print(f\"  Стабильных (PSI < 0.1): {stable} ({stable/len(psi_df_seg2)*100:.1f}%)\")\n",
    "print(f\"  Умеренный drift (0.1-0.2): {moderate} ({moderate/len(psi_df_seg2)*100:.1f}%)\")\n",
    "print(f\"  Высокий drift (PSI > 0.2): {high} ({high/len(psi_df_seg2)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nТОП-10 признаков с наибольшим PSI:\")\n",
    "print(psi_df_seg2.head(10).to_string(index=False))\n",
    "\n",
    "psi_df_seg2.to_csv('output/seg2_psi_analysis.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg2_psi_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Раздел 3.5.4 - Корреляционный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: КОРРЕЛЯЦИОННЫЙ АНАЛИЗ (Section 3.5.4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numeric_features = [c for c in seg2_train.select_dtypes(include=[np.number]).columns \n",
    "                   if c != config.TARGET_COLUMN]\n",
    "\n",
    "correlations_seg2 = []\n",
    "for col in numeric_features:\n",
    "    corr, pval = pointbiserialr(seg2_train[config.TARGET_COLUMN], seg2_train[col])\n",
    "    correlations_seg2.append({\n",
    "        'feature': col,\n",
    "        'correlation': corr,\n",
    "        'p_value': pval,\n",
    "        'significant': pval < 0.05\n",
    "    })\n",
    "\n",
    "corr_df_seg2 = pd.DataFrame(correlations_seg2)\n",
    "corr_df_seg2['abs_correlation'] = corr_df_seg2['correlation'].abs()\n",
    "corr_df_seg2 = corr_df_seg2.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "significant = corr_df_seg2['significant'].sum()\n",
    "avg_corr = corr_df_seg2['abs_correlation'].mean()\n",
    "max_corr = corr_df_seg2['abs_correlation'].max()\n",
    "\n",
    "print(f\"\\nОбщая статистика:\")\n",
    "print(f\"  Всего признаков: {len(corr_df_seg2)}\")\n",
    "print(f\"  Значимых (p<0.05): {significant}\")\n",
    "print(f\"  Средняя |корреляция|: {avg_corr:.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {max_corr:.4f}\")\n",
    "\n",
    "print(f\"\\nТОП-20 признаков по корреляции с target:\")\n",
    "print(corr_df_seg2[['feature', 'correlation', 'p_value', 'significant']].head(20).to_string(index=False))\n",
    "\n",
    "corr_df_seg2.to_csv('output/seg2_target_correlation.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg2_target_correlation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Раздел 4.1 - Разбиение выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: РАЗБИЕНИЕ ВЫБОРКИ (Section 4.1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "split_table_seg2 = {\n",
    "    'Роль данных': ['Train', 'Val', 'Test'],\n",
    "    'Количество наблюдений': [\n",
    "        f\"{len(seg2_train):,}\",\n",
    "        f\"{len(seg2_val):,}\",\n",
    "        f\"{len(seg2_test):,}\"\n",
    "    ],\n",
    "    'Количество событий': [\n",
    "        f\"{int(seg2_train[config.TARGET_COLUMN].sum()):,}\",\n",
    "        f\"{int(seg2_val[config.TARGET_COLUMN].sum()):,}\",\n",
    "        f\"{int(seg2_test[config.TARGET_COLUMN].sum()):,}\"\n",
    "    ],\n",
    "    'Churn Rate': [\n",
    "        f\"{seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\",\n",
    "        f\"{seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\",\n",
    "        f\"{seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "split_df_seg2 = pd.DataFrame(split_table_seg2)\n",
    "print(\"\\nТаблица для документации:\")\n",
    "print(split_df_seg2.to_markdown(index=False))\n",
    "\n",
    "split_df_seg2.to_csv('output/seg2_split_table.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg2_split_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Раздел 4.2-4.3 - Обучение модели и результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: ОБУЧЕНИЕ МОДЕЛИ (Section 4.2-4.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Подготовка данных\n",
    "X_train_seg2 = seg2_train.drop(columns=[config.TARGET_COLUMN])\n",
    "y_train_seg2 = seg2_train[config.TARGET_COLUMN]\n",
    "\n",
    "X_val_seg2 = seg2_val.drop(columns=[config.TARGET_COLUMN])\n",
    "y_val_seg2 = seg2_val[config.TARGET_COLUMN]\n",
    "\n",
    "X_test_seg2 = seg2_test.drop(columns=[config.TARGET_COLUMN])\n",
    "y_test_seg2 = seg2_test[config.TARGET_COLUMN]\n",
    "\n",
    "# CatBoost для Segment 2\n",
    "print(\"\\nОбучение CatBoost...\")\n",
    "model_seg2 = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    early_stopping_rounds=50,\n",
    "    use_best_model=True,\n",
    "    random_seed=42,\n",
    "    task_type='CPU',\n",
    "    verbose=False,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "model_seg2.fit(\n",
    "    X_train_seg2, y_train_seg2,\n",
    "    eval_set=(X_val_seg2, y_val_seg2),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Предсказания\n",
    "y_pred_proba_train_seg2 = model_seg2.predict_proba(X_train_seg2)[:, 1]\n",
    "y_pred_proba_val_seg2 = model_seg2.predict_proba(X_val_seg2)[:, 1]\n",
    "y_pred_proba_test_seg2 = model_seg2.predict_proba(X_test_seg2)[:, 1]\n",
    "\n",
    "# Метрики\n",
    "threshold_seg2 = 0.10\n",
    "\n",
    "roc_auc_train_seg2 = roc_auc_score(y_train_seg2, y_pred_proba_train_seg2)\n",
    "roc_auc_val_seg2 = roc_auc_score(y_val_seg2, y_pred_proba_val_seg2)\n",
    "roc_auc_test_seg2 = roc_auc_score(y_test_seg2, y_pred_proba_test_seg2)\n",
    "\n",
    "gini_train_seg2 = 2 * roc_auc_train_seg2 - 1\n",
    "gini_val_seg2 = 2 * roc_auc_val_seg2 - 1\n",
    "gini_test_seg2 = 2 * roc_auc_test_seg2 - 1\n",
    "\n",
    "print(f\"\\n✓ Модель обучена\")\n",
    "print(f\"\\nМетрики (CatBoost, threshold={threshold_seg2}):\")\n",
    "print(f\"  Train - ROC-AUC: {roc_auc_train_seg2:.4f}, Gini: {gini_train_seg2:.4f}\")\n",
    "print(f\"  Val   - ROC-AUC: {roc_auc_val_seg2:.4f}, Gini: {gini_val_seg2:.4f}\")\n",
    "print(f\"  Test  - ROC-AUC: {roc_auc_test_seg2:.4f}, Gini: {gini_test_seg2:.4f}\")\n",
    "\n",
    "# Сохранение модели\n",
    "with open('models/seg2_catboost_final.pkl', 'wb') as f:\n",
    "    pickle.dump(model_seg2, f)\n",
    "print(\"\\n✓ Модель сохранена: models/seg2_catboost_final.pkl\")\n",
    "\n",
    "# Сохраняем метрики\n",
    "metrics_seg2 = pd.DataFrame({\n",
    "    'Dataset': ['Train', 'Validation', 'Test'],\n",
    "    'ROC-AUC': [roc_auc_train_seg2, roc_auc_val_seg2, roc_auc_test_seg2],\n",
    "    'Gini': [gini_train_seg2, gini_val_seg2, gini_test_seg2]\n",
    "})\n",
    "metrics_seg2.to_csv('output/seg2_model_metrics.csv', index=False)\n",
    "print(\"✓ Метрики сохранены: output/seg2_model_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Раздел 5.2 - Важность признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: FEATURE IMPORTANCE (Section 5.2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_seg2 = pd.DataFrame({\n",
    "    'feature': X_train_seg2.columns,\n",
    "    'importance': model_seg2.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nТОП-20 признаков:\")\n",
    "print(feature_importance_seg2.head(20).to_string(index=False))\n",
    "\n",
    "# График\n",
    "plt.figure(figsize=(10, 8))\n",
    "top20 = feature_importance_seg2.head(20)\n",
    "plt.barh(range(len(top20)), top20['importance'])\n",
    "plt.yticks(range(len(top20)), top20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('SEGMENT 2: Top 20 Feature Importance (CatBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg2_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Сохраняем\n",
    "feature_importance_seg2.to_csv('output/seg2_feature_importance.csv', index=False)\n",
    "print(\"\\n✓ Сохранено: output/seg2_feature_importance.csv\")\n",
    "print(\"✓ График сохранен: figures/seg2_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. Раздел 5.3 - SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: SHAP ANALYSIS (Section 5.3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SHAP values\n",
    "print(\"\\nРасчет SHAP values...\")\n",
    "explainer_seg2 = shap.TreeExplainer(model_seg2)\n",
    "shap_values_seg2 = explainer_seg2.shap_values(X_val_seg2)\n",
    "\n",
    "# SHAP summary plot (importance)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_seg2, X_val_seg2, plot_type='bar', show=False, max_display=20)\n",
    "plt.title('SEGMENT 2: SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg2_shap_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# SHAP beeswarm plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_seg2, X_val_seg2, show=False, max_display=20)\n",
    "plt.title('SEGMENT 2: SHAP Summary (Beeswarm)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg2_shap_beeswarm.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n✓ SHAP графики сохранены:\")\n",
    "print(\"  - figures/seg2_shap_importance.png\")\n",
    "print(\"  - figures/seg2_shap_beeswarm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Раздел 5.4 - Decile Analysis и Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: DECILE ANALYSIS + LIFT (Section 5.4)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train\n",
    "decile_train_seg2 = calculate_deciles(y_train_seg2, y_pred_proba_train_seg2)\n",
    "print(\"\\nDECILE ANALYSIS - TRAIN:\")\n",
    "print(decile_train_seg2.to_string(index=False))\n",
    "\n",
    "# Test\n",
    "decile_test_seg2 = calculate_deciles(y_test_seg2, y_pred_proba_test_seg2)\n",
    "print(\"\\n\\nDECILE ANALYSIS - TEST:\")\n",
    "print(decile_test_seg2.to_string(index=False))\n",
    "\n",
    "# Сохраняем\n",
    "decile_train_seg2.to_csv('output/seg2_decile_analysis_train.csv', index=False)\n",
    "decile_test_seg2.to_csv('output/seg2_decile_analysis_test.csv', index=False)\n",
    "\n",
    "# График Lift\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Lift chart\n",
    "axes[0].plot(decile_train_seg2['Decile'], decile_train_seg2['Lift'], marker='o', label='Train')\n",
    "axes[0].plot(decile_test_seg2['Decile'], decile_test_seg2['Lift'], marker='s', label='Test')\n",
    "axes[0].axhline(y=1, color='r', linestyle='--', label='Baseline')\n",
    "axes[0].set_xlabel('Decile')\n",
    "axes[0].set_ylabel('Lift')\n",
    "axes[0].set_title('SEGMENT 2: Lift by Decile')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative Gain\n",
    "axes[1].plot(decile_train_seg2['Decile'], decile_train_seg2['Cum_Gain_%'], marker='o', label='Train')\n",
    "axes[1].plot(decile_test_seg2['Decile'], decile_test_seg2['Cum_Gain_%'], marker='s', label='Test')\n",
    "axes[1].plot([1, 10], [10, 100], 'r--', label='Random')\n",
    "axes[1].set_xlabel('Decile')\n",
    "axes[1].set_ylabel('Cumulative Gain (%)')\n",
    "axes[1].set_title('SEGMENT 2: Cumulative Gain')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg2_decile_lift.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n✓ Сохранено:\")\n",
    "print(\"  - output/seg2_decile_analysis_train.csv\")\n",
    "print(\"  - output/seg2_decile_analysis_test.csv\")\n",
    "print(\"  - figures/seg2_decile_lift.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11. ROC Curve для Segment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: ROC CURVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ROC curves\n",
    "fpr_train_seg2, tpr_train_seg2, _ = roc_curve(y_train_seg2, y_pred_proba_train_seg2)\n",
    "fpr_val_seg2, tpr_val_seg2, _ = roc_curve(y_val_seg2, y_pred_proba_val_seg2)\n",
    "fpr_test_seg2, tpr_test_seg2, _ = roc_curve(y_test_seg2, y_pred_proba_test_seg2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train_seg2, tpr_train_seg2, label=f'Train (AUC={roc_auc_train_seg2:.4f})', linewidth=2)\n",
    "plt.plot(fpr_val_seg2, tpr_val_seg2, label=f'Val (AUC={roc_auc_val_seg2:.4f})', linewidth=2)\n",
    "plt.plot(fpr_test_seg2, tpr_test_seg2, label=f'Test (AUC={roc_auc_test_seg2:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SEGMENT 2 (Middle + Large Business): ROC Curve - CatBoost')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/seg2_roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"✓ ROC кривая сохранена: figures/seg2_roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ИТОГОВАЯ СВОДКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ИТОГОВАЯ СВОДКА - ВСЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 1: SMALL BUSINESS (XGBoost)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nМетрики на Test:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_test_seg1:.4f}\")\n",
    "print(f\"  Gini: {gini_test_seg1:.4f}\")\n",
    "print(f\"  Threshold: {threshold_seg1}\")\n",
    "print(f\"  Churn Rate: {y_test_seg1.mean()*100:.2f}%\")\n",
    "print(f\"\\nТОП-5 важных признаков:\")\n",
    "print(feature_importance_seg1.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEGMENT 2: MIDDLE + LARGE BUSINESS (CatBoost)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nМетрики на Test:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_test_seg2:.4f}\")\n",
    "print(f\"  Gini: {gini_test_seg2:.4f}\")\n",
    "print(f\"  Threshold: {threshold_seg2}\")\n",
    "print(f\"  Churn Rate: {y_test_seg2.mean()*100:.2f}%\")\n",
    "print(f\"\\nТОП-5 важных признаков:\")\n",
    "print(feature_importance_seg2.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕННЫЕ ФАЙЛЫ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCSV таблицы (output/):\")\n",
    "csv_files = [\n",
    "    'seg1_flow_analysis.csv',\n",
    "    'seg1_abt_statistics.csv',\n",
    "    'seg1_psi_analysis.csv',\n",
    "    'seg1_target_correlation.csv',\n",
    "    'seg1_split_table.csv',\n",
    "    'seg1_model_metrics.csv',\n",
    "    'seg1_feature_importance.csv',\n",
    "    'seg1_decile_analysis_train.csv',\n",
    "    'seg1_decile_analysis_test.csv',\n",
    "    'seg2_flow_analysis.csv',\n",
    "    'seg2_abt_statistics.csv',\n",
    "    'seg2_psi_analysis.csv',\n",
    "    'seg2_target_correlation.csv',\n",
    "    'seg2_split_table.csv',\n",
    "    'seg2_model_metrics.csv',\n",
    "    'seg2_feature_importance.csv',\n",
    "    'seg2_decile_analysis_train.csv',\n",
    "    'seg2_decile_analysis_test.csv'\n",
    "]\n",
    "for f in csv_files:\n",
    "    print(f\"  ✓ {f}\")\n",
    "\n",
    "print(\"\\nГрафики (figures/):\")\n",
    "fig_files = [\n",
    "    'seg1_feature_importance.png',\n",
    "    'seg1_shap_importance.png',\n",
    "    'seg1_shap_beeswarm.png',\n",
    "    'seg1_decile_lift.png',\n",
    "    'seg1_roc_curve.png',\n",
    "    'seg2_feature_importance.png',\n",
    "    'seg2_shap_importance.png',\n",
    "    'seg2_shap_beeswarm.png',\n",
    "    'seg2_decile_lift.png',\n",
    "    'seg2_roc_curve.png'\n",
    "]\n",
    "for f in fig_files:\n",
    "    print(f\"  ✓ {f}\")\n",
    "\n",
    "print(\"\\nМодели (models/):\")\n",
    "print(\"  ✓ seg1_xgboost_final.pkl\")\n",
    "print(\"  ✓ seg2_catboost_final.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ВСЕ РЕЗУЛЬТАТЫ ГОТОВЫ ДЛЯ ДОКУМЕНТАЦИИ!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
