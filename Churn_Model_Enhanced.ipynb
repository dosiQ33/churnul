{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МОДЕЛЬ ПРОГНОЗИРОВАНИЯ ОТТОКА КЛИЕНТОВ БАНКА\n",
    "## Улучшенная версия с полным анализом\n",
    "\n",
    "===============================================================================\n",
    "\n",
    "**Дата:** 2025-01-13  \n",
    "**Версия:** 2.0 (Enhanced)  \n",
    "**Алгоритмы:** CatBoost, XGBoost, LightGBM\n",
    "\n",
    "## УЛУЧШЕНИЯ:\n",
    "1. ✅ Удаление segment_group после разделения по сегментам\n",
    "2. ✅ Сравнение 3 алгоритмов: CatBoost, XGBoost, LightGBM\n",
    "3. ✅ Анализ корреляции фичей с таргетом\n",
    "4. ✅ PSI (Population Stability Index) анализ\n",
    "5. ✅ Метрики по перцентилям (Decile Analysis, Lift, Cumulative Precision/Recall)\n",
    "6. ✅ Техники балансировки классов: Undersampling, SMOTE, Class Weights\n",
    "7. ✅ Полная документация для банка\n",
    "\n",
    "## ОСОБЕННОСТИ:\n",
    "- Две модели по сегментам:\n",
    "  * Модель 1: Малый бизнес (SMALL_BUSINESS)\n",
    "  * Модель 2: Средний + Крупный бизнес (MIDDLE + LARGE_BUSINESS)\n",
    "- Temporal Split (Train/Val/Test-OOT)\n",
    "- Полная воспроизводимость (random_seed=42)\n",
    "\n",
    "==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve,\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Балансировка классов\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHURN PREDICTION MODEL - УЛУЧШЕННЫЙ ПАЙПЛАЙН v2.0\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  XGBoost: {xgb.__version__}\")\n",
    "print(f\"  LightGBM: {lgb.__version__}\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ГЛОБАЛЬНАЯ КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация для воспроизводимости\"\"\"\n",
    "\n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # ПУТИ\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    MODEL_DIR = Path(\"models\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "\n",
    "    # ФАЙЛЫ\n",
    "    TRAIN_FILE = \"churn_train_ul.parquet\"\n",
    "    PROD_FILE = \"churn_prod_ul.parquet\"\n",
    "\n",
    "    # КОЛОНКИ\n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    SEGMENT_COLUMN = 'segment_group'\n",
    "    DATE_COLUMN = 'observation_point'\n",
    "    CATEGORICAL_FEATURES = ['obs_month', 'obs_quarter']  # УДАЛИЛИ segment_group!\n",
    "\n",
    "    # СЕГМЕНТЫ (ДВЕ МОДЕЛИ)\n",
    "    SEGMENT_1_NAME = \"Small Business\"\n",
    "    SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
    "\n",
    "    SEGMENT_2_NAME = \"Middle + Large Business\"\n",
    "    SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "\n",
    "    # ВРЕМЕННОЕ РАЗБИЕНИЕ\n",
    "    TRAIN_SIZE = 0.70\n",
    "    VAL_SIZE = 0.15\n",
    "    TEST_SIZE = 0.15\n",
    "\n",
    "    # PREPROCESSING\n",
    "    CORRELATION_THRESHOLD = 0.85\n",
    "    OUTLIER_IQR_MULTIPLIER = 1.5\n",
    "    REMOVE_GAPS = True\n",
    "    HANDLE_OUTLIERS = True\n",
    "    REMOVE_HIGH_CORRELATIONS = True\n",
    "\n",
    "    # PSI\n",
    "    PSI_BUCKETS = 10\n",
    "\n",
    "    # CATBOOST\n",
    "    CATBOOST_PARAMS = {\n",
    "        'iterations': 500,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 4,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'random_strength': 1,\n",
    "        'bagging_temperature': 1,\n",
    "        'border_count': 128,\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'early_stopping_rounds': 100,\n",
    "        'use_best_model': True,\n",
    "        'random_seed': 42,\n",
    "        'task_type': 'CPU',\n",
    "        'verbose': 100,\n",
    "        'allow_writing_files': False\n",
    "    }\n",
    "\n",
    "    # XGBOOST\n",
    "    XGBOOST_PARAMS = {\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 500,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 100,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 1\n",
    "    }\n",
    "\n",
    "    # LIGHTGBM\n",
    "    LIGHTGBM_PARAMS = {\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 500,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_samples': 100,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "    # IMBALANCE\n",
    "    THRESHOLD_METRIC = 'f1'\n",
    "\n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.MODEL_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @classmethod\n",
    "    def get_train_path(cls):\n",
    "        return cls.DATA_DIR / cls.TRAIN_FILE\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Модель 1: {config.SEGMENT_1_NAME} {config.SEGMENT_1_VALUES}\")\n",
    "print(f\"  Модель 2: {config.SEGMENT_2_NAME} {config.SEGMENT_2_VALUES}\")\n",
    "print(f\"  Split: {config.TRAIN_SIZE}/{config.VAL_SIZE}/{config.TEST_SIZE}\")\n",
    "print(f\"\\n⚠️  ВАЖНО: segment_group удален из CATEGORICAL_FEATURES после разделения!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "train_path = config.get_train_path()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Файл: {train_path}\")\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Файл не найден: {train_path}\\n\"\n",
    "        f\"Запустите: python csv_to_parquet_converter.py\"\n",
    "    )\n",
    "\n",
    "file_size = train_path.stat().st_size / (1024**2)\n",
    "print(f\"Размер: {file_size:.2f} MB\")\n",
    "\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(train_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "memory = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "print(f\"\\n✓ Загружено за {load_time:.2f} сек\")\n",
    "print(f\"  Размер: {df_full.shape}\")\n",
    "print(f\"  Память: {memory:.2f} MB\")\n",
    "\n",
    "# Целевая переменная\n",
    "churn_rate = df_full[config.TARGET_COLUMN].mean()\n",
    "print(f\"\\n  Target '{config.TARGET_COLUMN}':\")\n",
    "print(f\"    Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
    "print(f\"    Churned: {df_full[config.TARGET_COLUMN].sum():,}\")\n",
    "print(f\"    Ratio: 1:{(1-churn_rate)/churn_rate:.1f}\")\n",
    "\n",
    "# Временной диапазон\n",
    "df_full[config.DATE_COLUMN] = pd.to_datetime(df_full[config.DATE_COLUMN])\n",
    "print(f\"\\n  Период: {df_full[config.DATE_COLUMN].min().date()} - \"\n",
    "      f\"{df_full[config.DATE_COLUMN].max().date()}\")\n",
    "print(f\"  Уникальных дат: {df_full[config.DATE_COLUMN].nunique()}\")\n",
    "\n",
    "# Сегменты\n",
    "print(f\"\\n  Распределение по сегментам:\")\n",
    "for segment, count in df_full[config.SEGMENT_COLUMN].value_counts().items():\n",
    "    pct = count / len(df_full) * 100\n",
    "    churn_seg = df_full[df_full[config.SEGMENT_COLUMN]==segment][config.TARGET_COLUMN].mean()\n",
    "    print(f\"    {segment}: {count:,} ({pct:.1f}%) | Churn: {churn_seg*100:.2f}%\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ КАЧЕСТВА ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ КАЧЕСТВА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Пропуски\n",
    "missing = df_full.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df_full) * 100).round(2)\n",
    "}).sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"\\n1. Пропущенные значения:\")\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"   {len(missing_df)} колонок с пропусками:\")\n",
    "    print(missing_df.head(10).to_string())\n",
    "else:\n",
    "    print(\"   ✓ Нет пропусков\")\n",
    "\n",
    "# Константы\n",
    "constant_cols = [col for col in df_full.columns if df_full[col].nunique() == 1]\n",
    "print(f\"\\n2. Константные колонки: {len(constant_cols)}\")\n",
    "if constant_cols:\n",
    "    print(f\"   {constant_cols[:5]}...\")\n",
    "\n",
    "# Дубликаты\n",
    "n_dups = df_full.duplicated().sum()\n",
    "print(f\"\\n3. Дубликаты: {n_dups:,}\")\n",
    "\n",
    "# Типы\n",
    "print(f\"\\n4. Типы данных:\")\n",
    "for dtype, count in df_full.dtypes.value_counts().items():\n",
    "    print(f\"   {dtype}: {count}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: РАСПРЕДЕЛЕНИЕ TARGET\n",
    "# ====================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Общее\n",
    "target_dist = df_full[config.TARGET_COLUMN].value_counts()\n",
    "axes[0].bar(['No Churn', 'Churn'], [target_dist[0], target_dist[1]],\n",
    "           color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Распределение Target', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Количество')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate([target_dist[0], target_dist[1]]):\n",
    "    axes[0].text(i, v, f'{v:,}\\n({v/len(df_full)*100:.2f}%)',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# 2. По сегментам\n",
    "segment_churn = df_full.groupby([config.SEGMENT_COLUMN,\n",
    "                                  config.TARGET_COLUMN]).size().unstack(fill_value=0)\n",
    "segment_churn.plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                  color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('По сегментам', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Сегмент')\n",
    "axes[1].legend(['No Churn', 'Churn'])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Churn rate\n",
    "churn_rates = df_full.groupby(config.SEGMENT_COLUMN)[config.TARGET_COLUMN].mean() * 100\n",
    "axes[2].bar(range(len(churn_rates)), churn_rates.values,\n",
    "           color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_xticks(range(len(churn_rates)))\n",
    "axes[2].set_xticklabels(churn_rates.index, rotation=45, ha='right')\n",
    "axes[2].set_title('Churn Rate по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Churn Rate (%)')\n",
    "for i, v in enumerate(churn_rates.values):\n",
    "    axes[2].text(i, v, f'{v:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / '01_eda_target.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Сохранено: figures/01_eda_target.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ВРЕМЕННОЕ РАЗБИЕНИЕ (TRAIN / VAL / TEST-OOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# TEMPORAL SPLIT\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сортировка по времени\n",
    "df_sorted = df_full.sort_values(config.DATE_COLUMN).reset_index(drop=True)\n",
    "unique_dates = sorted(df_sorted[config.DATE_COLUMN].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "print(f\"\\nУникальных дат: {n_dates}\")\n",
    "print(f\"Период: {unique_dates[0].date()} - {unique_dates[-1].date()}\")\n",
    "\n",
    "# Cutoff indices\n",
    "train_cutoff = int(n_dates * config.TRAIN_SIZE)\n",
    "val_cutoff = int(n_dates * (config.TRAIN_SIZE + config.VAL_SIZE))\n",
    "\n",
    "train_end = unique_dates[train_cutoff - 1]\n",
    "val_end = unique_dates[val_cutoff - 1]\n",
    "\n",
    "print(f\"\\nCutoff даты:\")\n",
    "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
    "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
    "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
    "\n",
    "# Создание split\n",
    "train_df = df_sorted[df_sorted[config.DATE_COLUMN] <= train_end].copy()\n",
    "val_df = df_sorted[(df_sorted[config.DATE_COLUMN] > train_end) &\n",
    "                   (df_sorted[config.DATE_COLUMN] <= val_end)].copy()\n",
    "test_df = df_sorted[df_sorted[config.DATE_COLUMN] > val_end].copy()\n",
    "\n",
    "# Stats\n",
    "for name, df in [('TRAIN', train_df), ('VAL', val_df), ('TEST (OOT)', test_df)]:\n",
    "    churn_r = df[config.TARGET_COLUMN].mean()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Записей: {len(df):,}\")\n",
    "    print(f\"  Клиентов: {df['cli_code'].nunique():,}\")\n",
    "    print(f\"  Период: {df[config.DATE_COLUMN].min().date()} - {df[config.DATE_COLUMN].max().date()}\")\n",
    "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
    "\n",
    "# Проверка leakage\n",
    "assert train_df[config.DATE_COLUMN].max() < val_df[config.DATE_COLUMN].min()\n",
    "assert val_df[config.DATE_COLUMN].max() < test_df[config.DATE_COLUMN].min()\n",
    "print(\"\\n✓ Temporal ordering verified - no data leakage\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# GAP REMOVAL\n",
    "# ====================================================================================\n",
    "\n",
    "if config.REMOVE_GAPS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nАнализ пробелов в train...\")\n",
    "\n",
    "    # Chunked processing\n",
    "    unique_clients = train_df['cli_code'].unique()\n",
    "    chunk_size = 10000\n",
    "    clients_with_gaps_list = []\n",
    "\n",
    "    for i in range(0, len(unique_clients), chunk_size):\n",
    "        chunk_clients = unique_clients[i:i+chunk_size]\n",
    "        chunk = train_df[train_df['cli_code'].isin(chunk_clients)].copy()\n",
    "        chunk = chunk.sort_values(['cli_code', config.DATE_COLUMN])\n",
    "\n",
    "        chunk['month_num'] = chunk[config.DATE_COLUMN].dt.to_period('M').apply(lambda x: x.ordinal)\n",
    "        chunk['month_diff'] = chunk.groupby('cli_code')['month_num'].diff()\n",
    "\n",
    "        gaps = chunk.groupby('cli_code')['month_diff'].agg([\n",
    "            ('max_gap', 'max'),\n",
    "            ('total_gaps', lambda x: (x > 1).sum())\n",
    "        ]).reset_index()\n",
    "\n",
    "        chunk_gaps = gaps[gaps['max_gap'] > 1]\n",
    "        clients_with_gaps_list.append(chunk_gaps)\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "            print(f\"  Обработано {i+chunk_size:,}/{len(unique_clients):,} клиентов\")\n",
    "\n",
    "    clients_with_gaps = pd.concat(clients_with_gaps_list, ignore_index=True)\n",
    "\n",
    "    gap_pct = len(clients_with_gaps) / len(unique_clients) * 100\n",
    "    print(f\"\\nКлиентов с пробелами: {len(clients_with_gaps):,} ({gap_pct:.2f}%)\")\n",
    "\n",
    "    if len(clients_with_gaps) > 0:\n",
    "        bad_clients = set(clients_with_gaps['cli_code'])\n",
    "\n",
    "        train_before = len(train_df)\n",
    "        train_df = train_df[~train_df['cli_code'].isin(bad_clients)].copy()\n",
    "        val_df = val_df[~val_df['cli_code'].isin(bad_clients)].copy()\n",
    "        test_df = test_df[~test_df['cli_code'].isin(bad_clients)].copy()\n",
    "\n",
    "        print(f\"\\nУдалено:\")\n",
    "        print(f\"  Train: {train_before:,} → {len(train_df):,}\")\n",
    "        print(f\"  Val: {len(val_df):,}\")\n",
    "        print(f\"  Test: {len(test_df):,}\")\n",
    "\n",
    "        del clients_with_gaps, bad_clients\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. НОВОЕ: АНАЛИЗ КОРРЕЛЯЦИИ С ТАРГЕТОМ\n",
    "\n",
    "Важно для документации банка (раздел 3.5.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОРРЕЛЯЦИЯ С ТАРГЕТОМ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ КОРРЕЛЯЦИИ ПРИЗНАКОВ С ТАРГЕТОМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Только числовые признаки\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "print(f\"\\nЧисловых признаков: {len(numeric_cols)}\")\n",
    "\n",
    "# Расчет корреляции\n",
    "correlations = []\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        corr = train_df[[col, config.TARGET_COLUMN]].corr().iloc[0, 1]\n",
    "        correlations.append({'feature': col, 'correlation': corr})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "corr_df = pd.DataFrame(correlations)\n",
    "corr_df['abs_correlation'] = corr_df['correlation'].abs()\n",
    "corr_df = corr_df.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "print(f\"\\nТОП-20 признаков по корреляции с таргетом:\")\n",
    "print(corr_df.head(20)[['feature', 'correlation']].to_string(index=False))\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 20 positive\n",
    "top_positive = corr_df.nlargest(20, 'correlation')\n",
    "axes[0].barh(range(len(top_positive)), top_positive['correlation'].values, color='green', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_positive)))\n",
    "axes[0].set_yticklabels(top_positive['feature'].values, fontsize=8)\n",
    "axes[0].set_xlabel('Correlation')\n",
    "axes[0].set_title('ТОП-20 Положительных Корреляций с Target', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Top 20 negative\n",
    "top_negative = corr_df.nsmallest(20, 'correlation')\n",
    "axes[1].barh(range(len(top_negative)), top_negative['correlation'].values, color='red', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(top_negative)))\n",
    "axes[1].set_yticklabels(top_negative['feature'].values, fontsize=8)\n",
    "axes[1].set_xlabel('Correlation')\n",
    "axes[1].set_title('ТОП-20 Отрицательных Корреляций с Target', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / '02_correlation_with_target.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Сохранено: figures/02_correlation_with_target.png\")\n",
    "\n",
    "# Сохранить для отчета\n",
    "corr_df.to_csv(config.OUTPUT_DIR / 'feature_target_correlations.csv', index=False)\n",
    "print(\"✓ Сохранено: output/feature_target_correlations.csv\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# PREPROCESSING PIPELINE\n",
    "# ====================================================================================\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"Preprocessing pipeline для всех моделей\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.fitted_columns = None\n",
    "        self.final_features = None\n",
    "        self.constant_cols = []\n",
    "        self.outlier_bounds = {}\n",
    "        self.numeric_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numeric_cols_for_imputation = []\n",
    "        self.categorical_cols_for_imputation = []\n",
    "        self.features_to_drop_corr = []\n",
    "\n",
    "    def fit_transform(self, train_df):\n",
    "        \"\"\"Fit and transform training data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        df = train_df.copy()\n",
    "\n",
    "        # Store columns\n",
    "        self.fitted_columns = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        # 1. Remove constants\n",
    "        df = self._remove_constants(df, fit=True)\n",
    "\n",
    "        # 2. Handle outliers\n",
    "        df = self._handle_outliers(df, fit=True)\n",
    "\n",
    "        # 3. Handle missing\n",
    "        df = self._handle_missing(df, fit=True)\n",
    "\n",
    "        # 4. Remove correlations\n",
    "        df = self._remove_correlations(df, fit=True)\n",
    "\n",
    "        # Final features\n",
    "        self.final_features = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        print(f\"\\n✓ Preprocessing complete\")\n",
    "        print(f\"  Features: {len(self.final_features)}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, dataset_name='test'):\n",
    "        \"\"\"Transform new data\"\"\"\n",
    "        print(f\"\\nPreprocessing: {dataset_name}\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        df = self._remove_constants(df, fit=False)\n",
    "        df = self._handle_outliers(df, fit=False)\n",
    "        df = self._handle_missing(df, fit=False)\n",
    "        df = self._remove_correlations(df, fit=False)\n",
    "        df = self._align_columns(df, dataset_name)\n",
    "\n",
    "        print(f\"  ✓ {dataset_name}: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_constants(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n1. Removing constant columns...\")\n",
    "            for col in df.columns:\n",
    "                if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "                    continue\n",
    "                if df[col].nunique(dropna=False) == 1:\n",
    "                    self.constant_cols.append(col)\n",
    "\n",
    "            if self.constant_cols:\n",
    "                df = df.drop(columns=self.constant_cols)\n",
    "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
    "        return df\n",
    "\n",
    "    def _handle_outliers(self, df, fit):\n",
    "        if not config.HANDLE_OUTLIERS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(\"\\n2. Handling outliers...\")\n",
    "            keywords = ['profit', 'income', 'expense', 'margin', 'provision',\n",
    "                       'balance', 'assets', 'liabilities']\n",
    "            cols = [c for c in df.columns\n",
    "                   if any(kw in c.lower() for kw in keywords)\n",
    "                   and c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            for col in cols:\n",
    "                if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "                    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "                    IQR = Q3 - Q1\n",
    "                    self.outlier_bounds[col] = {\n",
    "                        'lower': Q1 - config.OUTLIER_IQR_MULTIPLIER * IQR,\n",
    "                        'upper': Q3 + config.OUTLIER_IQR_MULTIPLIER * IQR\n",
    "                    }\n",
    "\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "            print(f\"   Clipped: {len(self.outlier_bounds)} columns\")\n",
    "        else:\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_missing(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n3. Handling missing values...\")\n",
    "            self.numeric_cols_for_imputation = [\n",
    "                c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
    "            ]\n",
    "            self.categorical_cols_for_imputation = [\n",
    "                c for c in config.CATEGORICAL_FEATURES if c in df.columns\n",
    "            ]\n",
    "\n",
    "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                    df[self.numeric_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                    df[self.categorical_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
    "                  f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        else:\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.numeric_imputer.transform(df[present])\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.categorical_imputer.transform(df[present])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_correlations(self, df, fit):\n",
    "        if not config.REMOVE_HIGH_CORRELATIONS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(\"\\n4. Removing high correlations...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            if len(numeric) > 1:\n",
    "                corr = df[numeric].corr().abs()\n",
    "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                self.features_to_drop_corr = [c for c in upper.columns\n",
    "                                             if any(upper[c] > config.CORRELATION_THRESHOLD)]\n",
    "\n",
    "                if self.features_to_drop_corr:\n",
    "                    df = df.drop(columns=self.features_to_drop_corr)\n",
    "                    print(f\"   Removed: {len(self.features_to_drop_corr)}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _align_columns(self, df, name):\n",
    "        preserve = [c for c in config.ID_COLUMNS if c in df.columns]\n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            preserve.append(config.TARGET_COLUMN)\n",
    "\n",
    "        current = [c for c in df.columns if c not in preserve]\n",
    "        missing = [c for c in self.final_features if c not in current]\n",
    "        extra = [c for c in current if c not in self.final_features]\n",
    "\n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                df[c] = 0\n",
    "\n",
    "        if extra:\n",
    "            df = df.drop(columns=extra)\n",
    "\n",
    "        order = preserve + self.final_features\n",
    "        df = df[[c for c in order if c in df.columns]]\n",
    "\n",
    "        return df\n",
    "\n",
    "# Apply preprocessing\n",
    "pipeline = PreprocessingPipeline(config)\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "val_processed = pipeline.transform(val_df, 'validation')\n",
    "test_processed = pipeline.transform(test_df, 'test (OOT)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\n",
    "\n",
    "**ВАЖНО:** После разделения удаляем segment_group из признаков!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SEGMENT SPLIT + УДАЛЕНИЕ SEGMENT_GROUP\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Модель 1: Small Business\n",
    "seg1_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "# УДАЛЯЕМ SEGMENT_GROUP - он больше не нужен!\n",
    "if config.SEGMENT_COLUMN in seg1_train.columns:\n",
    "    seg1_train = seg1_train.drop(columns=[config.SEGMENT_COLUMN])\n",
    "    seg1_val = seg1_val.drop(columns=[config.SEGMENT_COLUMN])\n",
    "    seg1_test = seg1_test.drop(columns=[config.SEGMENT_COLUMN])\n",
    "    print(f\"\\n✓ УДАЛЕН {config.SEGMENT_COLUMN} из модели 1 (константа внутри сегмента)\")\n",
    "\n",
    "print(f\"\\n{config.SEGMENT_1_NAME}:\")\n",
    "print(f\"  Train: {len(seg1_train):,} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg1_val):,} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg1_test):,} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "# Модель 2: Middle + Large Business\n",
    "seg2_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "# УДАЛЯЕМ SEGMENT_GROUP\n",
    "if config.SEGMENT_COLUMN in seg2_train.columns:\n",
    "    seg2_train = seg2_train.drop(columns=[config.SEGMENT_COLUMN])\n",
    "    seg2_val = seg2_val.drop(columns=[config.SEGMENT_COLUMN])\n",
    "    seg2_test = seg2_test.drop(columns=[config.SEGMENT_COLUMN])\n",
    "    print(f\"\\n✓ УДАЛЕН {config.SEGMENT_COLUMN} из модели 2\")\n",
    "\n",
    "print(f\"\\n{config.SEGMENT_2_NAME}:\")\n",
    "print(f\"  Train: {len(seg2_train):,} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg2_val):,} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg2_test):,} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ОБЪЯСНЕНИЕ: segment_group удален, так как после разделения данных\")\n",
    "print(\"он является константой внутри каждой модели и не несет информации.\")\n",
    "print(\"Он был полезен только для разделения на два сегмента.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. НОВОЕ: ФУНКЦИИ ДЛЯ PSI И МЕТРИК ПО ПЕРЦЕНТИЛЯМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ====================================================================================\n",
    "\n",
    "def calculate_psi(expected, actual, buckets=10):\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index (PSI)\n",
    "    \n",
    "    PSI < 0.1: No significant change\n",
    "    0.1 <= PSI < 0.2: Moderate change\n",
    "    PSI >= 0.2: Significant change (требуется пересмотр модели)\n",
    "    \"\"\"\n",
    "    breakpoints = np.arange(0, buckets + 1) / buckets * 100\n",
    "    breakpoints = np.percentile(expected, breakpoints)\n",
    "    breakpoints[0] = -np.inf\n",
    "    breakpoints[-1] = np.inf\n",
    "    \n",
    "    expected_percents = pd.cut(expected, breakpoints, duplicates='drop').value_counts(normalize=True).sort_index()\n",
    "    actual_percents = pd.cut(actual, breakpoints, duplicates='drop').value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    # Ensure same bins\n",
    "    expected_percents = expected_percents.reindex(actual_percents.index, fill_value=0.001)\n",
    "    actual_percents = actual_percents.reindex(expected_percents.index, fill_value=0.001)\n",
    "    \n",
    "    psi_value = np.sum((actual_percents - expected_percents) * np.log(actual_percents / expected_percents))\n",
    "    \n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_decile_table(y_true, y_pred_proba, n_deciles=10):\n",
    "    \"\"\"\n",
    "    Создать таблицу метрик по децилям (перцентилям)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame с колонками: percentile, count, target_count, target_rate, \n",
    "                           precision_cum, recall_cum, lift\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    # Сортировка по вероятности (от высокой к низкой)\n",
    "    df = df.sort_values('y_pred_proba', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Разбиение на децили\n",
    "    df['decile'] = pd.qcut(df['y_pred_proba'], q=n_deciles, labels=False, duplicates='drop') + 1\n",
    "    df['decile'] = n_deciles - df['decile'] + 1  # Reverse (1 = highest prob)\n",
    "    \n",
    "    # Агрегация по децилям\n",
    "    decile_table = df.groupby('decile').agg(\n",
    "        count=('y_true', 'size'),\n",
    "        target_count=('y_true', 'sum'),\n",
    "        min_prob=('y_pred_proba', 'min'),\n",
    "        max_prob=('y_pred_proba', 'max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    decile_table['target_rate'] = decile_table['target_count'] / decile_table['count']\n",
    "    \n",
    "    # Cumulative\n",
    "    decile_table['count_cum'] = decile_table['count'].cumsum()\n",
    "    decile_table['target_count_cum'] = decile_table['target_count'].cumsum()\n",
    "    \n",
    "    # Precision (cumulative)\n",
    "    decile_table['precision_cum'] = decile_table['target_count_cum'] / decile_table['count_cum']\n",
    "    \n",
    "    # Recall (cumulative)\n",
    "    total_targets = df['y_true'].sum()\n",
    "    decile_table['recall_cum'] = decile_table['target_count_cum'] / total_targets\n",
    "    \n",
    "    # Lift\n",
    "    baseline_rate = total_targets / len(df)\n",
    "    decile_table['lift'] = decile_table['target_rate'] / baseline_rate\n",
    "    \n",
    "    # Rename\n",
    "    decile_table = decile_table.rename(columns={'decile': 'percentile'})\n",
    "    \n",
    "    return decile_table\n",
    "\n",
    "\n",
    "def prepare_data_for_model(df, categorical_features, exclude_cols, for_catboost=False):\n",
    "    \"\"\"\n",
    "    Подготовка данных для моделей\n",
    "    \n",
    "    for_catboost=True: вернет categorical indices для CatBoost\n",
    "    for_catboost=False: закодирует категориальные как числа для XGBoost/LightGBM\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[config.TARGET_COLUMN].copy() if config.TARGET_COLUMN in df.columns else None\n",
    "    \n",
    "    if for_catboost:\n",
    "        # Конвертация категориальных в string для CatBoost\n",
    "        for cat in categorical_features:\n",
    "            if cat in X.columns:\n",
    "                X[cat] = X[cat].astype(str).replace('nan', 'missing')\n",
    "        \n",
    "        # Индексы категориальных\n",
    "        cat_indices = [i for i, c in enumerate(feature_cols) if c in categorical_features]\n",
    "        return X, y, cat_indices\n",
    "    else:\n",
    "        # Label encoding для XGBoost/LightGBM\n",
    "        encoders = {}\n",
    "        for cat in categorical_features:\n",
    "            if cat in X.columns:\n",
    "                le = LabelEncoder()\n",
    "                X[cat] = le.fit_transform(X[cat].astype(str))\n",
    "                encoders[cat] = le\n",
    "        \n",
    "        return X, y, encoders\n",
    "\n",
    "\n",
    "def calculate_class_weights(y):\n",
    "    \"\"\"Расчет весов классов\"\"\"\n",
    "    n_samples = len(y)\n",
    "    n_classes = 2\n",
    "    n_class_0 = (y == 0).sum()\n",
    "    n_class_1 = (y == 1).sum()\n",
    "\n",
    "    weight_0 = n_samples / (n_classes * n_class_0)\n",
    "    weight_1 = n_samples / (n_classes * n_class_1)\n",
    "\n",
    "    weights = np.ones(len(y))\n",
    "    weights[y == 1] = weight_1\n",
    "    weights[y == 0] = weight_0\n",
    "\n",
    "    return weights, weight_0, weight_1\n",
    "\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
    "    \"\"\"Поиск оптимального порога\"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    optimal_idx = np.argmax(scores)\n",
    "    return thresholds[optimal_idx], scores[optimal_idx]\n",
    "\n",
    "\n",
    "def calculate_all_metrics(y_true, y_pred_proba, y_pred, threshold, dataset_name=''):\n",
    "    \"\"\"Расчет всех метрик\"\"\"\n",
    "    metrics = {\n",
    "        'dataset': dataset_name,\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'] = cm[0, 0]\n",
    "    metrics['fp'] = cm[0, 1]\n",
    "    metrics['fn'] = cm[1, 0]\n",
    "    metrics['tp'] = cm[1, 1]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"\\n✓ Helper functions определены\")\n",
    "print(\"  - calculate_psi: PSI расчет\")\n",
    "print(\"  - calculate_decile_table: Метрики по перцентилям\")\n",
    "print(\"  - prepare_data_for_model: Подготовка данных для моделей\")\n",
    "print(\"  - find_optimal_threshold: Оптимальный порог\")\n",
    "print(\"  - calculate_all_metrics: Все метрики\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}