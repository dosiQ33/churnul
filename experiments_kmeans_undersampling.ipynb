{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЭКСПЕРИМЕНТЫ: K-MEANS UNDERSAMPLING ДЛЯ БАЛАНСИРОВКИ КЛАССОВ\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "**ЦЕЛЬ:** Исследовать эффективность кастомного K-Means undersampling для борьбы с дисбалансом классов\n",
    "\n",
    "**ПОДХОД:**\n",
    "- Кластеризация класса 0 (не ушедшие) через MiniBatchKMeans\n",
    "- Умный sampling: маленькие кластеры берем полностью, из больших - пропорционально\n",
    "- Целевое соотношение: 1:20 (вместо исходного ~1:65)\n",
    "- Обучение 3 алгоритмов: CatBoost, LightGBM, XGBoost\n",
    "\n",
    "**КОНТЕНТ:**\n",
    "1. Загрузка и препроцессинг (из Churn_Model_Complete.ipynb)\n",
    "2. Визуализация распределений всех признаков\n",
    "3. Кастомный K-Means undersampling (30 кластеров)\n",
    "4. Обучение моделей на balanced данных\n",
    "5. Сравнение результатов\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML - Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# ML - Models\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# ML - Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve,\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"═\"*80)\n",
    "print(\"ЭКСПЕРИМЕНТЫ: K-MEANS UNDERSAMPLING\")\n",
    "print(\"═\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Воспроизводимость\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Пути\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "MODEL_DIR = Path(\"models\")\n",
    "FIGURES_DIR = Path(\"figures\")\n",
    "\n",
    "# Создание папок\n",
    "for dir_path in [OUTPUT_DIR, MODEL_DIR, FIGURES_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Файлы\n",
    "TRAIN_FILE = \"churn_train_ul.parquet\"\n",
    "\n",
    "# Колонки\n",
    "ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "TARGET_COLUMN = 'target_churn_3m'\n",
    "SEGMENT_COLUMN = 'segment_group'\n",
    "DATE_COLUMN = 'observation_point'\n",
    "CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']\n",
    "\n",
    "# Сегменты\n",
    "SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
    "SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "\n",
    "# Временное разбиение\n",
    "TRAIN_SIZE = 0.70\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "\n",
    "# K-Means параметры\n",
    "N_CLUSTERS = 30\n",
    "TARGET_RATIO = 20  # Целевое соотношение class_0 / class_1\n",
    "KMEANS_BATCH_SIZE = 10000\n",
    "\n",
    "# Preprocessing\n",
    "CORRELATION_THRESHOLD = 0.85\n",
    "OUTLIER_IQR_MULTIPLIER = 1.5\n",
    "\n",
    "print(\"\\n✓ Конфигурация установлена\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  K-Means кластеров: {N_CLUSTERS}\")\n",
    "print(f\"  Целевое соотношение: 1:{TARGET_RATIO}\")\n",
    "print(f\"  Segment 1: {SEGMENT_1_VALUES}\")\n",
    "print(f\"  Segment 2: {SEGMENT_2_VALUES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ЗАГРУЗКА PARQUET\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "train_path = DATA_DIR / TRAIN_FILE\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"═\"*80)\n",
    "print(f\"Файл: {train_path}\")\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(f\"Файл не найден: {train_path}\")\n",
    "\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(train_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "print(f\"\\n✓ Загружено за {load_time:.2f} сек\")\n",
    "print(f\"  Размер: {df_full.shape}\")\n",
    "print(f\"  Память: {df_full.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Целевая переменная\n",
    "churn_rate = df_full[TARGET_COLUMN].mean()\n",
    "print(f\"\\n  Target '{TARGET_COLUMN}':\")\n",
    "print(f\"    Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
    "print(f\"    Churned: {df_full[TARGET_COLUMN].sum():,}\")\n",
    "print(f\"    Ratio: 1:{(1-churn_rate)/churn_rate:.1f}\")\n",
    "\n",
    "# Временной диапазон\n",
    "df_full[DATE_COLUMN] = pd.to_datetime(df_full[DATE_COLUMN])\n",
    "print(f\"\\n  Период: {df_full[DATE_COLUMN].min().date()} - {df_full[DATE_COLUMN].max().date()}\")\n",
    "print(f\"  Уникальных дат: {df_full[DATE_COLUMN].nunique()}\")\n",
    "\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ВРЕМЕННОЕ РАЗБИЕНИЕ (TRAIN / VAL / TEST-OOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TEMPORAL SPLIT\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "# Сортировка по времени\n",
    "df_sorted = df_full.sort_values(DATE_COLUMN).reset_index(drop=True)\n",
    "unique_dates = sorted(df_sorted[DATE_COLUMN].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "# Cutoff indices\n",
    "train_cutoff = int(n_dates * TRAIN_SIZE)\n",
    "val_cutoff = int(n_dates * (TRAIN_SIZE + VAL_SIZE))\n",
    "\n",
    "train_end = unique_dates[train_cutoff - 1]\n",
    "val_end = unique_dates[val_cutoff - 1]\n",
    "\n",
    "print(f\"\\nCutoff даты:\")\n",
    "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
    "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
    "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
    "\n",
    "# Создание split\n",
    "train_df = df_sorted[df_sorted[DATE_COLUMN] <= train_end].copy()\n",
    "val_df = df_sorted[(df_sorted[DATE_COLUMN] > train_end) & \n",
    "                   (df_sorted[DATE_COLUMN] <= val_end)].copy()\n",
    "test_df = df_sorted[df_sorted[DATE_COLUMN] > val_end].copy()\n",
    "\n",
    "# Stats\n",
    "for name, df in [('TRAIN', train_df), ('VAL', val_df), ('TEST (OOT)', test_df)]:\n",
    "    churn_r = df[TARGET_COLUMN].mean()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Записей: {len(df):,}\")\n",
    "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ Temporal ordering verified - no data leakage\")\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# PREPROCESSING PIPELINE\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"Preprocessing pipeline для моделей\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted_columns = None\n",
    "        self.final_features = None\n",
    "        self.constant_cols = []\n",
    "        self.outlier_bounds = {}\n",
    "        self.numeric_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numeric_cols_for_imputation = []\n",
    "        self.categorical_cols_for_imputation = []\n",
    "        self.features_to_drop_corr = []\n",
    "    \n",
    "    def fit_transform(self, train_df):\n",
    "        \"\"\"Fit and transform training data\"\"\"\n",
    "        print(\"\\n\" + \"═\"*80)\n",
    "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
    "        print(\"═\"*80)\n",
    "        \n",
    "        df = train_df.copy()\n",
    "        \n",
    "        # Store columns\n",
    "        self.fitted_columns = [c for c in df.columns \n",
    "                              if c not in ID_COLUMNS + [TARGET_COLUMN]]\n",
    "        \n",
    "        # 1. Remove constants\n",
    "        df = self._remove_constants(df, fit=True)\n",
    "        \n",
    "        # 2. Handle outliers\n",
    "        df = self._handle_outliers(df, fit=True)\n",
    "        \n",
    "        # 3. Handle missing\n",
    "        df = self._handle_missing(df, fit=True)\n",
    "        \n",
    "        # 4. Remove correlations\n",
    "        df = self._remove_correlations(df, fit=True)\n",
    "        \n",
    "        # Final features\n",
    "        self.final_features = [c for c in df.columns \n",
    "                              if c not in ID_COLUMNS + [TARGET_COLUMN]]\n",
    "        \n",
    "        print(f\"\\n✓ Preprocessing complete\")\n",
    "        print(f\"  Features: {len(self.final_features)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, dataset_name='test'):\n",
    "        \"\"\"Transform new data\"\"\"\n",
    "        print(f\"\\nPreprocessing: {dataset_name}\")\n",
    "        df = df.copy()\n",
    "        \n",
    "        df = self._remove_constants(df, fit=False)\n",
    "        df = self._handle_outliers(df, fit=False)\n",
    "        df = self._handle_missing(df, fit=False)\n",
    "        df = self._remove_correlations(df, fit=False)\n",
    "        df = self._align_columns(df, dataset_name)\n",
    "        \n",
    "        print(f\"  ✓ {dataset_name}: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def _remove_constants(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n1. Removing constant columns...\")\n",
    "            for col in df.columns:\n",
    "                if col in ID_COLUMNS + [TARGET_COLUMN]:\n",
    "                    continue\n",
    "                if df[col].nunique(dropna=False) == 1:\n",
    "                    self.constant_cols.append(col)\n",
    "            \n",
    "            if self.constant_cols:\n",
    "                df = df.drop(columns=self.constant_cols)\n",
    "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
    "        return df\n",
    "    \n",
    "    def _handle_outliers(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n2. Handling outliers...\")\n",
    "            keywords = ['profit', 'income', 'expense', 'margin', 'provision',\n",
    "                       'balance', 'assets', 'liabilities']\n",
    "            cols = [c for c in df.columns \n",
    "                   if any(kw in c.lower() for kw in keywords)\n",
    "                   and c not in ID_COLUMNS + [TARGET_COLUMN] + CATEGORICAL_FEATURES]\n",
    "            \n",
    "            for col in cols:\n",
    "                if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "                    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "                    IQR = Q3 - Q1\n",
    "                    self.outlier_bounds[col] = {\n",
    "                        'lower': Q1 - OUTLIER_IQR_MULTIPLIER * IQR,\n",
    "                        'upper': Q3 + OUTLIER_IQR_MULTIPLIER * IQR\n",
    "                    }\n",
    "            \n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "            \n",
    "            print(f\"   Clipped: {len(self.outlier_bounds)} columns\")\n",
    "        else:\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _handle_missing(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n3. Handling missing values...\")\n",
    "            self.numeric_cols_for_imputation = [\n",
    "                c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in ID_COLUMNS + [TARGET_COLUMN]\n",
    "            ]\n",
    "            self.categorical_cols_for_imputation = [\n",
    "                c for c in CATEGORICAL_FEATURES if c in df.columns\n",
    "            ]\n",
    "            \n",
    "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            \n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                    df[self.numeric_cols_for_imputation]\n",
    "                )\n",
    "            \n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                    df[self.categorical_cols_for_imputation]\n",
    "                )\n",
    "            \n",
    "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
    "                  f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        else:\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.numeric_imputer.transform(df[present])\n",
    "            \n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.categorical_imputer.transform(df[present])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _remove_correlations(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n4. Removing high correlations...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in ID_COLUMNS + [TARGET_COLUMN] + CATEGORICAL_FEATURES]\n",
    "            \n",
    "            if len(numeric) > 1:\n",
    "                corr = df[numeric].corr().abs()\n",
    "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                self.features_to_drop_corr = [c for c in upper.columns\n",
    "                                             if any(upper[c] > CORRELATION_THRESHOLD)]\n",
    "                \n",
    "                if self.features_to_drop_corr:\n",
    "                    df = df.drop(columns=self.features_to_drop_corr)\n",
    "                    print(f\"   Removed: {len(self.features_to_drop_corr)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _align_columns(self, df, name):\n",
    "        preserve = [c for c in ID_COLUMNS if c in df.columns]\n",
    "        if TARGET_COLUMN in df.columns:\n",
    "            preserve.append(TARGET_COLUMN)\n",
    "        \n",
    "        current = [c for c in df.columns if c not in preserve]\n",
    "        missing = [c for c in self.final_features if c not in current]\n",
    "        extra = [c for c in current if c not in self.final_features]\n",
    "        \n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                df[c] = 0\n",
    "        \n",
    "        if extra:\n",
    "            df = df.drop(columns=extra)\n",
    "        \n",
    "        order = preserve + self.final_features\n",
    "        df = df[[c for c in order if c in df.columns]]\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "pipeline = PreprocessingPipeline()\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "val_processed = pipeline.transform(val_df, 'validation')\n",
    "test_processed = pipeline.transform(test_df, 'test (OOT)')\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# SEGMENT SPLIT\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "# Segment 1: Small Business\n",
    "seg1_train = train_processed[train_processed[SEGMENT_COLUMN].isin(SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_processed[val_processed[SEGMENT_COLUMN].isin(SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_processed[test_processed[SEGMENT_COLUMN].isin(SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "# Для seg1: УДАЛИТЬ segment_group (одно значение - бесполезен)\n",
    "if SEGMENT_COLUMN in seg1_train.columns:\n",
    "    seg1_train = seg1_train.drop(columns=[SEGMENT_COLUMN])\n",
    "    seg1_val = seg1_val.drop(columns=[SEGMENT_COLUMN])\n",
    "    seg1_test = seg1_test.drop(columns=[SEGMENT_COLUMN])\n",
    "    print(f\"\\n✓ Для Segment 1: удален '{SEGMENT_COLUMN}' (одно значение)\")\n",
    "\n",
    "print(f\"\\nSegment 1 (SMALL_BUSINESS):\")\n",
    "print(f\"  Train: {len(seg1_train):,} | Churn: {seg1_train[TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg1_val):,} | Churn: {seg1_val[TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg1_test):,} | Churn: {seg1_test[TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "# Segment 2: Middle + Large Business\n",
    "seg2_train = train_processed[train_processed[SEGMENT_COLUMN].isin(SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_processed[val_processed[SEGMENT_COLUMN].isin(SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_processed[test_processed[SEGMENT_COLUMN].isin(SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "# Для seg2: ОСТАВИТЬ segment_group (два значения - полезен)\n",
    "print(f\"\\n✓ Для Segment 2: сохранен '{SEGMENT_COLUMN}' (два значения: {SEGMENT_2_VALUES})\")\n",
    "\n",
    "print(f\"\\nSegment 2 (MIDDLE + LARGE BUSINESS):\")\n",
    "print(f\"  Train: {len(seg2_train):,} | Churn: {seg2_train[TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg2_val):,} | Churn: {seg2_val[TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg2_test):,} | Churn: {seg2_test[TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ВИЗУАЛИЗАЦИЯ РАСПРЕДЕЛЕНИЙ ПРИЗНАКОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ФУНКЦИЯ ДЛЯ ВИЗУАЛИЗАЦИИ РАСПРЕДЕЛЕНИЙ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def plot_feature_distributions(df, segment_name, color, save_path):\n",
    "    \"\"\"\n",
    "    Визуализация распределений всех числовых признаков\n",
    "    \"\"\"\n",
    "    # Выбираем только числовые признаки (без ID и target)\n",
    "    exclude_cols = ID_COLUMNS + [TARGET_COLUMN]\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                   if c not in exclude_cols]\n",
    "    \n",
    "    n_features = len(numeric_cols)\n",
    "    \n",
    "    # Определяем размер сетки\n",
    "    n_cols = 10\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    print(f\"\\nВизуализация распределений: {segment_name}\")\n",
    "    print(f\"  Признаков: {n_features}\")\n",
    "    print(f\"  Grid: {n_rows}x{n_cols}\")\n",
    "    \n",
    "    # Создаем большой график\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(40, 35))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(tqdm(numeric_cols, desc=f\"  Plotting {segment_name}\")):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Данные\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Статистики\n",
    "        nunique = df[col].nunique()\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        skew_val = data.skew()\n",
    "        \n",
    "        # Histogram\n",
    "        ax.hist(data, bins=50, color=color, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Median line\n",
    "        ax.axvline(median_val, color='red', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "        \n",
    "        # Заголовок\n",
    "        ax.set_title(f\"{col}\\nnunique={nunique}, mean={mean_val:.2f}, median={median_val:.2f}, skew={skew_val:.2f}\",\n",
    "                    fontsize=8)\n",
    "        ax.tick_params(labelsize=6)\n",
    "        ax.legend(fontsize=6)\n",
    "    \n",
    "    # Скрываем лишние оси\n",
    "    for idx in range(n_features, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Распределения признаков: {segment_name}\", fontsize=20, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ✓ Сохранено: {save_path}\")\n",
    "    \n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ВИЗУАЛИЗАЦИЯ: SEGMENT 1\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ РАСПРЕДЕЛЕНИЙ ПРИЗНАКОВ\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "seg1_numeric_cols = plot_feature_distributions(\n",
    "    seg1_train, \n",
    "    \"Segment 1 (SMALL_BUSINESS)\",\n",
    "    'steelblue',\n",
    "    FIGURES_DIR / 'distributions_segment1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ВИЗУАЛИЗАЦИЯ: SEGMENT 2\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "seg2_numeric_cols = plot_feature_distributions(\n",
    "    seg2_train, \n",
    "    \"Segment 2 (MIDDLE + LARGE BUSINESS)\",\n",
    "    'seagreen',\n",
    "    FIGURES_DIR / 'distributions_segment2.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# СТАТИСТИКИ ПРИЗНАКОВ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_feature_statistics(df, numeric_cols, save_path):\n",
    "    \"\"\"\n",
    "    Создает таблицу со статистиками признаков\n",
    "    \"\"\"\n",
    "    stats_list = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        stats_list.append({\n",
    "            'feature_name': col,\n",
    "            'nunique': df[col].nunique(),\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'mean': data.mean(),\n",
    "            'median': data.median(),\n",
    "            'std': data.std(),\n",
    "            'skewness': data.skew(),\n",
    "            'kurtosis': data.kurtosis()\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_list)\n",
    "    stats_df = stats_df.sort_values('skewness', key=abs, ascending=False).reset_index(drop=True)\n",
    "    stats_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Статистики сохранены: {save_path}\")\n",
    "    print(f\"\\nТоп-10 признаков с наибольшей асимметрией:\")\n",
    "    print(stats_df[['feature_name', 'skewness', 'mean', 'median']].head(10).to_string(index=False))\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "\n",
    "# Segment 1\n",
    "print(\"\\n\" + \"─\"*80)\n",
    "print(\"СТАТИСТИКИ: SEGMENT 1\")\n",
    "print(\"─\"*80)\n",
    "seg1_stats = create_feature_statistics(\n",
    "    seg1_train, seg1_numeric_cols, \n",
    "    OUTPUT_DIR / 'feature_statistics_seg1.csv'\n",
    ")\n",
    "\n",
    "# Segment 2\n",
    "print(\"\\n\" + \"─\"*80)\n",
    "print(\"СТАТИСТИКИ: SEGMENT 2\")\n",
    "print(\"─\"*80)\n",
    "seg2_stats = create_feature_statistics(\n",
    "    seg2_train, seg2_numeric_cols, \n",
    "    OUTPUT_DIR / 'feature_statistics_seg2.csv'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. КАСТОМНЫЙ K-MEANS UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ФУНКЦИЯ: KMEANS UNDERSAMPLING\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def kmeans_undersampling(X_train, y_train, target_ratio=20, n_clusters=30, random_state=42):\n",
    "    \"\"\"\n",
    "    Умный undersampling через k-means кластеризацию класса 0\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : DataFrame - обучающие данные\n",
    "    y_train : Series - таргет\n",
    "    target_ratio : int - желаемое соотношение class_0 / class_1 (по умолчанию 20)\n",
    "    n_clusters : int - количество кластеров для класса 0 (по умолчанию 30)\n",
    "    random_state : int - для воспроизводимости\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_balanced : DataFrame - сбалансированные данные\n",
    "    y_balanced : Series - сбалансированный таргет\n",
    "    cluster_info : dict - информация о кластерах\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"K-MEANS UNDERSAMPLING\")\n",
    "    print(\"─\"*80)\n",
    "    \n",
    "    # Шаг 1: Разделение по классам\n",
    "    X_class_0 = X_train[y_train == 0].copy()\n",
    "    X_class_1 = X_train[y_train == 1].copy()\n",
    "    y_class_0 = y_train[y_train == 0].copy()\n",
    "    y_class_1 = y_train[y_train == 1].copy()\n",
    "    \n",
    "    n_class_1 = len(X_class_1)\n",
    "    n_class_0_original = len(X_class_0)\n",
    "    \n",
    "    print(f\"\\nИсходное распределение:\")\n",
    "    print(f\"  Class 0 (не ушли): {n_class_0_original:,}\")\n",
    "    print(f\"  Class 1 (ушли): {n_class_1:,}\")\n",
    "    print(f\"  Соотношение: 1:{n_class_0_original/n_class_1:.1f}\")\n",
    "    \n",
    "    # Шаг 2: Расчет целевого количества\n",
    "    n_class_0_target = int(n_class_1 * target_ratio)\n",
    "    \n",
    "    print(f\"\\nЦелевое распределение:\")\n",
    "    print(f\"  Class 0 (целевое): {n_class_0_target:,}\")\n",
    "    print(f\"  Class 1: {n_class_1:,}\")\n",
    "    print(f\"  Целевое соотношение: 1:{target_ratio}\")\n",
    "    \n",
    "    # Шаг 3: Стандартизация признаков\n",
    "    print(f\"\\nШаг 1: Стандартизация признаков...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_class_0_scaled = scaler.fit_transform(X_class_0)\n",
    "    print(f\"  ✓ Признаки стандартизированы (важно для k-means)\")\n",
    "    \n",
    "    # Шаг 4: K-Means кластеризация класса 0\n",
    "    print(f\"\\nШаг 2: K-Means кластеризация класса 0...\")\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        batch_size=KMEANS_BATCH_SIZE,\n",
    "        random_state=random_state,\n",
    "        verbose=0,\n",
    "        n_init=3\n",
    "    )\n",
    "    cluster_labels = kmeans.fit_predict(X_class_0_scaled)\n",
    "    print(f\"  ✓ K-Means кластеризация выполнена\")\n",
    "    print(f\"  Количество кластеров: {n_clusters}\")\n",
    "    \n",
    "    # Шаг 5: Анализ размеров кластеров\n",
    "    cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    samples_per_cluster = n_class_0_target // n_clusters\n",
    "    \n",
    "    print(f\"  Среднее желаемое из кластера: {samples_per_cluster}\")\n",
    "    print(f\"\\nРаспределение по кластерам:\")\n",
    "    print(f\"  Min размер: {cluster_sizes.min()}\")\n",
    "    print(f\"  Max размер: {cluster_sizes.max()}\")\n",
    "    print(f\"  Mean размер: {cluster_sizes.mean():.0f}\")\n",
    "    \n",
    "    # Шаг 6: УМНЫЙ SAMPLING из кластеров\n",
    "    print(f\"\\nШаг 3: Умный sampling из кластеров...\")\n",
    "    \n",
    "    sampled_indices = []\n",
    "    small_clusters_count = 0\n",
    "    \n",
    "    # Первый проход\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = (cluster_labels == cluster_id)\n",
    "        cluster_indices = np.where(cluster_mask)[0]\n",
    "        cluster_size = len(cluster_indices)\n",
    "        \n",
    "        if cluster_size <= samples_per_cluster:\n",
    "            # МАЛЕНЬКИЙ кластер - берем ВСЕ\n",
    "            selected_indices = cluster_indices\n",
    "            small_clusters_count += 1\n",
    "        else:\n",
    "            # БОЛЬШОЙ кластер - берем samples_per_cluster случайно\n",
    "            selected_indices = np.random.choice(\n",
    "                cluster_indices, \n",
    "                size=samples_per_cluster, \n",
    "                replace=False\n",
    "            )\n",
    "        \n",
    "        sampled_indices.extend(selected_indices)\n",
    "    \n",
    "    n_sampled = len(sampled_indices)\n",
    "    \n",
    "    print(f\"  Первый проход sampling:\")\n",
    "    print(f\"    Маленьких кластеров (взяты полностью): {small_clusters_count} из {n_clusters}\")\n",
    "    print(f\"    Набрано примеров: {n_sampled:,} / {n_class_0_target:,}\")\n",
    "    \n",
    "    # Если не хватает - добираем из больших кластеров\n",
    "    if n_sampled < n_class_0_target:\n",
    "        shortage = n_class_0_target - n_sampled\n",
    "        print(f\"    Нехватка: {shortage:,}\")\n",
    "        \n",
    "        # Находим большие кластеры\n",
    "        large_clusters = []\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_size = (cluster_labels == cluster_id).sum()\n",
    "            if cluster_size > samples_per_cluster:\n",
    "                large_clusters.append(cluster_id)\n",
    "        \n",
    "        # Распределяем нехватку пропорционально\n",
    "        additional_per_cluster = shortage // len(large_clusters) + 1\n",
    "        \n",
    "        for cluster_id in large_clusters:\n",
    "            if len(sampled_indices) >= n_class_0_target:\n",
    "                break\n",
    "            \n",
    "            cluster_mask = (cluster_labels == cluster_id)\n",
    "            cluster_indices = np.where(cluster_mask)[0]\n",
    "            \n",
    "            # Исключаем уже взятые\n",
    "            available = np.setdiff1d(cluster_indices, sampled_indices)\n",
    "            \n",
    "            n_to_take = min(additional_per_cluster, len(available), \n",
    "                           n_class_0_target - len(sampled_indices))\n",
    "            \n",
    "            if n_to_take > 0:\n",
    "                additional = np.random.choice(available, size=n_to_take, replace=False)\n",
    "                sampled_indices.extend(additional)\n",
    "        \n",
    "        print(f\"    Добрано дополнительно: {len(sampled_indices) - n_sampled}\")\n",
    "    \n",
    "    # Финальный набор\n",
    "    sampled_indices = np.array(sampled_indices)[:n_class_0_target]\n",
    "    \n",
    "    print(f\"\\nИтого после умного sampling:\")\n",
    "    print(f\"  Class 0 отобрано: {len(sampled_indices):,}\")\n",
    "    print(f\"  Reduction: {(1 - len(sampled_indices)/n_class_0_original)*100:.1f}%\")\n",
    "    \n",
    "    # Шаг 7: Формирование финальной выборки\n",
    "    print(f\"\\nШаг 4: Формирование финальной выборки...\")\n",
    "    \n",
    "    # Отобранные примеры класса 0\n",
    "    X_class_0_sampled = X_class_0.iloc[sampled_indices].copy()\n",
    "    y_class_0_sampled = y_class_0.iloc[sampled_indices].copy()\n",
    "    \n",
    "    # Объединяем с классом 1 (весь)\n",
    "    X_balanced = pd.concat([X_class_0_sampled, X_class_1], ignore_index=True)\n",
    "    y_balanced = pd.concat([y_class_0_sampled, y_class_1], ignore_index=True)\n",
    "    \n",
    "    # SHUFFLE (важно!)\n",
    "    shuffle_idx = np.random.RandomState(random_state).permutation(len(X_balanced))\n",
    "    X_balanced = X_balanced.iloc[shuffle_idx].reset_index(drop=True)\n",
    "    y_balanced = y_balanced.iloc[shuffle_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Финальная проверка\n",
    "    final_ratio = (y_balanced == 0).sum() / (y_balanced == 1).sum()\n",
    "    print(f\"\\nФинальное распределение:\")\n",
    "    print(f\"  Class 0: {(y_balanced == 0).sum():,}\")\n",
    "    print(f\"  Class 1: {(y_balanced == 1).sum():,}\")\n",
    "    print(f\"  Итоговое соотношение: 1:{final_ratio:.1f}\")\n",
    "    print(f\"  ✓ Данные перемешаны (shuffle)\")\n",
    "    \n",
    "    # Информация о кластерах\n",
    "    cluster_info = {\n",
    "        'n_clusters': n_clusters,\n",
    "        'small_clusters_count': small_clusters_count,\n",
    "        'cluster_sizes': cluster_sizes.to_dict(),\n",
    "        'samples_per_cluster_target': samples_per_cluster,\n",
    "        'reduction_percent': (1 - len(sampled_indices)/n_class_0_original)*100,\n",
    "        'original_class_0': n_class_0_original,\n",
    "        'sampled_class_0': len(sampled_indices),\n",
    "        'class_1': n_class_1,\n",
    "        'final_ratio': final_ratio\n",
    "    }\n",
    "    \n",
    "    print(\"─\"*80)\n",
    "    \n",
    "    # Очистка памяти\n",
    "    del X_class_0_scaled, cluster_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    return X_balanced, y_balanced, cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ПРИМЕНЕНИЕ K-MEANS UNDERSAMPLING: SEGMENT 1\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"K-MEANS UNDERSAMPLING: SEGMENT 1 (SMALL BUSINESS)\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "seg1_train_balanced, seg1_y_train_balanced, seg1_cluster_info = kmeans_undersampling(\n",
    "    X_train=seg1_train.drop(columns=[TARGET_COLUMN]),\n",
    "    y_train=seg1_train[TARGET_COLUMN],\n",
    "    target_ratio=TARGET_RATIO,\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Сохранение информации о кластерах\n",
    "with open(OUTPUT_DIR / 'cluster_info_seg1.json', 'w') as f:\n",
    "    json.dump(seg1_cluster_info, f, indent=2)\n",
    "print(f\"\\n✓ Информация о кластерах сохранена: {OUTPUT_DIR / 'cluster_info_seg1.json'}\")\n",
    "\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ПРИМЕНЕНИЕ K-MEANS UNDERSAMPLING: SEGMENT 2\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"K-MEANS UNDERSAMPLING: SEGMENT 2 (MIDDLE + LARGE BUSINESS)\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "seg2_train_balanced, seg2_y_train_balanced, seg2_cluster_info = kmeans_undersampling(\n",
    "    X_train=seg2_train.drop(columns=[TARGET_COLUMN]),\n",
    "    y_train=seg2_train[TARGET_COLUMN],\n",
    "    target_ratio=TARGET_RATIO,\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Сохранение информации о кластерах\n",
    "with open(OUTPUT_DIR / 'cluster_info_seg2.json', 'w') as f:\n",
    "    json.dump(seg2_cluster_info, f, indent=2)\n",
    "print(f\"\\n✓ Информация о кластерах сохранена: {OUTPUT_DIR / 'cluster_info_seg2.json'}\")\n",
    "\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ОБУЧЕНИЕ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# HELPER FUNCTIONS ДЛЯ ОБУЧЕНИЯ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"Поиск оптимального порога по F1\"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        score = f1_score(y_true, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    return thresholds[optimal_idx], scores[optimal_idx]\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, optimal_threshold):\n",
    "    \"\"\"Расчет всех метрик\"\"\"\n",
    "    y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'] = cm[0, 0]\n",
    "    metrics['fp'] = cm[0, 1]\n",
    "    metrics['fn'] = cm[1, 0]\n",
    "    metrics['tp'] = cm[1, 1]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ОБУЧЕНИЕ МОДЕЛЕЙ: ОСНОВНАЯ ФУНКЦИЯ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def train_all_models(seg_name, seg_train_balanced, seg_y_train_balanced, \n",
    "                     seg_val, seg_test, categorical_features):\n",
    "    \"\"\"\n",
    "    Обучение 3 алгоритмов: CatBoost, LightGBM, XGBoost\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    trained_models = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train = seg_train_balanced\n",
    "    y_train = seg_y_train_balanced\n",
    "    \n",
    "    X_val = seg_val.drop(columns=[TARGET_COLUMN])\n",
    "    y_val = seg_val[TARGET_COLUMN]\n",
    "    \n",
    "    X_test = seg_test.drop(columns=[TARGET_COLUMN])\n",
    "    y_test = seg_test[TARGET_COLUMN]\n",
    "    \n",
    "    print(f\"\\nДанные:\")\n",
    "    print(f\"  Train (balanced): {X_train.shape}\")\n",
    "    print(f\"  Val (original): {X_val.shape}\")\n",
    "    print(f\"  Test (original): {X_test.shape}\")\n",
    "    \n",
    "    # Категориальные признаки (фильтруем те, которые есть)\n",
    "    cat_features_present = [c for c in categorical_features if c in X_train.columns]\n",
    "    print(f\"  Категориальные: {cat_features_present}\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    # 1. CATBOOST\n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    \n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"ОБУЧЕНИЕ: CatBoost\")\n",
    "    print(\"─\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Подготовка данных для CatBoost\n",
    "    X_train_cat = X_train.copy()\n",
    "    X_val_cat = X_val.copy()\n",
    "    X_test_cat = X_test.copy()\n",
    "    \n",
    "    for cat in cat_features_present:\n",
    "        X_train_cat[cat] = X_train_cat[cat].astype(str)\n",
    "        X_val_cat[cat] = X_val_cat[cat].astype(str)\n",
    "        X_test_cat[cat] = X_test_cat[cat].astype(str)\n",
    "    \n",
    "    cat_indices = [i for i, c in enumerate(X_train_cat.columns) if c in cat_features_present]\n",
    "    \n",
    "    train_pool = Pool(X_train_cat, y_train, cat_features=cat_indices)\n",
    "    val_pool = Pool(X_val_cat, y_val, cat_features=cat_indices)\n",
    "    test_pool = Pool(X_test_cat, y_test, cat_features=cat_indices)\n",
    "    \n",
    "    catboost_model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        task_type='CPU'\n",
    "    )\n",
    "    \n",
    "    catboost_model.fit(train_pool, eval_set=val_pool, plot=False)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Предсказания\n",
    "    y_val_proba_cb = catboost_model.predict_proba(val_pool)[:, 1]\n",
    "    y_test_proba_cb = catboost_model.predict_proba(test_pool)[:, 1]\n",
    "    \n",
    "    # Optimal threshold\n",
    "    optimal_thresh_cb, _ = find_optimal_threshold(y_val, y_val_proba_cb)\n",
    "    print(f\"\\n  Optimal threshold: {optimal_thresh_cb:.3f}\")\n",
    "    \n",
    "    # Metrics on test\n",
    "    metrics_cb = calculate_metrics(y_test, y_test_proba_cb, optimal_thresh_cb)\n",
    "    \n",
    "    results.append({\n",
    "        'segment_group': seg_name,\n",
    "        'algorithm': 'CatBoost',\n",
    "        'roc_auc': metrics_cb['roc_auc'],\n",
    "        'gini': metrics_cb['gini'],\n",
    "        'pr_auc': metrics_cb['pr_auc'],\n",
    "        'f1': metrics_cb['f1'],\n",
    "        'precision': metrics_cb['precision'],\n",
    "        'recall': metrics_cb['recall'],\n",
    "        'accuracy': metrics_cb['accuracy'],\n",
    "        'optimal_threshold': optimal_thresh_cb,\n",
    "        'tn': metrics_cb['tn'],\n",
    "        'fp': metrics_cb['fp'],\n",
    "        'fn': metrics_cb['fn'],\n",
    "        'tp': metrics_cb['tp'],\n",
    "        'train_time_sec': train_time\n",
    "    })\n",
    "    \n",
    "    trained_models['CatBoost'] = catboost_model\n",
    "    predictions['CatBoost'] = {'val': y_val_proba_cb, 'test': y_test_proba_cb}\n",
    "    \n",
    "    print(f\"\\n  Test ROC-AUC: {metrics_cb['roc_auc']:.4f} | Gini: {metrics_cb['gini']:.4f} | F1: {metrics_cb['f1']:.4f}\")\n",
    "    print(f\"  Train time: {train_time:.1f}s\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    # 2. LIGHTGBM\n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    \n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"ОБУЧЕНИЕ: LightGBM\")\n",
    "    print(\"─\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Подготовка данных для LightGBM\n",
    "    X_train_lgb = X_train.copy()\n",
    "    X_val_lgb = X_val.copy()\n",
    "    X_test_lgb = X_test.copy()\n",
    "    \n",
    "    for cat in cat_features_present:\n",
    "        X_train_lgb[cat] = X_train_lgb[cat].astype('category')\n",
    "        X_val_lgb[cat] = X_val_lgb[cat].astype('category')\n",
    "        X_test_lgb[cat] = X_test_lgb[cat].astype('category')\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=100,\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(\n",
    "        X_train_lgb, y_train,\n",
    "        eval_set=[(X_val_lgb, y_val)],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Предсказания\n",
    "    y_val_proba_lgb = lgb_model.predict_proba(X_val_lgb)[:, 1]\n",
    "    y_test_proba_lgb = lgb_model.predict_proba(X_test_lgb)[:, 1]\n",
    "    \n",
    "    # Optimal threshold\n",
    "    optimal_thresh_lgb, _ = find_optimal_threshold(y_val, y_val_proba_lgb)\n",
    "    print(f\"\\n  Optimal threshold: {optimal_thresh_lgb:.3f}\")\n",
    "    \n",
    "    # Metrics on test\n",
    "    metrics_lgb = calculate_metrics(y_test, y_test_proba_lgb, optimal_thresh_lgb)\n",
    "    \n",
    "    results.append({\n",
    "        'segment_group': seg_name,\n",
    "        'algorithm': 'LightGBM',\n",
    "        'roc_auc': metrics_lgb['roc_auc'],\n",
    "        'gini': metrics_lgb['gini'],\n",
    "        'pr_auc': metrics_lgb['pr_auc'],\n",
    "        'f1': metrics_lgb['f1'],\n",
    "        'precision': metrics_lgb['precision'],\n",
    "        'recall': metrics_lgb['recall'],\n",
    "        'accuracy': metrics_lgb['accuracy'],\n",
    "        'optimal_threshold': optimal_thresh_lgb,\n",
    "        'tn': metrics_lgb['tn'],\n",
    "        'fp': metrics_lgb['fp'],\n",
    "        'fn': metrics_lgb['fn'],\n",
    "        'tp': metrics_lgb['tp'],\n",
    "        'train_time_sec': train_time\n",
    "    })\n",
    "    \n",
    "    trained_models['LightGBM'] = lgb_model\n",
    "    predictions['LightGBM'] = {'val': y_val_proba_lgb, 'test': y_test_proba_lgb}\n",
    "    \n",
    "    print(f\"\\n  Test ROC-AUC: {metrics_lgb['roc_auc']:.4f} | Gini: {metrics_lgb['gini']:.4f} | F1: {metrics_lgb['f1']:.4f}\")\n",
    "    print(f\"  Train time: {train_time:.1f}s\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    # 3. XGBOOST\n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    \n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"ОБУЧЕНИЕ: XGBoost\")\n",
    "    print(\"─\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Подготовка данных для XGBoost (LabelEncoder для категориальных)\n",
    "    X_train_xgb = X_train.copy()\n",
    "    X_val_xgb = X_val.copy()\n",
    "    X_test_xgb = X_test.copy()\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for cat in cat_features_present:\n",
    "        le = LabelEncoder()\n",
    "        X_train_xgb[cat] = le.fit_transform(X_train_xgb[cat].astype(str))\n",
    "        X_val_xgb[cat] = le.transform(X_val_xgb[cat].astype(str))\n",
    "        X_test_xgb[cat] = le.transform(X_test_xgb[cat].astype(str))\n",
    "        label_encoders[cat] = le\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_child_weight=100,\n",
    "        random_state=RANDOM_SEED,\n",
    "        eval_metric='auc',\n",
    "        tree_method='hist',\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        X_train_xgb, y_train,\n",
    "        eval_set=[(X_val_xgb, y_val)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Предсказания\n",
    "    y_val_proba_xgb = xgb_model.predict_proba(X_val_xgb)[:, 1]\n",
    "    y_test_proba_xgb = xgb_model.predict_proba(X_test_xgb)[:, 1]\n",
    "    \n",
    "    # Optimal threshold\n",
    "    optimal_thresh_xgb, _ = find_optimal_threshold(y_val, y_val_proba_xgb)\n",
    "    print(f\"\\n  Optimal threshold: {optimal_thresh_xgb:.3f}\")\n",
    "    \n",
    "    # Metrics on test\n",
    "    metrics_xgb = calculate_metrics(y_test, y_test_proba_xgb, optimal_thresh_xgb)\n",
    "    \n",
    "    results.append({\n",
    "        'segment_group': seg_name,\n",
    "        'algorithm': 'XGBoost',\n",
    "        'roc_auc': metrics_xgb['roc_auc'],\n",
    "        'gini': metrics_xgb['gini'],\n",
    "        'pr_auc': metrics_xgb['pr_auc'],\n",
    "        'f1': metrics_xgb['f1'],\n",
    "        'precision': metrics_xgb['precision'],\n",
    "        'recall': metrics_xgb['recall'],\n",
    "        'accuracy': metrics_xgb['accuracy'],\n",
    "        'optimal_threshold': optimal_thresh_xgb,\n",
    "        'tn': metrics_xgb['tn'],\n",
    "        'fp': metrics_xgb['fp'],\n",
    "        'fn': metrics_xgb['fn'],\n",
    "        'tp': metrics_xgb['tp'],\n",
    "        'train_time_sec': train_time\n",
    "    })\n",
    "    \n",
    "    trained_models['XGBoost'] = xgb_model\n",
    "    predictions['XGBoost'] = {'val': y_val_proba_xgb, 'test': y_test_proba_xgb}\n",
    "    \n",
    "    print(f\"\\n  Test ROC-AUC: {metrics_xgb['roc_auc']:.4f} | Gini: {metrics_xgb['gini']:.4f} | F1: {metrics_xgb['f1']:.4f}\")\n",
    "    print(f\"  Train time: {train_time:.1f}s\")\n",
    "    \n",
    "    # Очистка памяти\n",
    "    gc.collect()\n",
    "    \n",
    "    return results, trained_models, predictions, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ОБУЧЕНИЕ: SEGMENT 1\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"ОБУЧЕНИЕ МОДЕЛЕЙ: SEGMENT 1\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "# Для seg1 категориальные без segment_group\n",
    "seg1_cat_features = [c for c in CATEGORICAL_FEATURES if c != SEGMENT_COLUMN]\n",
    "\n",
    "seg1_results, seg1_models, seg1_predictions, seg1_y_val, seg1_y_test = train_all_models(\n",
    "    seg_name='Segment 1 (SMALL_BUSINESS)',\n",
    "    seg_train_balanced=seg1_train_balanced,\n",
    "    seg_y_train_balanced=seg1_y_train_balanced,\n",
    "    seg_val=seg1_val,\n",
    "    seg_test=seg1_test,\n",
    "    categorical_features=seg1_cat_features\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ОБУЧЕНИЕ: SEGMENT 2\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"ОБУЧЕНИЕ МОДЕЛЕЙ: SEGMENT 2\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "# Для seg2 категориальные включают segment_group\n",
    "seg2_cat_features = CATEGORICAL_FEATURES\n",
    "\n",
    "seg2_results, seg2_models, seg2_predictions, seg2_y_val, seg2_y_test = train_all_models(\n",
    "    seg_name='Segment 2 (MIDDLE + LARGE)',\n",
    "    seg_train_balanced=seg2_train_balanced,\n",
    "    seg_y_train_balanced=seg2_y_train_balanced,\n",
    "    seg_val=seg2_val,\n",
    "    seg_test=seg2_test,\n",
    "    categorical_features=seg2_cat_features\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# СВОДНАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"СВОДНАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "# Объединяем результаты\n",
    "all_results = seg1_results + seg2_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Сортируем\n",
    "results_df = results_df.sort_values(['segment_group', 'roc_auc'], ascending=[True, False])\n",
    "\n",
    "# Сохраняем\n",
    "results_df.to_csv(OUTPUT_DIR / 'kmeans_undersampling_results.csv', index=False)\n",
    "print(f\"\\n✓ Результаты сохранены: {OUTPUT_DIR / 'kmeans_undersampling_results.csv'}\")\n",
    "\n",
    "# Выводим таблицу\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ROC CURVES: SEGMENT 1\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ: ROC CURVES\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = {'CatBoost': 'blue', 'LightGBM': 'green', 'XGBoost': 'red'}\n",
    "\n",
    "for algo in ['CatBoost', 'LightGBM', 'XGBoost']:\n",
    "    y_proba = seg1_predictions[algo]['test']\n",
    "    fpr, tpr, _ = roc_curve(seg1_y_test, y_proba)\n",
    "    auc_score = roc_auc_score(seg1_y_test, y_proba)\n",
    "    \n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'{algo} (AUC={auc_score:.4f})', color=colors[algo])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves: Segment 1 (SMALL_BUSINESS)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'roc_segment1_kmeans.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Сохранено: {FIGURES_DIR / 'roc_segment1_kmeans.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ROC CURVES: SEGMENT 2\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for algo in ['CatBoost', 'LightGBM', 'XGBoost']:\n",
    "    y_proba = seg2_predictions[algo]['test']\n",
    "    fpr, tpr, _ = roc_curve(seg2_y_test, y_proba)\n",
    "    auc_score = roc_auc_score(seg2_y_test, y_proba)\n",
    "    \n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'{algo} (AUC={auc_score:.4f})', color=colors[algo])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves: Segment 2 (MIDDLE + LARGE BUSINESS)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'roc_segment2_kmeans.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Сохранено: {FIGURES_DIR / 'roc_segment2_kmeans.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# BARPLOT СРАВНЕНИЕ МЕТРИК\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Segment 1\n",
    "seg1_data = results_df[results_df['segment_group'].str.contains('Segment 1')]\n",
    "ax1 = axes[0]\n",
    "x_pos = np.arange(len(seg1_data))\n",
    "bars1 = ax1.bar(x_pos, seg1_data['roc_auc'], \n",
    "                color=['blue', 'green', 'red'], alpha=0.7, edgecolor='black')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(seg1_data['algorithm'])\n",
    "ax1.set_ylabel('ROC-AUC', fontsize=12)\n",
    "ax1.set_title('Segment 1 (SMALL_BUSINESS)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars1, seg1_data['roc_auc'])):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Segment 2\n",
    "seg2_data = results_df[results_df['segment_group'].str.contains('Segment 2')]\n",
    "ax2 = axes[1]\n",
    "x_pos = np.arange(len(seg2_data))\n",
    "bars2 = ax2.bar(x_pos, seg2_data['roc_auc'], \n",
    "                color=['blue', 'green', 'red'], alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(seg2_data['algorithm'])\n",
    "ax2.set_ylabel('ROC-AUC', fontsize=12)\n",
    "ax2.set_title('Segment 2 (MIDDLE + LARGE)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars2, seg2_data['roc_auc'])):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Сравнение алгоритмов: ROC-AUC (Test OOT)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'metrics_comparison_kmeans.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Сохранено: {FIGURES_DIR / 'metrics_comparison_kmeans.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# FEATURE IMPORTANCE: ЛУЧШИЕ МОДЕЛИ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def plot_feature_importance(model, model_name, segment_name, save_path, top_n=20):\n",
    "    \"\"\"\n",
    "    Визуализация feature importance\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = model.feature_names_in_ if hasattr(model, 'feature_names_in_') else None\n",
    "    elif hasattr(model, 'get_feature_importance'):\n",
    "        importances = model.get_feature_importance()\n",
    "        feature_names = model.feature_names_\n",
    "    else:\n",
    "        print(f\"  Модель {model_name} не поддерживает feature importance\")\n",
    "        return\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
    "    \n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    })\n",
    "    fi_df = fi_df.sort_values('importance', ascending=False).head(top_n)\n",
    "    \n",
    "    # График\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(range(len(fi_df)), fi_df['importance'], color='steelblue', edgecolor='black')\n",
    "    ax.set_yticks(range(len(fi_df)))\n",
    "    ax.set_yticklabels(fi_df['feature'])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Importance', fontsize=12)\n",
    "    ax.set_title(f'Top-{top_n} Feature Importance: {model_name}\\n{segment_name}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ✓ Сохранено: {save_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "# Лучшая модель для Segment 1\n",
    "best_seg1 = seg1_data.iloc[0]\n",
    "best_model_seg1 = seg1_models[best_seg1['algorithm']]\n",
    "print(f\"\\nЛучшая модель Segment 1: {best_seg1['algorithm']} (ROC-AUC: {best_seg1['roc_auc']:.4f})\")\n",
    "plot_feature_importance(\n",
    "    best_model_seg1, \n",
    "    best_seg1['algorithm'], \n",
    "    'Segment 1 (SMALL_BUSINESS)',\n",
    "    FIGURES_DIR / 'feature_importance_seg1_kmeans.png'\n",
    ")\n",
    "\n",
    "# Лучшая модель для Segment 2\n",
    "best_seg2 = seg2_data.iloc[0]\n",
    "best_model_seg2 = seg2_models[best_seg2['algorithm']]\n",
    "print(f\"\\nЛучшая модель Segment 2: {best_seg2['algorithm']} (ROC-AUC: {best_seg2['roc_auc']:.4f})\")\n",
    "plot_feature_importance(\n",
    "    best_model_seg2, \n",
    "    best_seg2['algorithm'], \n",
    "    'Segment 2 (MIDDLE + LARGE)',\n",
    "    FIGURES_DIR / 'feature_importance_seg2_kmeans.png'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. ФИНАЛЬНЫЙ ОТЧЕТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ФИНАЛЬНЫЙ ОТЧЕТ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "report = []\n",
    "report.append(\"═\"*80)\n",
    "report.append(\"РЕЗУЛЬТАТЫ: K-MEANS UNDERSAMPLING (30 кластеров, соотношение 1:20)\")\n",
    "report.append(\"═\"*80)\n",
    "report.append(\"\")\n",
    "report.append(f\"Дата: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report.append(f\"Random seed: {RANDOM_SEED}\")\n",
    "report.append(\"\")\n",
    "\n",
    "# SEGMENT 1\n",
    "report.append(\"SEGMENT 1: SMALL BUSINESS\")\n",
    "report.append(\"─\"*80)\n",
    "report.append(\"\")\n",
    "report.append(\"Undersampling эффект:\")\n",
    "report.append(f\"  Исходно: {seg1_cluster_info['original_class_0']:,} примеров класса 0\")\n",
    "report.append(f\"  После: {seg1_cluster_info['sampled_class_0']:,} примеров класса 0\")\n",
    "report.append(f\"  Reduction: {seg1_cluster_info['reduction_percent']:.1f}%\")\n",
    "report.append(f\"  Маленьких кластеров сохранено полностью: {seg1_cluster_info['small_clusters_count']} из {N_CLUSTERS}\")\n",
    "report.append(\"\")\n",
    "report.append(\"Результаты моделей (Test OOT):\")\n",
    "for _, row in seg1_data.iterrows():\n",
    "    report.append(f\"  {row['algorithm']:<12}: ROC-AUC = {row['roc_auc']:.4f} | Gini = {row['gini']:.4f} | F1 = {row['f1']:.4f} | Time = {row['train_time_sec']:.0f}s\")\n",
    "report.append(\"\")\n",
    "report.append(f\"Лучшая модель: {best_seg1['algorithm']} (ROC-AUC: {best_seg1['roc_auc']:.4f})\")\n",
    "report.append(f\"Optimal threshold: {best_seg1['optimal_threshold']:.3f}\")\n",
    "report.append(\"\")\n",
    "\n",
    "# SEGMENT 2\n",
    "report.append(\"SEGMENT 2: MIDDLE + LARGE BUSINESS\")\n",
    "report.append(\"─\"*80)\n",
    "report.append(\"\")\n",
    "report.append(\"Undersampling эффект:\")\n",
    "report.append(f\"  Исходно: {seg2_cluster_info['original_class_0']:,} примеров класса 0\")\n",
    "report.append(f\"  После: {seg2_cluster_info['sampled_class_0']:,} примеров класса 0\")\n",
    "report.append(f\"  Reduction: {seg2_cluster_info['reduction_percent']:.1f}%\")\n",
    "report.append(f\"  Маленьких кластеров сохранено полностью: {seg2_cluster_info['small_clusters_count']} из {N_CLUSTERS}\")\n",
    "report.append(\"\")\n",
    "report.append(\"Результаты моделей (Test OOT):\")\n",
    "for _, row in seg2_data.iterrows():\n",
    "    report.append(f\"  {row['algorithm']:<12}: ROC-AUC = {row['roc_auc']:.4f} | Gini = {row['gini']:.4f} | F1 = {row['f1']:.4f} | Time = {row['train_time_sec']:.0f}s\")\n",
    "report.append(\"\")\n",
    "report.append(f\"Лучшая модель: {best_seg2['algorithm']} (ROC-AUC: {best_seg2['roc_auc']:.4f})\")\n",
    "report.append(f\"Optimal threshold: {best_seg2['optimal_threshold']:.3f}\")\n",
    "report.append(\"\")\n",
    "\n",
    "# ОБЩИЕ ВЫВОДЫ\n",
    "report.append(\"ОБЩИЕ ВЫВОДЫ\")\n",
    "report.append(\"─\"*80)\n",
    "report.append(\"\")\n",
    "report.append(\"1. Эффективность K-Means undersampling:\")\n",
    "report.append(f\"   - Умное сохранение маленьких кластеров позволило сохранить разнообразие данных\")\n",
    "report.append(f\"   - Reduction класса 0: Segment 1 = {seg1_cluster_info['reduction_percent']:.1f}%, Segment 2 = {seg2_cluster_info['reduction_percent']:.1f}%\")\n",
    "report.append(f\"   - Достигнуто целевое соотношение 1:{TARGET_RATIO} для обеих групп\")\n",
    "report.append(\"\")\n",
    "report.append(\"2. Сравнение алгоритмов:\")\n",
    "report.append(f\"   - Лучший для Segment 1: {best_seg1['algorithm']} (ROC-AUC: {best_seg1['roc_auc']:.4f})\")\n",
    "report.append(f\"   - Лучший для Segment 2: {best_seg2['algorithm']} (ROC-AUC: {best_seg2['roc_auc']:.4f})\")\n",
    "avg_roc_auc = results_df.groupby('algorithm')['roc_auc'].mean().sort_values(ascending=False)\n",
    "report.append(f\"   - В среднем лучший: {avg_roc_auc.index[0]} (средний ROC-AUC: {avg_roc_auc.values[0]:.4f})\")\n",
    "report.append(\"\")\n",
    "report.append(\"3. Рекомендации:\")\n",
    "report.append(f\"   - Использовать {best_seg1['algorithm']} для Segment 1 (SMALL_BUSINESS)\")\n",
    "report.append(f\"   - Использовать {best_seg2['algorithm']} для Segment 2 (MIDDLE + LARGE)\")\n",
    "report.append(f\"   - K-Means undersampling с {N_CLUSTERS} кластерами показал отличные результаты\")\n",
    "report.append(f\"   - Умная стратегия sampling превосходит простой random undersampling\")\n",
    "report.append(\"\")\n",
    "report.append(\"═\"*80)\n",
    "\n",
    "# Выводим отчет\n",
    "report_text = \"\\n\".join(report)\n",
    "print(report_text)\n",
    "\n",
    "# Сохраняем отчет\n",
    "with open(OUTPUT_DIR / 'final_report_kmeans.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"\\n✓ Отчет сохранен: {OUTPUT_DIR / 'final_report_kmeans.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ФИНАЛ\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\\n\" + \"═\"*80)\n",
    "print(\"✓✓✓ ЭКСПЕРИМЕНТЫ ЗАВЕРШЕНЫ УСПЕШНО ✓✓✓\")\n",
    "print(\"═\"*80)\n",
    "print(\"\\nСозданные файлы:\")\n",
    "print(\"\\n1. Визуализации распределений:\")\n",
    "print(\"   - figures/distributions_segment1.png\")\n",
    "print(\"   - figures/distributions_segment2.png\")\n",
    "print(\"\\n2. Статистики признаков:\")\n",
    "print(\"   - output/feature_statistics_seg1.csv\")\n",
    "print(\"   - output/feature_statistics_seg2.csv\")\n",
    "print(\"\\n3. Информация о кластерах:\")\n",
    "print(\"   - output/cluster_info_seg1.json\")\n",
    "print(\"   - output/cluster_info_seg2.json\")\n",
    "print(\"\\n4. Результаты моделей:\")\n",
    "print(\"   - output/kmeans_undersampling_results.csv\")\n",
    "print(\"\\n5. Визуализации результатов:\")\n",
    "print(\"   - figures/roc_segment1_kmeans.png\")\n",
    "print(\"   - figures/roc_segment2_kmeans.png\")\n",
    "print(\"   - figures/metrics_comparison_kmeans.png\")\n",
    "print(\"   - figures/feature_importance_seg1_kmeans.png\")\n",
    "print(\"   - figures/feature_importance_seg2_kmeans.png\")\n",
    "print(\"\\n6. Финальный отчет:\")\n",
    "print(\"   - output/final_report_kmeans.txt\")\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"Воспроизводимость: Run All → идентичные результаты (random_seed=42)\")\n",
    "print(\"═\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
