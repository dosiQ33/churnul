{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "МОДЕЛЬ ПРОГНОЗИРОВАНИЯ ОТТОКА КЛИЕНТОВ БАНКА (CHURN PREDICTION MODEL)\n",
        "===============================================================================\n",
        "\n",
        "Полный воспроизводимый пайплайн построения моделей\n",
        "\n",
        "Дата: 2025-01-13\n",
        "Версия: 1.0\n",
        "Алгоритм: CatBoost Classifier\n",
        "\n",
        "ОСОБЕННОСТИ:\n",
        "- Две модели по сегментам:\n",
        "  * Модель 1: Малый бизнес (SMALL_BUSINESS)\n",
        "  * Модель 2: Средний + Крупный бизнес (MIDDLE + LARGE_BUSINESS)\n",
        "- Temporal Split (Train/Val/Test-OOT)\n",
        "- Обработка дисбаланса классов\n",
        "- Полная воспроизводимость (random_seed=42)\n",
        "\n",
        "МЕТРИКИ:\n",
        "- GINI coefficient\n",
        "- ROC-AUC\n",
        "- PR-AUC\n",
        "- Accuracy, Precision, Recall, F1-Score\n",
        "- Confusion Matrix\n",
        "\n",
        "===============================================================================\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ИМПОРТ БИБЛИОТЕК\n",
        "# ====================================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pickle\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# Данные\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Визуализация\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    precision_recall_curve, roc_curve,\n",
        "    classification_report, confusion_matrix,\n",
        "    accuracy_score, f1_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "# Настройки\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CHURN PREDICTION MODEL - ПОЛНЫЙ ПАЙПЛАЙН\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✓ Библиотеки импортированы\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  NumPy: {np.__version__}\")\n",
        "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ГЛОБАЛЬНАЯ КОНФИГУРАЦИЯ\n",
        "# ====================================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Централизованная конфигурация для воспроизводимости\"\"\"\n",
        "\n",
        "    # ВОСПРОИЗВОДИМОСТЬ\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "    # ПУТИ\n",
        "    DATA_DIR = Path(\"data\")\n",
        "    OUTPUT_DIR = Path(\"output\")\n",
        "    MODEL_DIR = Path(\"models\")\n",
        "    FIGURES_DIR = Path(\"figures\")\n",
        "\n",
        "    # ФАЙЛЫ\n",
        "    TRAIN_FILE = \"churn_train_ul.parquet\"\n",
        "    PROD_FILE = \"churn_prod_ul.parquet\"\n",
        "\n",
        "    # КОЛОНКИ\n",
        "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
        "    TARGET_COLUMN = 'target_churn_3m'\n",
        "    SEGMENT_COLUMN = 'segment_group'\n",
        "    DATE_COLUMN = 'observation_point'\n",
        "    CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']\n",
        "\n",
        "    # СЕГМЕНТЫ (ДВЕ МОДЕЛИ)\n",
        "    SEGMENT_1_NAME = \"Small Business\"\n",
        "    SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
        "\n",
        "    SEGMENT_2_NAME = \"Middle + Large Business\"\n",
        "    SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
        "\n",
        "    # ВРЕМЕННОЕ РАЗБИЕНИЕ\n",
        "    TRAIN_SIZE = 0.70\n",
        "    VAL_SIZE = 0.15\n",
        "    TEST_SIZE = 0.15\n",
        "\n",
        "    # PREPROCESSING\n",
        "    CORRELATION_THRESHOLD = 0.85\n",
        "    OUTLIER_IQR_MULTIPLIER = 1.5\n",
        "    REMOVE_GAPS = True\n",
        "    HANDLE_OUTLIERS = True\n",
        "    REMOVE_HIGH_CORRELATIONS = True\n",
        "\n",
        "    # CATBOOST\n",
        "    CATBOOST_PARAMS = {\n",
        "        'iterations': 500,\n",
        "        'learning_rate': 0.05,\n",
        "        'depth': 4,\n",
        "        'l2_leaf_reg': 3,\n",
        "        'min_data_in_leaf': 100,\n",
        "        'random_strength': 1,\n",
        "        'bagging_temperature': 1,\n",
        "        'border_count': 128,\n",
        "        'loss_function': 'Logloss',\n",
        "        'eval_metric': 'AUC',\n",
        "        'early_stopping_rounds': 100,\n",
        "        'use_best_model': True,\n",
        "        'random_seed': 42,\n",
        "        'task_type': 'CPU',\n",
        "        'verbose': 100,\n",
        "        'allow_writing_files': False\n",
        "    }\n",
        "\n",
        "    # IMBALANCE\n",
        "    USE_CLASS_WEIGHTS = True\n",
        "    TUNE_THRESHOLD = True\n",
        "    THRESHOLD_METRIC = 'f1'\n",
        "\n",
        "    @classmethod\n",
        "    def create_directories(cls):\n",
        "        for dir_path in [cls.OUTPUT_DIR, cls.MODEL_DIR, cls.FIGURES_DIR]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    @classmethod\n",
        "    def get_train_path(cls):\n",
        "        return cls.DATA_DIR / cls.TRAIN_FILE\n",
        "\n",
        "config = Config()\n",
        "config.create_directories()\n",
        "np.random.seed(config.RANDOM_SEED)\n",
        "\n",
        "print(\"\\\\n✓ Конфигурация инициализирована\")\n",
        "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
        "print(f\"  Модель 1: {config.SEGMENT_1_NAME} {config.SEGMENT_1_VALUES}\")\n",
        "print(f\"  Модель 2: {config.SEGMENT_2_NAME} {config.SEGMENT_2_VALUES}\")\n",
        "print(f\"  Split: {config.TRAIN_SIZE}/{config.VAL_SIZE}/{config.TEST_SIZE}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. ЗАГРУЗКА ДАННЫХ (PARQUET)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ЗАГРУЗКА ДАННЫХ\n",
        "# ====================================================================================\n",
        "\n",
        "train_path = config.get_train_path()\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Файл: {train_path}\")\n",
        "\n",
        "if not train_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Файл не найден: {train_path}\\\\n\"\n",
        "        f\"Запустите: python csv_to_parquet_converter.py\"\n",
        "    )\n",
        "\n",
        "file_size = train_path.stat().st_size / (1024**2)\n",
        "print(f\"Размер: {file_size:.2f} MB\")\n",
        "\n",
        "start = time.time()\n",
        "df_full = pd.read_parquet(train_path)\n",
        "load_time = time.time() - start\n",
        "\n",
        "memory = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
        "\n",
        "print(f\"\\\\n✓ Загружено за {load_time:.2f} сек\")\n",
        "print(f\"  Размер: {df_full.shape}\")\n",
        "print(f\"  Память: {memory:.2f} MB\")\n",
        "\n",
        "# Целевая переменная\n",
        "churn_rate = df_full[config.TARGET_COLUMN].mean()\n",
        "print(f\"\\\\n  Target '{config.TARGET_COLUMN}':\")\n",
        "print(f\"    Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
        "print(f\"    Churned: {df_full[config.TARGET_COLUMN].sum():,}\")\n",
        "print(f\"    Ratio: 1:{(1-churn_rate)/churn_rate:.1f}\")\n",
        "\n",
        "# Временной диапазон\n",
        "df_full[config.DATE_COLUMN] = pd.to_datetime(df_full[config.DATE_COLUMN])\n",
        "print(f\"\\\\n  Период: {df_full[config.DATE_COLUMN].min().date()} - \"\n",
        "      f\"{df_full[config.DATE_COLUMN].max().date()}\")\n",
        "print(f\"  Уникальных дат: {df_full[config.DATE_COLUMN].nunique()}\")\n",
        "\n",
        "# Сегменты\n",
        "print(f\"\\\\n  Распределение по сегментам:\")\n",
        "for segment, count in df_full[config.SEGMENT_COLUMN].value_counts().items():\n",
        "    pct = count / len(df_full) * 100\n",
        "    churn_seg = df_full[df_full[config.SEGMENT_COLUMN]==segment][config.TARGET_COLUMN].mean()\n",
        "    print(f\"    {segment}: {count:,} ({pct:.1f}%) | Churn: {churn_seg*100:.2f}%\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. EXPLORATORY DATA ANALYSIS (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# АНАЛИЗ КАЧЕСТВА ДАННЫХ\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"АНАЛИЗ КАЧЕСТВА ДАННЫХ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Пропуски\n",
        "missing = df_full.isnull().sum()\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing': missing[missing > 0],\n",
        "    'Percent': (missing[missing > 0] / len(df_full) * 100).round(2)\n",
        "}).sort_values('Missing', ascending=False)\n",
        "\n",
        "print(f\"\\\\n1. Пропущенные значения:\")\n",
        "if len(missing_df) > 0:\n",
        "    print(f\"   {len(missing_df)} колонок с пропусками:\")\n",
        "    print(missing_df.head(10).to_string())\n",
        "else:\n",
        "    print(\"   ✓ Нет пропусков\")\n",
        "\n",
        "# Константы\n",
        "constant_cols = [col for col in df_full.columns if df_full[col].nunique() == 1]\n",
        "print(f\"\\\\n2. Константные колонки: {len(constant_cols)}\")\n",
        "if constant_cols:\n",
        "    print(f\"   {constant_cols[:5]}...\")\n",
        "\n",
        "# Дубликаты\n",
        "n_dups = df_full.duplicated().sum()\n",
        "print(f\"\\\\n3. Дубликаты: {n_dups:,}\")\n",
        "\n",
        "# Типы\n",
        "print(f\"\\\\n4. Типы данных:\")\n",
        "for dtype, count in df_full.dtypes.value_counts().items():\n",
        "    print(f\"   {dtype}: {count}\")\n",
        "\n",
        "# Категориальные\n",
        "cat_cols = df_full.select_dtypes(include='category').columns\n",
        "print(f\"\\\\n5. Категориальные: {len(cat_cols)}\")\n",
        "for col in cat_cols:\n",
        "    print(f\"   {col}: {df_full[col].nunique()} unique\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ВИЗУАЛИЗАЦИЯ: РАСПРЕДЕЛЕНИЕ TARGET\n",
        "# ====================================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Общее\n",
        "target_dist = df_full[config.TARGET_COLUMN].value_counts()\n",
        "axes[0].bar(['No Churn', 'Churn'], [target_dist[0], target_dist[1]],\n",
        "           color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Распределение Target', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Количество')\n",
        "axes[0].set_yscale('log')\n",
        "for i, v in enumerate([target_dist[0], target_dist[1]]):\n",
        "    axes[0].text(i, v, f'{v:,}\\\\n({v/len(df_full)*100:.2f}%)',\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# 2. По сегментам\n",
        "segment_churn = df_full.groupby([config.SEGMENT_COLUMN,\n",
        "                                  config.TARGET_COLUMN]).size().unstack(fill_value=0)\n",
        "segment_churn.plot(kind='bar', stacked=True, ax=axes[1],\n",
        "                  color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('По сегментам', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Сегмент')\n",
        "axes[1].legend(['No Churn', 'Churn'])\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Churn rate\n",
        "churn_rates = df_full.groupby(config.SEGMENT_COLUMN)[config.TARGET_COLUMN].mean() * 100\n",
        "axes[2].bar(range(len(churn_rates)), churn_rates.values,\n",
        "           color='coral', alpha=0.7, edgecolor='black')\n",
        "axes[2].set_xticks(range(len(churn_rates)))\n",
        "axes[2].set_xticklabels(churn_rates.index, rotation=45, ha='right')\n",
        "axes[2].set_title('Churn Rate по сегментам', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Churn Rate (%)')\n",
        "for i, v in enumerate(churn_rates.values):\n",
        "    axes[2].text(i, v, f'{v:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(config.FIGURES_DIR / '01_eda_target.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Сохранено: figures/01_eda_target.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4. ВРЕМЕННОЕ РАЗБИЕНИЕ (TRAIN / VAL / TEST-OOT)\n",
        "\n",
        "Разбиение по времени для предотвращения data leakage.\n",
        "**OOT (Out-of-Time)** - тестовая выборка из будущих периодов.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# TEMPORAL SPLIT\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Сортировка по времени\n",
        "df_sorted = df_full.sort_values(config.DATE_COLUMN).reset_index(drop=True)\n",
        "unique_dates = sorted(df_sorted[config.DATE_COLUMN].unique())\n",
        "n_dates = len(unique_dates)\n",
        "\n",
        "print(f\"\\\\nУникальных дат: {n_dates}\")\n",
        "print(f\"Период: {unique_dates[0].date()} - {unique_dates[-1].date()}\")\n",
        "\n",
        "# Cutoff indices\n",
        "train_cutoff = int(n_dates * config.TRAIN_SIZE)\n",
        "val_cutoff = int(n_dates * (config.TRAIN_SIZE + config.VAL_SIZE))\n",
        "\n",
        "train_end = unique_dates[train_cutoff - 1]\n",
        "val_end = unique_dates[val_cutoff - 1]\n",
        "\n",
        "print(f\"\\\\nCutoff даты:\")\n",
        "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
        "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
        "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
        "\n",
        "# Создание split\n",
        "train_df = df_sorted[df_sorted[config.DATE_COLUMN] <= train_end].copy()\n",
        "val_df = df_sorted[(df_sorted[config.DATE_COLUMN] > train_end) &\n",
        "                   (df_sorted[config.DATE_COLUMN] <= val_end)].copy()\n",
        "test_df = df_sorted[df_sorted[config.DATE_COLUMN] > val_end].copy()\n",
        "\n",
        "# Stats\n",
        "for name, df in [('TRAIN', train_df), ('VAL', val_df), ('TEST (OOT)', test_df)]:\n",
        "    churn_r = df[config.TARGET_COLUMN].mean()\n",
        "    print(f\"\\\\n{name}:\")\n",
        "    print(f\"  Записей: {len(df):,}\")\n",
        "    print(f\"  Клиентов: {df['cli_code'].nunique():,}\")\n",
        "    print(f\"  Период: {df[config.DATE_COLUMN].min().date()} - {df[config.DATE_COLUMN].max().date()}\")\n",
        "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
        "\n",
        "# Проверка leakage\n",
        "assert train_df[config.DATE_COLUMN].max() < val_df[config.DATE_COLUMN].min()\n",
        "assert val_df[config.DATE_COLUMN].max() < test_df[config.DATE_COLUMN].min()\n",
        "print(\"\\\\n✓ Temporal ordering verified - no data leakage\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5. УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ В НАБЛЮДЕНИЯХ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# GAP REMOVAL\n",
        "# ====================================================================================\n",
        "\n",
        "if config.REMOVE_GAPS:\n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\\\nАнализ пробелов в train...\")\n",
        "\n",
        "    # Chunked processing\n",
        "    unique_clients = train_df['cli_code'].unique()\n",
        "    chunk_size = 10000\n",
        "    clients_with_gaps_list = []\n",
        "\n",
        "    for i in range(0, len(unique_clients), chunk_size):\n",
        "        chunk_clients = unique_clients[i:i+chunk_size]\n",
        "        chunk = train_df[train_df['cli_code'].isin(chunk_clients)].copy()\n",
        "        chunk = chunk.sort_values(['cli_code', config.DATE_COLUMN])\n",
        "\n",
        "        chunk['month_num'] = chunk[config.DATE_COLUMN].dt.to_period('M').apply(lambda x: x.ordinal)\n",
        "        chunk['month_diff'] = chunk.groupby('cli_code')['month_num'].diff()\n",
        "\n",
        "        gaps = chunk.groupby('cli_code')['month_diff'].agg([\n",
        "            ('max_gap', 'max'),\n",
        "            ('total_gaps', lambda x: (x > 1).sum())\n",
        "        ]).reset_index()\n",
        "\n",
        "        chunk_gaps = gaps[gaps['max_gap'] > 1]\n",
        "        clients_with_gaps_list.append(chunk_gaps)\n",
        "\n",
        "        if (i // chunk_size + 1) % 10 == 0:\n",
        "            gc.collect()\n",
        "            print(f\"  Обработано {i+chunk_size:,}/{len(unique_clients):,} клиентов\")\n",
        "\n",
        "    clients_with_gaps = pd.concat(clients_with_gaps_list, ignore_index=True)\n",
        "\n",
        "    gap_pct = len(clients_with_gaps) / len(unique_clients) * 100\n",
        "    print(f\"\\\\nКлиентов с пробелами: {len(clients_with_gaps):,} ({gap_pct:.2f}%)\")\n",
        "\n",
        "    if len(clients_with_gaps) > 0:\n",
        "        bad_clients = set(clients_with_gaps['cli_code'])\n",
        "\n",
        "        train_before = len(train_df)\n",
        "        train_df = train_df[~train_df['cli_code'].isin(bad_clients)].copy()\n",
        "        val_df = val_df[~val_df['cli_code'].isin(bad_clients)].copy()\n",
        "        test_df = test_df[~test_df['cli_code'].isin(bad_clients)].copy()\n",
        "\n",
        "        print(f\"\\\\nУдалено:\")\n",
        "        print(f\"  Train: {train_before:,} → {len(train_df):,}\")\n",
        "        print(f\"  Val: {len(val_df):,}\")\n",
        "        print(f\"  Test: {len(test_df):,}\")\n",
        "\n",
        "        del clients_with_gaps, bad_clients\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. PREPROCESSING PIPELINE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# PREPROCESSING PIPELINE\n",
        "# ====================================================================================\n",
        "\n",
        "class PreprocessingPipeline:\n",
        "    \"\"\"Preprocessing pipeline для CatBoost\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.fitted_columns = None\n",
        "        self.final_features = None\n",
        "        self.constant_cols = []\n",
        "        self.outlier_bounds = {}\n",
        "        self.numeric_imputer = None\n",
        "        self.categorical_imputer = None\n",
        "        self.numeric_cols_for_imputation = []\n",
        "        self.categorical_cols_for_imputation = []\n",
        "        self.features_to_drop_corr = []\n",
        "\n",
        "    def fit_transform(self, train_df):\n",
        "        \"\"\"Fit and transform training data\"\"\"\n",
        "        print(\"\\\\n\" + \"=\"*80)\n",
        "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        df = train_df.copy()\n",
        "\n",
        "        # Store columns\n",
        "        self.fitted_columns = [c for c in df.columns\n",
        "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
        "\n",
        "        # 1. Remove constants\n",
        "        df = self._remove_constants(df, fit=True)\n",
        "\n",
        "        # 2. Handle outliers\n",
        "        df = self._handle_outliers(df, fit=True)\n",
        "\n",
        "        # 3. Handle missing\n",
        "        df = self._handle_missing(df, fit=True)\n",
        "\n",
        "        # 4. Remove correlations\n",
        "        df = self._remove_correlations(df, fit=True)\n",
        "\n",
        "        # Final features\n",
        "        self.final_features = [c for c in df.columns\n",
        "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
        "\n",
        "        print(f\"\\\\n✓ Preprocessing complete\")\n",
        "        print(f\"  Features: {len(self.final_features)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def transform(self, df, dataset_name='test'):\n",
        "        \"\"\"Transform new data\"\"\"\n",
        "        print(f\"\\\\nPreprocessing: {dataset_name}\")\n",
        "\n",
        "        df = df.copy()\n",
        "\n",
        "        df = self._remove_constants(df, fit=False)\n",
        "        df = self._handle_outliers(df, fit=False)\n",
        "        df = self._handle_missing(df, fit=False)\n",
        "        df = self._remove_correlations(df, fit=False)\n",
        "        df = self._align_columns(df, dataset_name)\n",
        "\n",
        "        print(f\"  ✓ {dataset_name}: {df.shape}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _remove_constants(self, df, fit):\n",
        "        if fit:\n",
        "            print(\"\\\\n1. Removing constant columns...\")\n",
        "            for col in df.columns:\n",
        "                if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
        "                    continue\n",
        "                if df[col].nunique(dropna=False) == 1:\n",
        "                    self.constant_cols.append(col)\n",
        "\n",
        "            if self.constant_cols:\n",
        "                df = df.drop(columns=self.constant_cols)\n",
        "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
        "        return df\n",
        "\n",
        "    def _handle_outliers(self, df, fit):\n",
        "        if not config.HANDLE_OUTLIERS:\n",
        "            return df\n",
        "\n",
        "        if fit:\n",
        "            print(\"\\\\n2. Handling outliers...\")\n",
        "            keywords = ['profit', 'income', 'expense', 'margin', 'provision',\n",
        "                       'balance', 'assets', 'liabilities']\n",
        "            cols = [c for c in df.columns\n",
        "                   if any(kw in c.lower() for kw in keywords)\n",
        "                   and c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
        "\n",
        "            for col in cols:\n",
        "                if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
        "                    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
        "                    IQR = Q3 - Q1\n",
        "                    self.outlier_bounds[col] = {\n",
        "                        'lower': Q1 - config.OUTLIER_IQR_MULTIPLIER * IQR,\n",
        "                        'upper': Q3 + config.OUTLIER_IQR_MULTIPLIER * IQR\n",
        "                    }\n",
        "\n",
        "            for col, bounds in self.outlier_bounds.items():\n",
        "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
        "\n",
        "            print(f\"   Clipped: {len(self.outlier_bounds)} columns\")\n",
        "        else:\n",
        "            for col, bounds in self.outlier_bounds.items():\n",
        "                if col in df.columns:\n",
        "                    df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _handle_missing(self, df, fit):\n",
        "        if fit:\n",
        "            print(\"\\\\n3. Handling missing values...\")\n",
        "            self.numeric_cols_for_imputation = [\n",
        "                c for c in df.select_dtypes(include=[np.number]).columns\n",
        "                if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        "            ]\n",
        "            self.categorical_cols_for_imputation = [\n",
        "                c for c in config.CATEGORICAL_FEATURES if c in df.columns\n",
        "            ]\n",
        "\n",
        "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
        "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "            if len(self.numeric_cols_for_imputation) > 0:\n",
        "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
        "                    df[self.numeric_cols_for_imputation]\n",
        "                )\n",
        "\n",
        "            if len(self.categorical_cols_for_imputation) > 0:\n",
        "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
        "                    df[self.categorical_cols_for_imputation]\n",
        "                )\n",
        "\n",
        "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
        "                  f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
        "        else:\n",
        "            if len(self.numeric_cols_for_imputation) > 0:\n",
        "                present = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
        "                if present:\n",
        "                    df[present] = self.numeric_imputer.transform(df[present])\n",
        "\n",
        "            if len(self.categorical_cols_for_imputation) > 0:\n",
        "                present = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
        "                if present:\n",
        "                    df[present] = self.categorical_imputer.transform(df[present])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _remove_correlations(self, df, fit):\n",
        "        if not config.REMOVE_HIGH_CORRELATIONS:\n",
        "            return df\n",
        "\n",
        "        if fit:\n",
        "            print(\"\\\\n4. Removing high correlations...\")\n",
        "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
        "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
        "\n",
        "            if len(numeric) > 1:\n",
        "                corr = df[numeric].corr().abs()\n",
        "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "                self.features_to_drop_corr = [c for c in upper.columns\n",
        "                                             if any(upper[c] > config.CORRELATION_THRESHOLD)]\n",
        "\n",
        "                if self.features_to_drop_corr:\n",
        "                    df = df.drop(columns=self.features_to_drop_corr)\n",
        "                    print(f\"   Removed: {len(self.features_to_drop_corr)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _align_columns(self, df, name):\n",
        "        preserve = [c for c in config.ID_COLUMNS if c in df.columns]\n",
        "        if config.TARGET_COLUMN in df.columns:\n",
        "            preserve.append(config.TARGET_COLUMN)\n",
        "\n",
        "        current = [c for c in df.columns if c not in preserve]\n",
        "        missing = [c for c in self.final_features if c not in current]\n",
        "        extra = [c for c in current if c not in self.final_features]\n",
        "\n",
        "        if missing:\n",
        "            for c in missing:\n",
        "                df[c] = 0\n",
        "\n",
        "        if extra:\n",
        "            df = df.drop(columns=extra)\n",
        "\n",
        "        order = preserve + self.final_features\n",
        "        df = df[[c for c in order if c in df.columns]]\n",
        "\n",
        "        return df\n",
        "\n",
        "# Apply preprocessing\n",
        "pipeline = PreprocessingPipeline(config)\n",
        "train_processed = pipeline.fit_transform(train_df)\n",
        "val_processed = pipeline.transform(val_df, 'validation')\n",
        "test_processed = pipeline.transform(test_df, 'test (OOT)')\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "\n",
        "# Сохранение продолжается...\n",
        "# Из-за ограничения размера, продолжу в следующем файле\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7. РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\n",
        "\n",
        "Создание двух датасетов:\n",
        "- **Модель 1:** Small Business\n",
        "- **Модель 2:** Middle + Large Business\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# SEGMENT SPLIT\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Модель 1: Small Business\n",
        "seg1_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
        "seg1_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
        "seg1_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
        "\n",
        "print(f\"\\n{config.SEGMENT_1_NAME}:\")\n",
        "print(f\"  Train: {len(seg1_train):,} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
        "print(f\"  Val: {len(seg1_val):,} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
        "print(f\"  Test: {len(seg1_test):,} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
        "\n",
        "# Модель 2: Middle + Large Business\n",
        "seg2_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
        "seg2_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
        "seg2_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
        "\n",
        "print(f\"\\n{config.SEGMENT_2_NAME}:\")\n",
        "print(f\"  Train: {len(seg2_train):,} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
        "print(f\"  Val: {len(seg2_val):,} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
        "print(f\"  Test: {len(seg2_test):,} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 8. МОДЕЛЬ 1: МАЛЫЙ БИЗНЕС (SMALL BUSINESS)\n",
        "\n",
        "Обучение модели на данных малого бизнеса\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ====================================================================================\n",
        "\n",
        "def prepare_data_for_catboost(df, categorical_features, exclude_cols):\n",
        "    \"\"\"Подготовка данных для CatBoost\"\"\"\n",
        "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "    X = df[feature_cols].copy()\n",
        "    y = df[config.TARGET_COLUMN].copy() if config.TARGET_COLUMN in df.columns else None\n",
        "\n",
        "    # Конвертация категориальных в string\n",
        "    for cat in categorical_features:\n",
        "        if cat in X.columns:\n",
        "            X[cat] = X[cat].astype(str).replace('nan', 'missing')\n",
        "\n",
        "    # Индексы категориальных\n",
        "    cat_indices = [i for i, c in enumerate(feature_cols) if c in categorical_features]\n",
        "\n",
        "    return X, y, cat_indices\n",
        "\n",
        "\n",
        "def calculate_class_weights(y):\n",
        "    \"\"\"Расчет весов классов\"\"\"\n",
        "    n_samples = len(y)\n",
        "    n_classes = 2\n",
        "    n_class_0 = (y == 0).sum()\n",
        "    n_class_1 = (y == 1).sum()\n",
        "\n",
        "    weight_0 = n_samples / (n_classes * n_class_0)\n",
        "    weight_1 = n_samples / (n_classes * n_class_1)\n",
        "\n",
        "    weights = np.ones(len(y))\n",
        "    weights[y == 1] = weight_1\n",
        "    weights[y == 0] = weight_0\n",
        "\n",
        "    return weights, weight_0, weight_1\n",
        "\n",
        "\n",
        "def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):\n",
        "    \"\"\"Поиск оптимального порога\"\"\"\n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "    scores = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "        if metric == 'f1':\n",
        "            score = f1_score(y_true, y_pred)\n",
        "        elif metric == 'recall':\n",
        "            score = recall_score(y_true, y_pred)\n",
        "        elif metric == 'precision':\n",
        "            score = precision_score(y_true, y_pred)\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    optimal_idx = np.argmax(scores)\n",
        "    return thresholds[optimal_idx], scores[optimal_idx]\n",
        "\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred_proba, y_pred, threshold, dataset_name=''):\n",
        "    \"\"\"Расчет всех метрик\"\"\"\n",
        "    metrics = {\n",
        "        'dataset': dataset_name,\n",
        "        'threshold': threshold,\n",
        "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
        "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "    }\n",
        "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    metrics['tn'] = cm[0, 0]\n",
        "    metrics['fp'] = cm[0, 1]\n",
        "    metrics['fn'] = cm[1, 0]\n",
        "    metrics['tp'] = cm[1, 1]\n",
        "\n",
        "    return metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# МОДЕЛЬ 1: SMALL BUSINESS - ПОДГОТОВКА ДАННЫХ\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"МОДЕЛЬ 1: {config.SEGMENT_1_NAME.upper()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Подготовка\n",
        "X_train_1, y_train_1, cat_idx_1 = prepare_data_for_catboost(\n",
        "    seg1_train, config.CATEGORICAL_FEATURES,\n",
        "    config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        ")\n",
        "X_val_1, y_val_1, _ = prepare_data_for_catboost(\n",
        "    seg1_val, config.CATEGORICAL_FEATURES,\n",
        "    config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        ")\n",
        "X_test_1, y_test_1, _ = prepare_data_for_catboost(\n",
        "    seg1_test, config.CATEGORICAL_FEATURES,\n",
        "    config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        ")\n",
        "\n",
        "print(f\"\\nДанные подготовлены:\")\n",
        "print(f\"  X_train: {X_train_1.shape}\")\n",
        "print(f\"  X_val: {X_val_1.shape}\")\n",
        "print(f\"  X_test: {X_test_1.shape}\")\n",
        "print(f\"  Категориальных признаков: {len(cat_idx_1)}\")\n",
        "\n",
        "# Class weights\n",
        "if config.USE_CLASS_WEIGHTS:\n",
        "    weights_1, w0_1, w1_1 = calculate_class_weights(y_train_1)\n",
        "    print(f\"\\nВеса классов:\")\n",
        "    print(f\"  Class 0: {w0_1:.4f}\")\n",
        "    print(f\"  Class 1: {w1_1:.4f}\")\n",
        "    print(f\"  Ratio: 1:{w1_1/w0_1:.1f}\")\n",
        "else:\n",
        "    weights_1 = None\n",
        "\n",
        "# CatBoost Pools\n",
        "train_pool_1 = Pool(X_train_1, y_train_1, cat_features=cat_idx_1, weight=weights_1)\n",
        "val_pool_1 = Pool(X_val_1, y_val_1, cat_features=cat_idx_1)\n",
        "test_pool_1 = Pool(X_test_1, y_test_1, cat_features=cat_idx_1)\n",
        "\n",
        "print(f\"\\n✓ CatBoost Pools созданы\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# МОДЕЛЬ 1: ОБУЧЕНИЕ\n",
        "# ====================================================================================\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"ОБУЧЕНИЕ МОДЕЛИ 1\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "model_1 = CatBoostClassifier(**config.CATBOOST_PARAMS)\n",
        "\n",
        "print(\"\\nНачало обучения...\")\n",
        "model_1.fit(train_pool_1, eval_set=val_pool_1, plot=False)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено\")\n",
        "print(f\"  Best iteration: {model_1.best_iteration_}\")\n",
        "print(f\"  Best validation AUC: {model_1.best_score_['validation']['AUC']:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# МОДЕЛЬ 1: ОЦЕНКА И МЕТРИКИ\n",
        "# ====================================================================================\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"ОЦЕНКА МОДЕЛИ 1\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Predictions\n",
        "y_val_pred_proba_1 = model_1.predict_proba(val_pool_1)[:, 1]\n",
        "y_test_pred_proba_1 = model_1.predict_proba(test_pool_1)[:, 1]\n",
        "\n",
        "# Optimal threshold\n",
        "optimal_threshold_1, optimal_score_1 = find_optimal_threshold(\n",
        "    y_val_1, y_val_pred_proba_1, config.THRESHOLD_METRIC\n",
        ")\n",
        "\n",
        "print(f\"\\nОптимальный порог: {optimal_threshold_1:.3f}\")\n",
        "print(f\"  {config.THRESHOLD_METRIC.upper()} на валидации: {optimal_score_1:.4f}\")\n",
        "\n",
        "# Predictions with optimal threshold\n",
        "y_val_pred_1 = (y_val_pred_proba_1 >= optimal_threshold_1).astype(int)\n",
        "y_test_pred_1 = (y_test_pred_proba_1 >= optimal_threshold_1).astype(int)\n",
        "\n",
        "# Metrics\n",
        "val_metrics_1 = calculate_all_metrics(y_val_1, y_val_pred_proba_1, y_val_pred_1,\n",
        "                                      optimal_threshold_1, 'Validation')\n",
        "test_metrics_1 = calculate_all_metrics(y_test_1, y_test_pred_proba_1, y_test_pred_1,\n",
        "                                       optimal_threshold_1, 'Test (OOT)')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\n{'Метрика':<20} {'Validation':<15} {'Test (OOT)':<15}\")\n",
        "print(\"-\"*50)\n",
        "print(f\"{'ROC-AUC':<20} {val_metrics_1['roc_auc']:<15.4f} {test_metrics_1['roc_auc']:<15.4f}\")\n",
        "print(f\"{'GINI':<20} {val_metrics_1['gini']:<15.4f} {test_metrics_1['gini']:<15.4f}\")\n",
        "print(f\"{'PR-AUC':<20} {val_metrics_1['pr_auc']:<15.4f} {test_metrics_1['pr_auc']:<15.4f}\")\n",
        "print(f\"{'Accuracy':<20} {val_metrics_1['accuracy']:<15.4f} {test_metrics_1['accuracy']:<15.4f}\")\n",
        "print(f\"{'Precision':<20} {val_metrics_1['precision']:<15.4f} {test_metrics_1['precision']:<15.4f}\")\n",
        "print(f\"{'Recall':<20} {val_metrics_1['recall']:<15.4f} {test_metrics_1['recall']:<15.4f}\")\n",
        "print(f\"{'F1-Score':<20} {val_metrics_1['f1']:<15.4f} {test_metrics_1['f1']:<15.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix (Test OOT):\")\n",
        "print(f\"  TN: {test_metrics_1['tn']:,}  FP: {test_metrics_1['fp']:,}\")\n",
        "print(f\"  FN: {test_metrics_1['fn']:,}  TP: {test_metrics_1['tp']:,}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 9. МОДЕЛЬ 2: СРЕДНИЙ + КРУПНЫЙ БИЗНЕС\n",
        "\n",
        "Обучение модели на объединенных данных среднего и крупного бизнеса\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# МОДЕЛЬ 2: MIDDLE + LARGE BUSINESS - ПОДГОТОВКА\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"МОДЕЛЬ 2: {config.SEGMENT_2_NAME.upper()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Подготовка\n",
        "X_train_2, y_train_2, cat_idx_2 = prepare_data_for_catboost(\n",
        "    seg2_train, config.CATEGORICAL_FEATURES,\n",
        "    config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        ")\n",
        "X_val_2, y_val_2, _ = prepare_data_for_catboost(\n",
        "    seg2_val, config.CATEGORICAL_FEATURES,\n",
        "    config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        ")\n",
        "X_test_2, y_test_2, _ = prepare_data_for_catboost(\n",
        "    seg2_test, config.CATEGORICAL_FEATURES,\n",
        "    config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
        ")\n",
        "\n",
        "print(f\"\\nДанные подготовлены:\")\n",
        "print(f\"  X_train: {X_train_2.shape}\")\n",
        "print(f\"  X_val: {X_val_2.shape}\")\n",
        "print(f\"  X_test: {X_test_2.shape}\")\n",
        "print(f\"  Категориальных признаков: {len(cat_idx_2)}\")\n",
        "\n",
        "# Class weights\n",
        "if config.USE_CLASS_WEIGHTS:\n",
        "    weights_2, w0_2, w1_2 = calculate_class_weights(y_train_2)\n",
        "    print(f\"\\nВеса классов:\")\n",
        "    print(f\"  Class 0: {w0_2:.4f}\")\n",
        "    print(f\"  Class 1: {w1_2:.4f}\")\n",
        "    print(f\"  Ratio: 1:{w1_2/w0_2:.1f}\")\n",
        "else:\n",
        "    weights_2 = None\n",
        "\n",
        "# CatBoost Pools\n",
        "train_pool_2 = Pool(X_train_2, y_train_2, cat_features=cat_idx_2, weight=weights_2)\n",
        "val_pool_2 = Pool(X_val_2, y_val_2, cat_features=cat_idx_2)\n",
        "test_pool_2 = Pool(X_test_2, y_test_2, cat_features=cat_idx_2)\n",
        "\n",
        "print(f\"\\n✓ CatBoost Pools созданы\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# МОДЕЛЬ 2: ОБУЧЕНИЕ\n",
        "# ====================================================================================\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"ОБУЧЕНИЕ МОДЕЛИ 2\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "model_2 = CatBoostClassifier(**config.CATBOOST_PARAMS)\n",
        "\n",
        "print(\"\\nНачало обучения...\")\n",
        "model_2.fit(train_pool_2, eval_set=val_pool_2, plot=False)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено\")\n",
        "print(f\"  Best iteration: {model_2.best_iteration_}\")\n",
        "print(f\"  Best validation AUC: {model_2.best_score_['validation']['AUC']:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# МОДЕЛЬ 2: ОЦЕНКА И МЕТРИКИ\n",
        "# ====================================================================================\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"ОЦЕНКА МОДЕЛИ 2\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Predictions\n",
        "y_val_pred_proba_2 = model_2.predict_proba(val_pool_2)[:, 1]\n",
        "y_test_pred_proba_2 = model_2.predict_proba(test_pool_2)[:, 1]\n",
        "\n",
        "# Optimal threshold\n",
        "optimal_threshold_2, optimal_score_2 = find_optimal_threshold(\n",
        "    y_val_2, y_val_pred_proba_2, config.THRESHOLD_METRIC\n",
        ")\n",
        "\n",
        "print(f\"\\nОптимальный порог: {optimal_threshold_2:.3f}\")\n",
        "print(f\"  {config.THRESHOLD_METRIC.upper()} на валидации: {optimal_score_2:.4f}\")\n",
        "\n",
        "# Predictions with optimal threshold\n",
        "y_val_pred_2 = (y_val_pred_proba_2 >= optimal_threshold_2).astype(int)\n",
        "y_test_pred_2 = (y_test_pred_proba_2 >= optimal_threshold_2).astype(int)\n",
        "\n",
        "# Metrics\n",
        "val_metrics_2 = calculate_all_metrics(y_val_2, y_val_pred_proba_2, y_val_pred_2,\n",
        "                                      optimal_threshold_2, 'Validation')\n",
        "test_metrics_2 = calculate_all_metrics(y_test_2, y_test_pred_proba_2, y_test_pred_2,\n",
        "                                       optimal_threshold_2, 'Test (OOT)')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\n{'Метрика':<20} {'Validation':<15} {'Test (OOT)':<15}\")\n",
        "print(\"-\"*50)\n",
        "print(f\"{'ROC-AUC':<20} {val_metrics_2['roc_auc']:<15.4f} {test_metrics_2['roc_auc']:<15.4f}\")\n",
        "print(f\"{'GINI':<20} {val_metrics_2['gini']:<15.4f} {test_metrics_2['gini']:<15.4f}\")\n",
        "print(f\"{'PR-AUC':<20} {val_metrics_2['pr_auc']:<15.4f} {test_metrics_2['pr_auc']:<15.4f}\")\n",
        "print(f\"{'Accuracy':<20} {val_metrics_2['accuracy']:<15.4f} {test_metrics_2['accuracy']:<15.4f}\")\n",
        "print(f\"{'Precision':<20} {val_metrics_2['precision']:<15.4f} {test_metrics_2['precision']:<15.4f}\")\n",
        "print(f\"{'Recall':<20} {val_metrics_2['recall']:<15.4f} {test_metrics_2['recall']:<15.4f}\")\n",
        "print(f\"{'F1-Score':<20} {val_metrics_2['f1']:<15.4f} {test_metrics_2['f1']:<15.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix (Test OOT):\")\n",
        "print(f\"  TN: {test_metrics_2['tn']:,}  FP: {test_metrics_2['fp']:,}\")\n",
        "print(f\"  FN: {test_metrics_2['fn']:,}  TP: {test_metrics_2['tp']:,}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 10. СРАВНЕНИЕ МОДЕЛЕЙ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# СРАВНЕНИЕ МОДЕЛЕЙ\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"СРАВНЕНИЕ МОДЕЛЕЙ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Таблица сравнения\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Модель': config.SEGMENT_1_NAME,\n",
        "        'Сегмент': ', '.join(config.SEGMENT_1_VALUES),\n",
        "        'Train Size': len(X_train_1),\n",
        "        'Test Size': len(X_test_1),\n",
        "        'Test Churn %': y_test_1.mean() * 100,\n",
        "        'ROC-AUC': test_metrics_1['roc_auc'],\n",
        "        'GINI': test_metrics_1['gini'],\n",
        "        'PR-AUC': test_metrics_1['pr_auc'],\n",
        "        'Precision': test_metrics_1['precision'],\n",
        "        'Recall': test_metrics_1['recall'],\n",
        "        'F1-Score': test_metrics_1['f1'],\n",
        "        'Threshold': optimal_threshold_1\n",
        "    },\n",
        "    {\n",
        "        'Модель': config.SEGMENT_2_NAME,\n",
        "        'Сегмент': ', '.join(config.SEGMENT_2_VALUES),\n",
        "        'Train Size': len(X_train_2),\n",
        "        'Test Size': len(X_test_2),\n",
        "        'Test Churn %': y_test_2.mean() * 100,\n",
        "        'ROC-AUC': test_metrics_2['roc_auc'],\n",
        "        'GINI': test_metrics_2['gini'],\n",
        "        'PR-AUC': test_metrics_2['pr_auc'],\n",
        "        'Precision': test_metrics_2['precision'],\n",
        "        'Recall': test_metrics_2['recall'],\n",
        "        'F1-Score': test_metrics_2['f1'],\n",
        "        'Threshold': optimal_threshold_2\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\nТаблица сравнения (Test OOT):\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ВИЗУАЛИЗАЦИЯ: СРАВНЕНИЕ МОДЕЛЕЙ\n",
        "# ====================================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "models_data = [\n",
        "    (\"Модель 1: \" + config.SEGMENT_1_NAME, test_metrics_1, y_test_1, y_test_pred_proba_1, 'blue'),\n",
        "    (\"Модель 2: \" + config.SEGMENT_2_NAME, test_metrics_2, y_test_2, y_test_pred_proba_2, 'green')\n",
        "]\n",
        "\n",
        "# Основные метрики\n",
        "metrics_to_plot = ['roc_auc', 'gini', 'pr_auc', 'precision', 'recall', 'f1']\n",
        "metric_names = ['ROC-AUC', 'GINI', 'PR-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "for idx, metric_key in enumerate(metrics_to_plot):\n",
        "    row, col = idx // 3, idx % 3\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    values = [models_data[0][1][metric_key], models_data[1][1][metric_key]]\n",
        "    colors = [models_data[0][4], models_data[1][4]]\n",
        "\n",
        "    bars = ax.bar(range(len(values)), values, color=colors, alpha=0.7, edgecolor='black')\n",
        "    ax.set_xticks(range(len(values)))\n",
        "    ax.set_xticklabels([m[0] for m in models_data], rotation=15, ha='right')\n",
        "    ax.set_title(f'{metric_names[idx]} (Test OOT)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylim(0, max(values) * 1.2)\n",
        "\n",
        "    for i, v in enumerate(values):\n",
        "        ax.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(config.FIGURES_DIR / '02_models_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Сохранено: figures/02_models_comparison.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ROC CURVES\n",
        "# ====================================================================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for model_name, metrics, y_true, y_pred_proba, color in models_data:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    auc = metrics['roc_auc']\n",
        "\n",
        "    ax.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC={auc:.4f})',\n",
        "            color=color)\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random')\n",
        "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
        "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
        "ax.set_title('ROC Curves - Model Comparison (Test OOT)', fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(config.FIGURES_DIR / '03_roc_curves_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Сохранено: figures/03_roc_curves_comparison.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 11. ФИНАЛЬНЫЕ МЕТРИКИ ДЛЯ ОТЧЕТА БАНКУ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ИТОГОВЫЙ ОТЧЕТ\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ИТОГОВЫЙ ОТЧЕТ ДЛЯ БАНКА\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nДата создания: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Random seed: {config.RANDOM_SEED}\")\n",
        "print(f\"Алгоритм: CatBoost\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"МОДЕЛЬ 1: \" + config.SEGMENT_1_NAME.upper())\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Сегмент: {', '.join(config.SEGMENT_1_VALUES)}\")\n",
        "print(f\"Train samples: {len(X_train_1):,}\")\n",
        "print(f\"Test samples (OOT): {len(X_test_1):,}\")\n",
        "print(f\"Test churn rate: {y_test_1.mean()*100:.2f}%\")\n",
        "print(f\"Optimal threshold: {optimal_threshold_1:.3f}\")\n",
        "\n",
        "print(f\"\\nМЕТРИКИ (Test OOT):\")\n",
        "print(f\"  ROC-AUC:    {test_metrics_1['roc_auc']:.4f}\")\n",
        "print(f\"  GINI:       {test_metrics_1['gini']:.4f}\")\n",
        "print(f\"  PR-AUC:     {test_metrics_1['pr_auc']:.4f}\")\n",
        "print(f\"  Accuracy:   {test_metrics_1['accuracy']:.4f}\")\n",
        "print(f\"  Precision:  {test_metrics_1['precision']:.4f}\")\n",
        "print(f\"  Recall:     {test_metrics_1['recall']:.4f}\")\n",
        "print(f\"  F1-Score:   {test_metrics_1['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nCONFUSION MATRIX:\")\n",
        "print(f\"  TN: {test_metrics_1['tn']:>8,}  |  FP: {test_metrics_1['fp']:>8,}\")\n",
        "print(f\"  FN: {test_metrics_1['fn']:>8,}  |  TP: {test_metrics_1['tp']:>8,}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"МОДЕЛЬ 2: \" + config.SEGMENT_2_NAME.upper())\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Сегмент: {', '.join(config.SEGMENT_2_VALUES)}\")\n",
        "print(f\"Train samples: {len(X_train_2):,}\")\n",
        "print(f\"Test samples (OOT): {len(X_test_2):,}\")\n",
        "print(f\"Test churn rate: {y_test_2.mean()*100:.2f}%\")\n",
        "print(f\"Optimal threshold: {optimal_threshold_2:.3f}\")\n",
        "\n",
        "print(f\"\\nМЕТРИКИ (Test OOT):\")\n",
        "print(f\"  ROC-AUC:    {test_metrics_2['roc_auc']:.4f}\")\n",
        "print(f\"  GINI:       {test_metrics_2['gini']:.4f}\")\n",
        "print(f\"  PR-AUC:     {test_metrics_2['pr_auc']:.4f}\")\n",
        "print(f\"  Accuracy:   {test_metrics_2['accuracy']:.4f}\")\n",
        "print(f\"  Precision:  {test_metrics_2['precision']:.4f}\")\n",
        "print(f\"  Recall:     {test_metrics_2['recall']:.4f}\")\n",
        "print(f\"  F1-Score:   {test_metrics_2['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nCONFUSION MATRIX:\")\n",
        "print(f\"  TN: {test_metrics_2['tn']:>8,}  |  FP: {test_metrics_2['fp']:>8,}\")\n",
        "print(f\"  FN: {test_metrics_2['fn']:>8,}  |  TP: {test_metrics_2['tp']:>8,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 12. СОХРАНЕНИЕ МОДЕЛЕЙ И РЕЗУЛЬТАТОВ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# СОХРАНЕНИЕ МОДЕЛЕЙ\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"СОХРАНЕНИЕ МОДЕЛЕЙ И РЕЗУЛЬТАТОВ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Модель 1\n",
        "model_1_path = config.MODEL_DIR / \"model_1_small_business.cbm\"\n",
        "model_1.save_model(model_1_path)\n",
        "print(f\"\\n✓ Модель 1 сохранена: {model_1_path}\")\n",
        "\n",
        "# Модель 2\n",
        "model_2_path = config.MODEL_DIR / \"model_2_middle_large_business.cbm\"\n",
        "model_2.save_model(model_2_path)\n",
        "print(f\"✓ Модель 2 сохранена: {model_2_path}\")\n",
        "\n",
        "# Метаданные модели 1\n",
        "metadata_1 = {\n",
        "    'model_name': config.SEGMENT_1_NAME,\n",
        "    'segments': config.SEGMENT_1_VALUES,\n",
        "    'created_date': datetime.now().isoformat(),\n",
        "    'random_seed': config.RANDOM_SEED,\n",
        "    'algorithm': 'CatBoost',\n",
        "    'catboost_params': config.CATBOOST_PARAMS,\n",
        "    'train_samples': len(X_train_1),\n",
        "    'test_samples': len(X_test_1),\n",
        "    'features': X_train_1.columns.tolist(),\n",
        "    'categorical_features': config.CATEGORICAL_FEATURES,\n",
        "    'optimal_threshold': float(optimal_threshold_1),\n",
        "    'test_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else v\n",
        "                    for k, v in test_metrics_1.items()},\n",
        "    'class_weights': {'class_0': float(w0_1), 'class_1': float(w1_1)} if config.USE_CLASS_WEIGHTS else None\n",
        "}\n",
        "\n",
        "metadata_1_path = config.MODEL_DIR / \"model_1_metadata.json\"\n",
        "with open(metadata_1_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata_1, f, indent=2, ensure_ascii=False)\n",
        "print(f\"✓ Метаданные модели 1: {metadata_1_path}\")\n",
        "\n",
        "# Метаданные модели 2\n",
        "metadata_2 = {\n",
        "    'model_name': config.SEGMENT_2_NAME,\n",
        "    'segments': config.SEGMENT_2_VALUES,\n",
        "    'created_date': datetime.now().isoformat(),\n",
        "    'random_seed': config.RANDOM_SEED,\n",
        "    'algorithm': 'CatBoost',\n",
        "    'catboost_params': config.CATBOOST_PARAMS,\n",
        "    'train_samples': len(X_train_2),\n",
        "    'test_samples': len(X_test_2),\n",
        "    'features': X_train_2.columns.tolist(),\n",
        "    'categorical_features': config.CATEGORICAL_FEATURES,\n",
        "    'optimal_threshold': float(optimal_threshold_2),\n",
        "    'test_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else v\n",
        "                    for k, v in test_metrics_2.items()},\n",
        "    'class_weights': {'class_0': float(w0_2), 'class_1': float(w1_2)} if config.USE_CLASS_WEIGHTS else None\n",
        "}\n",
        "\n",
        "metadata_2_path = config.MODEL_DIR / \"model_2_metadata.json\"\n",
        "with open(metadata_2_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata_2, f, indent=2, ensure_ascii=False)\n",
        "print(f\"✓ Метаданные модели 2: {metadata_2_path}\")\n",
        "\n",
        "# Comparison table\n",
        "comparison_df.to_csv(config.OUTPUT_DIR / \"models_comparison.csv\", index=False)\n",
        "print(f\"✓ Таблица сравнения: {config.OUTPUT_DIR / 'models_comparison.csv'}\")\n",
        "\n",
        "# Preprocessing pipeline\n",
        "pipeline_path = config.MODEL_DIR / \"preprocessing_pipeline.pkl\"\n",
        "with open(pipeline_path, 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "print(f\"✓ Preprocessing pipeline: {pipeline_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ ВСЕ МОДЕЛИ И РЕЗУЛЬТАТЫ СОХРАНЕНЫ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nФайлы:\")\n",
        "print(f\"  Модели:\")\n",
        "print(f\"    - {model_1_path}\")\n",
        "print(f\"    - {model_2_path}\")\n",
        "print(f\"  Метаданные:\")\n",
        "print(f\"    - {metadata_1_path}\")\n",
        "print(f\"    - {metadata_2_path}\")\n",
        "print(f\"  Результаты:\")\n",
        "print(f\"    - {config.OUTPUT_DIR / 'models_comparison.csv'}\")\n",
        "print(f\"  Preprocessing:\")\n",
        "print(f\"    - {pipeline_path}\")\n",
        "print(f\"  Визуализации:\")\n",
        "print(f\"    - {config.FIGURES_DIR}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================================================\n",
        "# ФИНАЛ\n",
        "# ====================================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"✓✓✓ ПОСТРОЕНИЕ МОДЕЛЕЙ ЗАВЕРШЕНО УСПЕШНО ✓✓✓\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nИтоговые метрики (Test OOT):\")\n",
        "print(f\"\\n{config.SEGMENT_1_NAME}:\")\n",
        "print(f\"  GINI: {test_metrics_1['gini']:.4f} | ROC-AUC: {test_metrics_1['roc_auc']:.4f} | F1: {test_metrics_1['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n{config.SEGMENT_2_NAME}:\")\n",
        "print(f\"  GINI: {test_metrics_2['gini']:.4f} | ROC-AUC: {test_metrics_2['roc_auc']:.4f} | F1: {test_metrics_2['f1']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Воспроизводимость: Run All → идентичные результаты (random_seed=42)\")\n",
        "print(\"=\"*80)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}