{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ===============================================================================\n",
    "# ФИНАЛЬНЫЕ PRODUCTION МОДЕЛИ + SHAP ANALYSIS\n",
    "# ===============================================================================\n",
    "\n",
    "**Цель:** Воспроизводимый notebook с лучшими моделями для каждого сегмента\n",
    "\n",
    "**Содержание:**\n",
    "1. Загрузка подготовленных данных\n",
    "2. Загрузка результатов экспериментов → выбор лучших моделей\n",
    "3. Обучение лучших моделей с оптимальными параметрами\n",
    "4. SHAP Analysis (Feature Importance + Explainability)\n",
    "5. Полные метрики и визуализации\n",
    "6. Сохранение финальных моделей\n",
    "\n",
    "**Reproducibility:** Random seed = 42, Run All должен давать те же результаты\n",
    "\n",
    "**Дата:** 2025-01-13\n",
    "\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML Models\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ФИНАЛЬНЫЕ PRODUCTION МОДЕЛИ + SHAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация\"\"\"\n",
    "    \n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # ПУТИ\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    MODELS_DIR = Path(\"models\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # КОЛОНКИ\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    \n",
    "    # СЕГМЕНТЫ\n",
    "    SEGMENTS = {\n",
    "        'Segment 1': {\n",
    "            'name': 'Small Business',\n",
    "            'train': 'seg1_train.parquet',\n",
    "            'val': 'seg1_val.parquet',\n",
    "            'test': 'seg1_test.parquet'\n",
    "        },\n",
    "        'Segment 2': {\n",
    "            'name': 'Middle + Large Business',\n",
    "            'train': 'seg2_train.parquet',\n",
    "            'val': 'seg2_val.parquet',\n",
    "            'test': 'seg2_test.parquet'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # SHAP PARAMETERS\n",
    "    SHAP_SAMPLE_SIZE = 100  # Для Tree SHAP используем все данные или sample\n",
    "    SHAP_TOP_FEATURES = 20  # Топ признаков для визуализации\n",
    "    \n",
    "    # ВИЗУАЛИЗАЦИЯ\n",
    "    FIGURE_SIZE = (12, 8)\n",
    "    FIGURE_DPI = 100\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.MODELS_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегментов: {len(config.SEGMENTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ПОДГОТОВЛЕННЫХ ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for seg_id, seg_info in config.SEGMENTS.items():\n",
    "    print(f\"\\n{seg_id}: {seg_info['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    data[seg_id] = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = config.OUTPUT_DIR / seg_info[split]\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Файл не найден: {file_path}\\n\"\n",
    "                f\"Сначала запустите notebook 01_data_preparation_eda.ipynb\"\n",
    "            )\n",
    "        \n",
    "        df = pd.read_parquet(file_path)\n",
    "        data[seg_id][split] = df\n",
    "        \n",
    "        churn_rate = df[config.TARGET_COLUMN].mean()\n",
    "        print(f\"  {split.upper():5s}: {df.shape} | Churn: {churn_rate*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все данные загружены успешно\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ВЫБОР ЛУЧШИХ МОДЕЛЕЙ ИЗ ЭКСПЕРИМЕНТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА РЕЗУЛЬТАТОВ ЭКСПЕРИМЕНТОВ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВЫБОР ЛУЧШИХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Попытка загрузить результаты экспериментов\n",
    "experiments_file = config.OUTPUT_DIR / 'experiments_all.csv'\n",
    "\n",
    "if experiments_file.exists():\n",
    "    experiments_df = pd.read_csv(experiments_file)\n",
    "    print(f\"\\n✓ Загружены результаты: {experiments_file}\")\n",
    "    print(f\"  Всего экспериментов: {len(experiments_df)}\")\n",
    "    \n",
    "    # Выбираем лучшую модель для каждого сегмента\n",
    "    best_models_config = {}\n",
    "    \n",
    "    for seg_id in config.SEGMENTS.keys():\n",
    "        seg_experiments = experiments_df[experiments_df['segment_group'] == seg_id]\n",
    "        best_exp = seg_experiments.sort_values('roc_auc', ascending=False).iloc[0]\n",
    "        \n",
    "        best_models_config[seg_id] = {\n",
    "            'algorithm': best_exp['algorithm'],\n",
    "            'balancing_method': best_exp['balancing_method'],\n",
    "            'roc_auc': best_exp['roc_auc'],\n",
    "            'threshold': best_exp['threshold']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  Лучшая модель: {best_exp['algorithm']}\")\n",
    "        print(f\"  Балансировка: {best_exp['balancing_method']}\")\n",
    "        print(f\"  ROC-AUC (test): {best_exp['roc_auc']:.4f}\")\n",
    "        print(f\"  Gini: {best_exp['gini']:.4f}\")\n",
    "        print(f\"  F1: {best_exp['f1']:.4f}\")\n",
    "        print(f\"  Optimal Threshold: {best_exp['threshold']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Результаты экспериментов не найдены: {experiments_file}\")\n",
    "    print(\"\\nИспользуем параметры по умолчанию для каждого сегмента:\")\n",
    "    \n",
    "    # Fallback: используем разумные defaults\n",
    "    best_models_config = {\n",
    "        'Segment 1': {\n",
    "            'algorithm': 'LightGBM',\n",
    "            'balancing_method': 'SMOTE',\n",
    "            'threshold': 0.5\n",
    "        },\n",
    "        'Segment 2': {\n",
    "            'algorithm': 'CatBoost',\n",
    "            'balancing_method': 'Class weights',\n",
    "            'threshold': 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
    "# ====================================================================================\n",
    "\n",
    "def prepare_data(df, target_col):\n",
    "    \"\"\"Разделение на X и y\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def apply_balancing(X_train, y_train, method, random_seed=42):\n",
    "    \"\"\"\n",
    "    Применить метод балансировки.\n",
    "    \"\"\"\n",
    "    if method == 'No balancing':\n",
    "        return X_train.copy(), y_train.copy(), None\n",
    "    \n",
    "    elif method == 'Class weights':\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        sample_weights = np.array([class_weights[int(y)] for y in y_train])\n",
    "        return X_train.copy(), y_train.copy(), sample_weights\n",
    "    \n",
    "    elif method == 'SMOTE':\n",
    "        smote = SMOTE(random_state=random_seed, k_neighbors=5)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    elif method == 'Random Undersampling':\n",
    "        rus = RandomUnderSampler(random_state=random_seed)\n",
    "        X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    elif method == 'SMOTE + Undersampling':\n",
    "        smote = SMOTE(random_state=random_seed, k_neighbors=5)\n",
    "        X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "        rus = RandomUnderSampler(random_state=random_seed)\n",
    "        X_res, y_res = rus.fit_resample(X_smote, y_smote)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown balancing method: {method}\")\n",
    "\n",
    "\n",
    "def train_model(algorithm, X_train, y_train, X_val, y_val, sample_weights=None, random_seed=42):\n",
    "    \"\"\"\n",
    "    Обучить модель с оптимальными параметрами.\n",
    "    \"\"\"\n",
    "    if algorithm == 'CatBoost':\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "            random_seed=random_seed,\n",
    "            task_type='CPU',\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        from catboost import Pool\n",
    "        train_pool = Pool(X_train, y_train, weight=sample_weights)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "        model.fit(train_pool, eval_set=val_pool)\n",
    "        \n",
    "    elif algorithm == 'LightGBM':\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            random_state=random_seed,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[]\n",
    "        )\n",
    "        \n",
    "    elif algorithm == 'XGBoost':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            random_state=random_seed,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "    elif algorithm == 'RandomForest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=100,\n",
    "            random_state=random_seed,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, threshold):\n",
    "    \"\"\"\n",
    "    Рассчитать все метрики.\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'] = cm[0, 0]\n",
    "    metrics['fp'] = cm[0, 1]\n",
    "    metrics['fn'] = cm[1, 0]\n",
    "    metrics['tp'] = cm[1, 1]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"✓ Вспомогательные функции определены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-best-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ ДЛЯ КАЖДОГО СЕГМЕНТА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_models = {}\n",
    "final_results = {}\n",
    "\n",
    "for seg_id, model_config in best_models_config.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Алгоритм: {model_config['algorithm']}\")\n",
    "    print(f\"  Балансировка: {model_config['balancing_method']}\")\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train, y_train = prepare_data(data[seg_id]['train'], config.TARGET_COLUMN)\n",
    "    X_val, y_val = prepare_data(data[seg_id]['val'], config.TARGET_COLUMN)\n",
    "    X_test, y_test = prepare_data(data[seg_id]['test'], config.TARGET_COLUMN)\n",
    "    \n",
    "    # Применение балансировки\n",
    "    X_train_balanced, y_train_balanced, sample_weights = apply_balancing(\n",
    "        X_train, y_train, \n",
    "        model_config['balancing_method'],\n",
    "        config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Размер после балансировки: {X_train_balanced.shape}\")\n",
    "    print(f\"  Churn rate: {y_train_balanced.mean()*100:.2f}%\")\n",
    "    \n",
    "    # Обучение модели\n",
    "    print(f\"\\n  Обучение модели...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = train_model(\n",
    "        model_config['algorithm'],\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        X_val, y_val,\n",
    "        sample_weights,\n",
    "        config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"  ✓ Обучение завершено за {train_time:.2f} сек\")\n",
    "    \n",
    "    # Предсказания\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Метрики\n",
    "    threshold = model_config.get('threshold', 0.5)\n",
    "    metrics = calculate_metrics(y_test, y_test_proba, threshold)\n",
    "    \n",
    "    print(f\"\\n  МЕТРИКИ НА TEST:\")\n",
    "    print(f\"    ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"    Gini:      {metrics['gini']:.4f}\")\n",
    "    print(f\"    F1-Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"    Threshold: {threshold:.4f}\")\n",
    "    \n",
    "    # Сохранение\n",
    "    final_models[seg_id] = {\n",
    "        'model': model,\n",
    "        'X_train': X_train,  # Для SHAP\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_test_proba': y_test_proba,\n",
    "        'feature_names': X_train.columns.tolist()\n",
    "    }\n",
    "    \n",
    "    final_results[seg_id] = {\n",
    "        'algorithm': model_config['algorithm'],\n",
    "        'balancing_method': model_config['balancing_method'],\n",
    "        'train_time': train_time,\n",
    "        **metrics\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все модели обучены\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. SHAP ANALYSIS - EXPLAINABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SHAP ANALYSIS ДЛЯ ЛУЧШИХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP ANALYSIS - EXPLAINABILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "shap_values_storage = {}\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    model = model_data['model']\n",
    "    X_test = model_data['X_test']\n",
    "    \n",
    "    # Используем sample для SHAP если данных много\n",
    "    if len(X_test) > config.SHAP_SAMPLE_SIZE:\n",
    "        print(f\"  Используем sample {config.SHAP_SAMPLE_SIZE} из {len(X_test)} для SHAP\")\n",
    "        X_shap = X_test.sample(n=config.SHAP_SAMPLE_SIZE, random_state=config.RANDOM_SEED)\n",
    "    else:\n",
    "        X_shap = X_test\n",
    "    \n",
    "    print(f\"  Расчет SHAP values...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Tree SHAP для tree-based моделей\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        \n",
    "        # Для бинарной классификации берем SHAP values для класса 1\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ✓ SHAP values рассчитаны за {elapsed:.2f} сек\")\n",
    "        \n",
    "        shap_values_storage[seg_id] = {\n",
    "            'explainer': explainer,\n",
    "            'shap_values': shap_values,\n",
    "            'X_shap': X_shap\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Ошибка при расчете SHAP: {str(e)}\")\n",
    "        shap_values_storage[seg_id] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SHAP analysis завершен\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SHAP SUMMARY PLOTS\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP SUMMARY PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, shap_data in shap_values_storage.items():\n",
    "    if shap_data is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    shap_values = shap_data['shap_values']\n",
    "    X_shap = shap_data['X_shap']\n",
    "    \n",
    "    # 1. Summary Plot (Bar) - Feature Importance\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values, \n",
    "        X_shap,\n",
    "        plot_type=\"bar\",\n",
    "        max_display=config.SHAP_TOP_FEATURES,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Importance: {seg_id}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    seg_num = seg_id.split()[1]\n",
    "    bar_path = config.FIGURES_DIR / f'shap_importance_seg{seg_num}.png'\n",
    "    plt.savefig(bar_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"  ✓ Сохранено: {bar_path}\")\n",
    "    \n",
    "    # 2. Summary Plot (Beeswarm) - Feature Impact\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_shap,\n",
    "        max_display=config.SHAP_TOP_FEATURES,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Impact: {seg_id}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    beeswarm_path = config.FIGURES_DIR / f'shap_beeswarm_seg{seg_num}.png'\n",
    "    plt.savefig(beeswarm_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"  ✓ Сохранено: {beeswarm_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SHAP FEATURE IMPORTANCE TABLE\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP FEATURE IMPORTANCE (ТОП-20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, shap_data in shap_values_storage.items():\n",
    "    if shap_data is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    shap_values = shap_data['shap_values']\n",
    "    X_shap = shap_data['X_shap']\n",
    "    \n",
    "    # Рассчитываем mean absolute SHAP values\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_shap.columns,\n",
    "        'SHAP_Importance': mean_abs_shap\n",
    "    }).sort_values('SHAP_Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Топ-20\n",
    "    top20 = importance_df.head(20)\n",
    "    print(\"\\nТОП-20 признаков по SHAP importance:\")\n",
    "    print(top20.to_string(index=False))\n",
    "    \n",
    "    # Сохраняем\n",
    "    seg_num = seg_id.split()[1]\n",
    "    importance_file = config.OUTPUT_DIR / f'shap_importance_seg{seg_num}.csv'\n",
    "    importance_df.to_csv(importance_file, index=False)\n",
    "    print(f\"\\n✓ Сохранено: {importance_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ROC CURVES\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "for idx, (seg_id, model_data) in enumerate(final_models.items()):\n",
    "    y_test = model_data['y_test']\n",
    "    y_test_proba = model_data['y_test_proba']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    roc_auc = final_results[seg_id]['roc_auc']\n",
    "    algorithm = final_results[seg_id]['algorithm']\n",
    "    \n",
    "    label = f\"{seg_id} | {algorithm} (AUC = {roc_auc:.4f})\"\n",
    "    ax.plot(fpr, tpr, color=colors[idx], lw=2, label=label)\n",
    "\n",
    "# Diagonal\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC CURVES - ФИНАЛЬНЫЕ МОДЕЛИ', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "roc_path = config.FIGURES_DIR / 'final_roc_curves.png'\n",
    "plt.savefig(roc_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Сохранено: {roc_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CONFUSION MATRICES\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, (seg_id, result) in enumerate(final_results.items()):\n",
    "    cm = np.array([[result['tn'], result['fp']], \n",
    "                   [result['fn'], result['tp']]])\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        ax=axes[idx],\n",
    "        cbar=True,\n",
    "        square=True\n",
    "    )\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(\n",
    "        f'{seg_id}\\n{result[\"algorithm\"]}',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "    axes[idx].set_xticklabels(['No Churn', 'Churn'])\n",
    "    axes[idx].set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "plt.tight_layout()\n",
    "cm_path = config.FIGURES_DIR / 'final_confusion_matrices.png'\n",
    "plt.savefig(cm_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Сохранено: {cm_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    algorithm = final_results[seg_id]['algorithm']\n",
    "    \n",
    "    # Имя файла\n",
    "    algo_name = algorithm.lower().replace(' ', '_')\n",
    "    model_filename = f\"final_model_seg{seg_num}_{algo_name}.pkl\"\n",
    "    model_path = config.MODELS_DIR / model_filename\n",
    "    \n",
    "    # Сохраняем модель\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model_data['model'], f)\n",
    "    \n",
    "    file_size = model_path.stat().st_size / 1024\n",
    "    \n",
    "    print(f\"\\n✓ {model_filename}\")\n",
    "    print(f\"  Сегмент: {seg_id}\")\n",
    "    print(f\"  Алгоритм: {algorithm}\")\n",
    "    print(f\"  ROC-AUC: {final_results[seg_id]['roc_auc']:.4f}\")\n",
    "    print(f\"  Размер: {file_size:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все модели сохранены\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "results_list = []\n",
    "for seg_id, result in final_results.items():\n",
    "    results_list.append({\n",
    "        'segment': seg_id,\n",
    "        'segment_name': config.SEGMENTS[seg_id]['name'],\n",
    "        **result\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Сохраняем\n",
    "results_file = config.OUTPUT_DIR / 'final_production_models_results.csv'\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Результаты сохранены: {results_file}\")\n",
    "print(f\"\\nИтоговая таблица:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. ФИНАЛЬНАЯ СВОДКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФИНАЛЬНАЯ СВОДКА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"✓✓✓ ФИНАЛЬНЫЕ PRODUCTION МОДЕЛИ ГОТОВЫ ✓✓✓\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nДата: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Random seed: {config.RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ФИНАЛЬНЫЕ МОДЕЛИ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for seg_id, result in final_results.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Алгоритм: {result['algorithm']}\")\n",
    "    print(f\"  Балансировка: {result['balancing_method']}\")\n",
    "    print(f\"  ROC-AUC: {result['roc_auc']:.4f}\")\n",
    "    print(f\"  Gini: {result['gini']:.4f}\")\n",
    "    print(f\"  F1-Score: {result['f1']:.4f}\")\n",
    "    print(f\"  Precision: {result['precision']:.4f}\")\n",
    "    print(f\"  Recall: {result['recall']:.4f}\")\n",
    "    print(f\"  Threshold: {result['threshold']:.4f}\")\n",
    "    print(f\"  Время обучения: {result['train_time']:.2f} сек\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СОХРАНЕННЫЕ ФАЙЛЫ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nМОДЕЛИ (models/):\")\n",
    "for seg_id, result in final_results.items():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    algo_name = result['algorithm'].lower().replace(' ', '_')\n",
    "    print(f\"  • final_model_seg{seg_num}_{algo_name}.pkl\")\n",
    "\n",
    "print(f\"\\nРЕЗУЛЬТАТЫ (output/):\")\n",
    "print(f\"  • final_production_models_results.csv\")\n",
    "for seg_id in final_results.keys():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    print(f\"  • shap_importance_seg{seg_num}.csv\")\n",
    "\n",
    "print(f\"\\nВИЗУАЛИЗАЦИИ (figures/):\")\n",
    "print(f\"  • final_roc_curves.png\")\n",
    "print(f\"  • final_confusion_matrices.png\")\n",
    "for seg_id in final_results.keys():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    print(f\"  • shap_importance_seg{seg_num}.png\")\n",
    "    print(f\"  • shap_beeswarm_seg{seg_num}.png\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REPRODUCIBILITY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n✓ Random seed зафиксирован: {config.RANDOM_SEED}\")\n",
    "print(f\"✓ Все параметры моделей зафиксированы\")\n",
    "print(f\"✓ Балансировка использует фиксированный seed\")\n",
    "print(f\"✓ Run All должен давать идентичные результаты\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ ГОТОВО К ВАЛИДАЦИИ\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
