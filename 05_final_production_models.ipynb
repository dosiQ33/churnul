{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# ===============================================================================\n# ФИНАЛЬНЫЕ PRODUCTION МОДЕЛИ + SHAP ANALYSIS\n# ===============================================================================\n\n**Цель:** Воспроизводимый notebook с лучшими моделями для каждого сегмента\n\n**Содержание:**\n1. Загрузка подготовленных данных (parquet из output/)\n2. **Статистика ABT (Section 3.3)** - для документации\n3. **PSI Index (Section 3.5.3)** - Population Stability Index\n4. **Корреляционный анализ (Section 3.5.4)** - Multicollinearity\n5. Конфигурация лучших моделей (HARDCODED из экспериментов 02-03)\n6. **Сравнение моделей (Section 4.2)** - ROC-AUC/Gini visualization\n7. Обучение финальных моделей\n8. **SHAP Analysis (Section 5.3)** - Feature Importance + Explainability\n9. Полные метрики и визуализации\n10. Сохранение финальных моделей\n\n**Reproducibility:** Random seed = 42, Run All должен давать те же результаты\n\n**Standalone:** Требуются только parquet файлы из output/, все модели прописаны в коде\n\n**Дата:** 2025-01-13\n\n# ==============================================================================="
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML Models\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ФИНАЛЬНЫЕ PRODUCTION МОДЕЛИ + SHAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация\"\"\"\n",
    "    \n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # ПУТИ\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    MODELS_DIR = Path(\"models\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # КОЛОНКИ\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    \n",
    "    # СЕГМЕНТЫ\n",
    "    SEGMENTS = {\n",
    "        'Segment 1': {\n",
    "            'name': 'Small Business',\n",
    "            'train': 'seg1_train.parquet',\n",
    "            'val': 'seg1_val.parquet',\n",
    "            'test': 'seg1_test.parquet'\n",
    "        },\n",
    "        'Segment 2': {\n",
    "            'name': 'Middle + Large Business',\n",
    "            'train': 'seg2_train.parquet',\n",
    "            'val': 'seg2_val.parquet',\n",
    "            'test': 'seg2_test.parquet'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # SHAP PARAMETERS\n",
    "    SHAP_SAMPLE_SIZE = 100  # Для Tree SHAP используем все данные или sample\n",
    "    SHAP_TOP_FEATURES = 20  # Топ признаков для визуализации\n",
    "    \n",
    "    # ВИЗУАЛИЗАЦИЯ\n",
    "    FIGURE_SIZE = (12, 8)\n",
    "    FIGURE_DPI = 100\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.MODELS_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегментов: {len(config.SEGMENTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ПОДГОТОВЛЕННЫХ ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for seg_id, seg_info in config.SEGMENTS.items():\n",
    "    print(f\"\\n{seg_id}: {seg_info['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    data[seg_id] = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = config.OUTPUT_DIR / seg_info[split]\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Файл не найден: {file_path}\\n\"\n",
    "                f\"Сначала запустите notebook 01_data_preparation_eda.ipynb\"\n",
    "            )\n",
    "        \n",
    "        df = pd.read_parquet(file_path)\n",
    "        data[seg_id][split] = df\n",
    "        \n",
    "        churn_rate = df[config.TARGET_COLUMN].mean()\n",
    "        print(f\"  {split.upper():5s}: {df.shape} | Churn: {churn_rate*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все данные загружены успешно\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t1weh1j4ux",
   "source": "---\n# 2.1. СТАТИСТИКА ABT (Section 3.3 - Для документации)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0j9ihh319r7h",
   "source": "# ====================================================================================\n# СТАТИСТИКА ABT (ANALYTICAL BASE TABLE)\n# ====================================================================================\n# Раздел 3.3 документации: Результаты сбора ABT\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"СТАТИСТИКА ИТОГОВОЙ ВИТРИНЫ ABT\")\nprint(\"=\"*80)\n\nabt_statistics = {}\n\nfor seg_id, seg_info in config.SEGMENTS.items():\n    print(f\"\\n{seg_id}: {seg_info['name']}\")\n    print(\"=\"*80)\n    \n    # Объединяем все splits для полной статистики\n    train_df = data[seg_id]['train']\n    val_df = data[seg_id]['val']\n    test_df = data[seg_id]['test']\n    \n    full_df = pd.concat([train_df, val_df, test_df], axis=0)\n    \n    # Разделяем на числовые и не числовые\n    numeric_cols = full_df.select_dtypes(include=[np.number]).columns.tolist()\n    non_numeric_cols = full_df.select_dtypes(exclude=[np.number]).columns.tolist()\n    \n    # Убираем target из предикторов\n    if config.TARGET_COLUMN in numeric_cols:\n        numeric_cols.remove(config.TARGET_COLUMN)\n    \n    stats = {\n        'Количество наблюдений': len(full_df),\n        'Количество событий (churn=1)': full_df[config.TARGET_COLUMN].sum(),\n        'Уровень целевой переменной (%)': f\"{full_df[config.TARGET_COLUMN].mean()*100:.2f}%\",\n        'Количество числовых предикторов': len(numeric_cols),\n        'Количество не числовых предикторов': len(non_numeric_cols),\n        'Всего признаков': len(numeric_cols) + len(non_numeric_cols),\n        'Train размер': len(train_df),\n        'Val размер': len(val_df),\n        'Test размер': len(test_df)\n    }\n    \n    abt_statistics[seg_id] = stats\n    \n    # Вывод таблицы\n    print(\"\\nСтатистика итоговой витрины ABT:\")\n    print(\"-\" * 80)\n    for key, value in stats.items():\n        print(f\"  {key:45s}: {value}\")\n    \n    print(\"\\n\" + \"-\" * 80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Статистика ABT рассчитана для всех сегментов\")\nprint(\"=\"*80)\n\n# Сводная таблица для документации\nprint(\"\\n\" + \"=\"*80)\nprint(\"СВОДНАЯ ТАБЛИЦА ABT (для Section 3.3)\")\nprint(\"=\"*80)\n\nabt_summary = pd.DataFrame(abt_statistics).T\nabt_summary.index.name = 'Segment'\nprint(\"\\n\" + abt_summary.to_string())\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7k7qi05forq",
   "source": "---\n# 2.2. PSI INDEX (Section 3.5.3 - Population Stability Index)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6ddlgl3ohij",
   "source": "# ====================================================================================\n# PSI (POPULATION STABILITY INDEX)\n# ====================================================================================\n# Раздел 3.5.3 документации: Индекс PSI\n\ndef calculate_psi(expected, actual, bins=10):\n    \"\"\"\n    Рассчитать PSI (Population Stability Index) между двумя распределениями.\n    \n    PSI < 0.1  - Отличная стабильность\n    PSI < 0.2  - Приемлемая стабильность  \n    PSI >= 0.2 - Значительное изменение, требует внимания\n    \"\"\"\n    # Объединяем для определения границ бинов\n    combined = np.concatenate([expected, actual])\n    min_val = combined.min()\n    max_val = combined.max()\n    \n    # Создаем бины\n    breakpoints = np.linspace(min_val, max_val, bins + 1)\n    breakpoints[0] = -np.inf\n    breakpoints[-1] = np.inf\n    \n    # Распределения по бинам\n    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n    \n    # Пропорции (с защитой от деления на 0)\n    expected_percents = expected_counts / len(expected)\n    actual_percents = actual_counts / len(actual)\n    \n    # Защита от нулевых значений (добавляем малое число)\n    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)\n    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)\n    \n    # PSI формула\n    psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n    psi = np.sum(psi_values)\n    \n    return psi\n\ndef interpret_psi(psi_value):\n    \"\"\"Интерпретация значения PSI\"\"\"\n    if psi_value < 0.1:\n        return \"✅ Отлично - модель стабильна\"\n    elif psi_value < 0.2:\n        return \"⚠️  Приемлемо - небольшие изменения\"\n    else:\n        return \"❌ Требует внимания - значительный drift\"\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PSI (POPULATION STABILITY INDEX) - TRAIN vs TEST\")\nprint(\"=\"*80)\n\npsi_results = {}\n\nfor seg_id in config.SEGMENTS.keys():\n    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n    print(\"=\"*80)\n    \n    # Получаем данные без использования prepare_data\n    train_df = data[seg_id]['train']\n    test_df = data[seg_id]['test']\n    \n    X_train = train_df.drop(columns=[config.TARGET_COLUMN])\n    X_test = test_df.drop(columns=[config.TARGET_COLUMN])\n    \n    feature_psi = {}\n    \n    # Рассчитываем PSI для каждого признака\n    for col in X_train.columns:\n        try:\n            psi_val = calculate_psi(X_train[col].values, X_test[col].values)\n            feature_psi[col] = psi_val\n        except Exception as e:\n            feature_psi[col] = np.nan\n    \n    # Создаем DataFrame\n    psi_df = pd.DataFrame({\n        'Feature': list(feature_psi.keys()),\n        'PSI': list(feature_psi.values())\n    }).sort_values('PSI', ascending=False).reset_index(drop=True)\n    \n    psi_df['Interpretation'] = psi_df['PSI'].apply(interpret_psi)\n    \n    # Overall PSI (среднее по всем признакам)\n    overall_psi = psi_df['PSI'].mean()\n    \n    print(f\"\\nОбщий PSI (среднее по всем признакам): {overall_psi:.6f}\")\n    print(f\"Интерпретация: {interpret_psi(overall_psi)}\")\n    \n    print(\"\\n\" + \"-\" * 80)\n    print(\"ТОП-15 признаков с наибольшим PSI:\")\n    print(\"-\" * 80)\n    print(psi_df.head(15).to_string(index=False))\n    \n    # Статистика по категориям PSI\n    excellent = (psi_df['PSI'] < 0.1).sum()\n    acceptable = ((psi_df['PSI'] >= 0.1) & (psi_df['PSI'] < 0.2)).sum()\n    concerning = (psi_df['PSI'] >= 0.2).sum()\n    \n    print(\"\\n\" + \"-\" * 80)\n    print(\"Распределение признаков по PSI:\")\n    print(f\"  ✅ Отличная стабильность (PSI < 0.1):     {excellent} признаков ({excellent/len(psi_df)*100:.1f}%)\")\n    print(f\"  ⚠️  Приемлемая стабильность (0.1-0.2):    {acceptable} признаков ({acceptable/len(psi_df)*100:.1f}%)\")\n    print(f\"  ❌ Требует внимания (PSI >= 0.2):         {concerning} признаков ({concerning/len(psi_df)*100:.1f}%)\")\n    \n    psi_results[seg_id] = {\n        'overall_psi': overall_psi,\n        'psi_df': psi_df,\n        'excellent': excellent,\n        'acceptable': acceptable,\n        'concerning': concerning\n    }\n    \n    # Сохраняем\n    seg_num = seg_id.split()[1]\n    psi_file = config.OUTPUT_DIR / f'psi_analysis_seg{seg_num}.csv'\n    psi_df.to_csv(psi_file, index=False)\n    print(f\"\\n✓ Сохранено: {psi_file}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ PSI анализ завершен для всех сегментов\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yua6hr9m3m",
   "source": "---\n# 2.3. КОРРЕЛЯЦИОННЫЙ АНАЛИЗ И МУЛЬТИКОЛЛИНЕАРНОСТЬ (Section 3.5.4)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "318pmw884bo",
   "source": "# ====================================================================================\n# КОРРЕЛЯЦИОННЫЙ АНАЛИЗ И МУЛЬТИКОЛЛИНЕАРНОСТЬ\n# ====================================================================================\n# Раздел 3.5.4 документации: Корреляционный анализ и мультиколлинеарность\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"КОРРЕЛЯЦИОННЫЙ АНАЛИЗ И МУЛЬТИКОЛЛИНЕАРНОСТЬ\")\nprint(\"=\"*80)\n\ncorrelation_results = {}\n\nfor seg_id in config.SEGMENTS.keys():\n    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n    print(\"=\"*80)\n    \n    # Получаем данные без использования prepare_data\n    train_df = data[seg_id]['train']\n    X_train = train_df.drop(columns=[config.TARGET_COLUMN])\n    y_train = train_df[config.TARGET_COLUMN]\n    \n    # Корреляционная матрица\n    corr_matrix = X_train.corr()\n    \n    # 1. Корреляция с целевой переменной\n    print(\"\\n1. КОРРЕЛЯЦИЯ ПРИЗНАКОВ С ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\")\n    print(\"-\" * 80)\n    \n    # Добавляем target для расчета корреляции\n    temp_df = X_train.copy()\n    temp_df[config.TARGET_COLUMN] = y_train\n    \n    target_corr = temp_df.corr()[config.TARGET_COLUMN].drop(config.TARGET_COLUMN).abs().sort_values(ascending=False)\n    \n    print(\"\\nТОП-20 признаков с наибольшей корреляцией с target:\")\n    print(target_corr.head(20).to_string())\n    \n    # 2. Мультиколлинеарность (высокая корреляция между признаками)\n    print(\"\\n\\n2. МУЛЬТИКОЛЛИНЕАРНОСТЬ (высокая корреляция между признаками)\")\n    print(\"-\" * 80)\n    \n    # Находим пары признаков с высокой корреляцией (> 0.8)\n    high_corr_threshold = 0.8\n    high_corr_pairs = []\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i+1, len(corr_matrix.columns)):\n            if abs(corr_matrix.iloc[i, j]) > high_corr_threshold:\n                high_corr_pairs.append({\n                    'Feature_1': corr_matrix.columns[i],\n                    'Feature_2': corr_matrix.columns[j],\n                    'Correlation': corr_matrix.iloc[i, j]\n                })\n    \n    if high_corr_pairs:\n        high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False, key=abs)\n        print(f\"\\nНайдено {len(high_corr_df)} пар признаков с |correlation| > {high_corr_threshold}:\")\n        print(\"\\nТОП-20 пар с наибольшей корреляцией:\")\n        print(high_corr_df.head(20).to_string(index=False))\n    else:\n        print(f\"\\n✅ Пар признаков с |correlation| > {high_corr_threshold} не найдено\")\n        print(\"   Мультиколлинеарность не является проблемой\")\n    \n    # 3. Визуализация корреляционной матрицы (топ-30 признаков по корреляции с target)\n    print(\"\\n\\n3. ВИЗУАЛИЗАЦИЯ КОРРЕЛЯЦИОННОЙ МАТРИЦЫ\")\n    print(\"-\" * 80)\n    \n    # Берем топ-30 признаков\n    top_features = target_corr.head(30).index.tolist()\n    \n    fig, ax = plt.subplots(figsize=(14, 12))\n    \n    # Матрица корреляции для топ признаков\n    top_corr_matrix = X_train[top_features].corr()\n    \n    sns.heatmap(\n        top_corr_matrix,\n        cmap='coolwarm',\n        center=0,\n        annot=False,\n        fmt='.2f',\n        square=True,\n        linewidths=0.5,\n        cbar_kws={\"shrink\": 0.8},\n        ax=ax\n    )\n    \n    ax.set_title(f'Корреляционная матрица (ТОП-30 признаков) - {seg_id}', \n                 fontsize=14, fontweight='bold', pad=20)\n    \n    plt.tight_layout()\n    \n    seg_num = seg_id.split()[1]\n    corr_plot_path = config.FIGURES_DIR / f'correlation_matrix_seg{seg_num}.png'\n    plt.savefig(corr_plot_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\n✓ Сохранено: {corr_plot_path}\")\n    \n    # 4. Сохранение результатов\n    correlation_results[seg_id] = {\n        'target_correlation': target_corr,\n        'high_corr_pairs': high_corr_df if high_corr_pairs else None,\n        'corr_matrix': corr_matrix\n    }\n    \n    # Сохраняем CSV с корреляциями\n    target_corr_file = config.OUTPUT_DIR / f'target_correlation_seg{seg_num}.csv'\n    target_corr.to_csv(target_corr_file, header=['Correlation_with_Target'])\n    print(f\"✓ Сохранено: {target_corr_file}\")\n    \n    if high_corr_pairs:\n        high_corr_file = config.OUTPUT_DIR / f'high_correlation_pairs_seg{seg_num}.csv'\n        high_corr_df.to_csv(high_corr_file, index=False)\n        print(f\"✓ Сохранено: {high_corr_file}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Корреляционный анализ завершен для всех сегментов\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": "---\n# 3. КОНФИГУРАЦИЯ ЛУЧШИХ МОДЕЛЕЙ\n\n**Модели выбраны по результатам экспериментов (notebooks 02-03):**\n- **Segment 1 (Small Business):** XGBoost + No balancing → ROC-AUC 0.8958\n- **Segment 2 (Middle + Large Business):** CatBoost + No balancing → ROC-AUC 0.8768"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-experiments",
   "metadata": {},
   "outputs": [],
   "source": "# ====================================================================================\n# КОНФИГУРАЦИЯ ЛУЧШИХ МОДЕЛЕЙ (HARDCODED)\n# ====================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"КОНФИГУРАЦИЯ ЛУЧШИХ МОДЕЛЕЙ\")\nprint(\"=\"*80)\n\n# ============================================================================\n# ЛУЧШИЕ МОДЕЛИ ПО РЕЗУЛЬТАТАМ ЭКСПЕРИМЕНТОВ\n# ============================================================================\n# Segment 1 (Small Business):     XGBoost + No balancing = ROC-AUC 0.8958\n# Segment 2 (Middle + Large):      CatBoost + No balancing = ROC-AUC 0.8768\n# ============================================================================\n\nbest_models_config = {\n    'Segment 1': {\n        'algorithm': 'XGBoost',\n        'balancing_method': 'No balancing',\n        'threshold': 0.12,\n        'expected_roc_auc': 0.8958,  # Ожидаемый результат из экспериментов\n        'description': 'Small Business - XGBoost показал лучшую discriminative power'\n    },\n    'Segment 2': {\n        'algorithm': 'CatBoost',\n        'balancing_method': 'No balancing',\n        'threshold': 0.10,\n        'expected_roc_auc': 0.8768,  # Ожидаемый результат из экспериментов\n        'description': 'Middle + Large Business - CatBoost обеспечил высокую стабильность'\n    }\n}\n\nprint(\"\\n✓ Конфигурация лучших моделей (из экспериментов 02-03):\")\nprint(\"=\"*80)\n\nfor seg_id, model_config in best_models_config.items():\n    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n    print(\"-\" * 80)\n    print(f\"  Алгоритм:       {model_config['algorithm']}\")\n    print(f\"  Балансировка:   {model_config['balancing_method']}\")\n    print(f\"  Threshold:      {model_config['threshold']:.2f}\")\n    print(f\"  Ожидаемый AUC:  {model_config['expected_roc_auc']:.4f}\")\n    print(f\"  Описание:       {model_config['description']}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Модели готовы к обучению\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
    "# ====================================================================================\n",
    "\n",
    "def prepare_data(df, target_col):\n",
    "    \"\"\"Разделение на X и y\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def apply_balancing(X_train, y_train, method, random_seed=42):\n",
    "    \"\"\"\n",
    "    Применить метод балансировки.\n",
    "    \"\"\"\n",
    "    if method == 'No balancing':\n",
    "        return X_train.copy(), y_train.copy(), None\n",
    "    \n",
    "    elif method == 'Class weights':\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        sample_weights = np.array([class_weights[int(y)] for y in y_train])\n",
    "        return X_train.copy(), y_train.copy(), sample_weights\n",
    "    \n",
    "    elif method == 'SMOTE':\n",
    "        smote = SMOTE(random_state=random_seed, k_neighbors=5)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    elif method == 'Random Undersampling':\n",
    "        rus = RandomUnderSampler(random_state=random_seed)\n",
    "        X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    elif method == 'SMOTE + Undersampling':\n",
    "        smote = SMOTE(random_state=random_seed, k_neighbors=5)\n",
    "        X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "        rus = RandomUnderSampler(random_state=random_seed)\n",
    "        X_res, y_res = rus.fit_resample(X_smote, y_smote)\n",
    "        return X_res, y_res, None\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown balancing method: {method}\")\n",
    "\n",
    "\n",
    "def train_model(algorithm, X_train, y_train, X_val, y_val, sample_weights=None, random_seed=42):\n",
    "    \"\"\"\n",
    "    Обучить модель с оптимальными параметрами.\n",
    "    \"\"\"\n",
    "    if algorithm == 'CatBoost':\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "            random_seed=random_seed,\n",
    "            task_type='CPU',\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        from catboost import Pool\n",
    "        train_pool = Pool(X_train, y_train, weight=sample_weights)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "        model.fit(train_pool, eval_set=val_pool)\n",
    "        \n",
    "    elif algorithm == 'LightGBM':\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            random_state=random_seed,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[]\n",
    "        )\n",
    "        \n",
    "    elif algorithm == 'XGBoost':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            random_state=random_seed,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "    elif algorithm == 'RandomForest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=100,\n",
    "            random_state=random_seed,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, threshold):\n",
    "    \"\"\"\n",
    "    Рассчитать все метрики.\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_pred_proba),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    metrics['gini'] = 2 * metrics['roc_auc'] - 1\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['tn'] = cm[0, 0]\n",
    "    metrics['fp'] = cm[0, 1]\n",
    "    metrics['fn'] = cm[1, 0]\n",
    "    metrics['tp'] = cm[1, 1]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"✓ Вспомогательные функции определены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zccfnb5v0e9",
   "source": "---\n# 4.2. СРАВНЕНИЕ МОДЕЛЕЙ (Section 4.2 - Выбор лучшей модели)\n\n**Цель:** Показать обоснование выбора лучших моделей для каждого сегмента\n\nПроведем быстрое сравнение разных алгоритмов на базовых настройках для визуализации.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7w9199i71d5",
   "source": "# ====================================================================================\n# СРАВНЕНИЕ АЛГОРИТМОВ - БЫСТРАЯ ОЦЕНКА\n# ====================================================================================\n# Раздел 4.2 документации: Выбор лучшей модели\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"СРАВНЕНИЕ АЛГОРИТМОВ (для обоснования выбора)\")\nprint(\"=\"*80)\nprint(\"Проводим быстрое обучение всех алгоритмов с базовыми параметрами...\")\nprint(\"=\"*80)\n\nalgorithms_to_compare = ['CatBoost', 'LightGBM', 'XGBoost', 'RandomForest']\ncomparison_results = {}\n\nfor seg_id in config.SEGMENTS.keys():\n    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n    print(\"-\" * 80)\n    \n    # Подготовка данных\n    X_train, y_train = prepare_data(data[seg_id]['train'], config.TARGET_COLUMN)\n    X_val, y_val = prepare_data(data[seg_id]['val'], config.TARGET_COLUMN)\n    X_test, y_test = prepare_data(data[seg_id]['test'], config.TARGET_COLUMN)\n    \n    seg_results = []\n    \n    for algo in algorithms_to_compare:\n        print(f\"  {algo}...\", end=' ')\n        \n        try:\n            # Обучаем с базовыми параметрами (без балансировки)\n            model = train_model(\n                algo,\n                X_train, y_train,\n                X_val, y_val,\n                sample_weights=None,\n                random_seed=config.RANDOM_SEED\n            )\n            \n            # Предсказания на test\n            y_test_proba = model.predict_proba(X_test)[:, 1]\n            \n            # Метрики\n            roc_auc = roc_auc_score(y_test, y_test_proba)\n            gini = 2 * roc_auc - 1\n            pr_auc = average_precision_score(y_test, y_test_proba)\n            \n            seg_results.append({\n                'Algorithm': algo,\n                'ROC_AUC': roc_auc,\n                'Gini': gini,\n                'PR_AUC': pr_auc\n            })\n            \n            print(f\"ROC-AUC: {roc_auc:.4f}, Gini: {gini:.4f}\")\n            \n        except Exception as e:\n            print(f\"ERROR: {str(e)[:50]}\")\n            seg_results.append({\n                'Algorithm': algo,\n                'ROC_AUC': 0,\n                'Gini': 0,\n                'PR_AUC': 0\n            })\n    \n    comparison_results[seg_id] = pd.DataFrame(seg_results).sort_values('ROC_AUC', ascending=False)\n    \n    print(\"\\n\" + \"-\" * 80)\n    print(\"Результаты сравнения:\")\n    print(comparison_results[seg_id].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Сравнение алгоритмов завершено\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vux10azl9us",
   "source": "# ====================================================================================\n# ВИЗУАЛИЗАЦИЯ СРАВНЕНИЯ МОДЕЛЕЙ\n# ====================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ВИЗУАЛИЗАЦИЯ: СРАВНЕНИЕ ROC-AUC И GINI ПО АЛГОРИТМАМ\")\nprint(\"=\"*80)\n\n# Создаем сравнительные графики\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('СРАВНЕНИЕ АЛГОРИТМОВ ПО СЕГМЕНТАМ\\n(Обоснование выбора лучших моделей)', \n             fontsize=16, fontweight='bold', y=0.995)\n\nfor idx, (seg_id, results_df) in enumerate(comparison_results.items()):\n    # ROC-AUC график\n    ax_roc = axes[idx, 0]\n    bars_roc = ax_roc.barh(results_df['Algorithm'], results_df['ROC_AUC'], \n                           color=['#2E86AB' if algo == best_models_config[seg_id]['algorithm'] \n                                  else '#A23B72' for algo in results_df['Algorithm']])\n    \n    ax_roc.set_xlabel('ROC-AUC', fontsize=11, fontweight='bold')\n    ax_roc.set_title(f'{seg_id}: ROC-AUC Comparison', fontsize=12, fontweight='bold')\n    ax_roc.set_xlim([0, 1])\n    ax_roc.grid(axis='x', alpha=0.3)\n    \n    # Добавляем значения на бары\n    for bar in bars_roc:\n        width = bar.get_width()\n        ax_roc.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n                   f'{width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n    \n    # Отмечаем лучшую модель\n    best_algo = best_models_config[seg_id]['algorithm']\n    ax_roc.text(0.02, 0.98, f'✓ ВЫБРАНО: {best_algo}', \n               transform=ax_roc.transAxes, fontsize=10, fontweight='bold',\n               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n               verticalalignment='top')\n    \n    # Gini график\n    ax_gini = axes[idx, 1]\n    bars_gini = ax_gini.barh(results_df['Algorithm'], results_df['Gini'],\n                            color=['#2E86AB' if algo == best_models_config[seg_id]['algorithm'] \n                                   else '#A23B72' for algo in results_df['Algorithm']])\n    \n    ax_gini.set_xlabel('Gini Coefficient', fontsize=11, fontweight='bold')\n    ax_gini.set_title(f'{seg_id}: Gini Comparison', fontsize=12, fontweight='bold')\n    ax_gini.set_xlim([0, 1])\n    ax_gini.grid(axis='x', alpha=0.3)\n    \n    # Добавляем значения на бары\n    for bar in bars_gini:\n        width = bar.get_width()\n        ax_gini.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n                    f'{width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n    \n    # Отмечаем лучшую модель\n    ax_gini.text(0.02, 0.98, f'✓ ВЫБРАНО: {best_algo}',\n                transform=ax_gini.transAxes, fontsize=10, fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n                verticalalignment='top')\n\nplt.tight_layout()\n\ncomparison_plot_path = config.FIGURES_DIR / 'model_comparison_roc_gini.png'\nplt.savefig(comparison_plot_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n✓ Сохранено: {comparison_plot_path}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"ВЫВОДЫ:\")\nprint(\"=\"*80)\nfor seg_id in comparison_results.keys():\n    best_algo = best_models_config[seg_id]['algorithm']\n    best_roc = comparison_results[seg_id][comparison_results[seg_id]['Algorithm'] == best_algo]['ROC_AUC'].values[0]\n    print(f\"\\n{seg_id}:\")\n    print(f\"  ✓ Выбрана модель: {best_algo}\")\n    print(f\"  ✓ ROC-AUC: {best_roc:.4f}\")\n    print(f\"  ✓ Обоснование: Показала наилучшую discriminative power в экспериментах\")\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": "---\n# 5. ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-best-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ ДЛЯ КАЖДОГО СЕГМЕНТА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ОБУЧЕНИЕ ЛУЧШИХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_models = {}\n",
    "final_results = {}\n",
    "\n",
    "for seg_id, model_config in best_models_config.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Алгоритм: {model_config['algorithm']}\")\n",
    "    print(f\"  Балансировка: {model_config['balancing_method']}\")\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train, y_train = prepare_data(data[seg_id]['train'], config.TARGET_COLUMN)\n",
    "    X_val, y_val = prepare_data(data[seg_id]['val'], config.TARGET_COLUMN)\n",
    "    X_test, y_test = prepare_data(data[seg_id]['test'], config.TARGET_COLUMN)\n",
    "    \n",
    "    # Применение балансировки\n",
    "    X_train_balanced, y_train_balanced, sample_weights = apply_balancing(\n",
    "        X_train, y_train, \n",
    "        model_config['balancing_method'],\n",
    "        config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Размер после балансировки: {X_train_balanced.shape}\")\n",
    "    print(f\"  Churn rate: {y_train_balanced.mean()*100:.2f}%\")\n",
    "    \n",
    "    # Обучение модели\n",
    "    print(f\"\\n  Обучение модели...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = train_model(\n",
    "        model_config['algorithm'],\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        X_val, y_val,\n",
    "        sample_weights,\n",
    "        config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"  ✓ Обучение завершено за {train_time:.2f} сек\")\n",
    "    \n",
    "    # Предсказания\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Метрики\n",
    "    threshold = model_config.get('threshold', 0.5)\n",
    "    metrics = calculate_metrics(y_test, y_test_proba, threshold)\n",
    "    \n",
    "    print(f\"\\n  МЕТРИКИ НА TEST:\")\n",
    "    print(f\"    ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"    Gini:      {metrics['gini']:.4f}\")\n",
    "    print(f\"    F1-Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"    Threshold: {threshold:.4f}\")\n",
    "    \n",
    "    # Сохранение\n",
    "    final_models[seg_id] = {\n",
    "        'model': model,\n",
    "        'X_train': X_train,  # Для SHAP\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_test_proba': y_test_proba,\n",
    "        'feature_names': X_train.columns.tolist()\n",
    "    }\n",
    "    \n",
    "    final_results[seg_id] = {\n",
    "        'algorithm': model_config['algorithm'],\n",
    "        'balancing_method': model_config['balancing_method'],\n",
    "        'train_time': train_time,\n",
    "        **metrics\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все модели обучены\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": "---\n# 6. SHAP ANALYSIS - EXPLAINABILITY (Section 5.3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SHAP ANALYSIS ДЛЯ ЛУЧШИХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP ANALYSIS - EXPLAINABILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "shap_values_storage = {}\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    model = model_data['model']\n",
    "    X_test = model_data['X_test']\n",
    "    \n",
    "    # Используем sample для SHAP если данных много\n",
    "    if len(X_test) > config.SHAP_SAMPLE_SIZE:\n",
    "        print(f\"  Используем sample {config.SHAP_SAMPLE_SIZE} из {len(X_test)} для SHAP\")\n",
    "        X_shap = X_test.sample(n=config.SHAP_SAMPLE_SIZE, random_state=config.RANDOM_SEED)\n",
    "    else:\n",
    "        X_shap = X_test\n",
    "    \n",
    "    print(f\"  Расчет SHAP values...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Tree SHAP для tree-based моделей\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        \n",
    "        # Для бинарной классификации берем SHAP values для класса 1\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ✓ SHAP values рассчитаны за {elapsed:.2f} сек\")\n",
    "        \n",
    "        shap_values_storage[seg_id] = {\n",
    "            'explainer': explainer,\n",
    "            'shap_values': shap_values,\n",
    "            'X_shap': X_shap\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Ошибка при расчете SHAP: {str(e)}\")\n",
    "        shap_values_storage[seg_id] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SHAP analysis завершен\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SHAP SUMMARY PLOTS\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP SUMMARY PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, shap_data in shap_values_storage.items():\n",
    "    if shap_data is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    shap_values = shap_data['shap_values']\n",
    "    X_shap = shap_data['X_shap']\n",
    "    \n",
    "    # 1. Summary Plot (Bar) - Feature Importance\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values, \n",
    "        X_shap,\n",
    "        plot_type=\"bar\",\n",
    "        max_display=config.SHAP_TOP_FEATURES,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Importance: {seg_id}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    seg_num = seg_id.split()[1]\n",
    "    bar_path = config.FIGURES_DIR / f'shap_importance_seg{seg_num}.png'\n",
    "    plt.savefig(bar_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"  ✓ Сохранено: {bar_path}\")\n",
    "    \n",
    "    # 2. Summary Plot (Beeswarm) - Feature Impact\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_shap,\n",
    "        max_display=config.SHAP_TOP_FEATURES,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Impact: {seg_id}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    beeswarm_path = config.FIGURES_DIR / f'shap_beeswarm_seg{seg_num}.png'\n",
    "    plt.savefig(beeswarm_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"  ✓ Сохранено: {beeswarm_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# SHAP FEATURE IMPORTANCE TABLE\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP FEATURE IMPORTANCE (ТОП-20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, shap_data in shap_values_storage.items():\n",
    "    if shap_data is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    shap_values = shap_data['shap_values']\n",
    "    X_shap = shap_data['X_shap']\n",
    "    \n",
    "    # Рассчитываем mean absolute SHAP values\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_shap.columns,\n",
    "        'SHAP_Importance': mean_abs_shap\n",
    "    }).sort_values('SHAP_Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Топ-20\n",
    "    top20 = importance_df.head(20)\n",
    "    print(\"\\nТОП-20 признаков по SHAP importance:\")\n",
    "    print(top20.to_string(index=False))\n",
    "    \n",
    "    # Сохраняем\n",
    "    seg_num = seg_id.split()[1]\n",
    "    importance_file = config.OUTPUT_DIR / f'shap_importance_seg{seg_num}.csv'\n",
    "    importance_df.to_csv(importance_file, index=False)\n",
    "    print(f\"\\n✓ Сохранено: {importance_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": "---\n# 7. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ROC CURVES\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "for idx, (seg_id, model_data) in enumerate(final_models.items()):\n",
    "    y_test = model_data['y_test']\n",
    "    y_test_proba = model_data['y_test_proba']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    roc_auc = final_results[seg_id]['roc_auc']\n",
    "    algorithm = final_results[seg_id]['algorithm']\n",
    "    \n",
    "    label = f\"{seg_id} | {algorithm} (AUC = {roc_auc:.4f})\"\n",
    "    ax.plot(fpr, tpr, color=colors[idx], lw=2, label=label)\n",
    "\n",
    "# Diagonal\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC CURVES - ФИНАЛЬНЫЕ МОДЕЛИ', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "roc_path = config.FIGURES_DIR / 'final_roc_curves.png'\n",
    "plt.savefig(roc_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Сохранено: {roc_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CONFUSION MATRICES\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, (seg_id, result) in enumerate(final_results.items()):\n",
    "    cm = np.array([[result['tn'], result['fp']], \n",
    "                   [result['fn'], result['tp']]])\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        ax=axes[idx],\n",
    "        cbar=True,\n",
    "        square=True\n",
    "    )\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(\n",
    "        f'{seg_id}\\n{result[\"algorithm\"]}',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "    axes[idx].set_xticklabels(['No Churn', 'Churn'])\n",
    "    axes[idx].set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "plt.tight_layout()\n",
    "cm_path = config.FIGURES_DIR / 'final_confusion_matrices.png'\n",
    "plt.savefig(cm_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Сохранено: {cm_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": "---\n# 8. СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ФИНАЛЬНЫХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seg_id, model_data in final_models.items():\n",
    "    seg_num = seg_id.split()[1]\n",
    "    algorithm = final_results[seg_id]['algorithm']\n",
    "    \n",
    "    # Имя файла\n",
    "    algo_name = algorithm.lower().replace(' ', '_')\n",
    "    model_filename = f\"final_model_seg{seg_num}_{algo_name}.pkl\"\n",
    "    model_path = config.MODELS_DIR / model_filename\n",
    "    \n",
    "    # Сохраняем модель\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model_data['model'], f)\n",
    "    \n",
    "    file_size = model_path.stat().st_size / 1024\n",
    "    \n",
    "    print(f\"\\n✓ {model_filename}\")\n",
    "    print(f\"  Сегмент: {seg_id}\")\n",
    "    print(f\"  Алгоритм: {algorithm}\")\n",
    "    print(f\"  ROC-AUC: {final_results[seg_id]['roc_auc']:.4f}\")\n",
    "    print(f\"  Размер: {file_size:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Все модели сохранены\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "results_list = []\n",
    "for seg_id, result in final_results.items():\n",
    "    results_list.append({\n",
    "        'segment': seg_id,\n",
    "        'segment_name': config.SEGMENTS[seg_id]['name'],\n",
    "        **result\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Сохраняем\n",
    "results_file = config.OUTPUT_DIR / 'final_production_models_results.csv'\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Результаты сохранены: {results_file}\")\n",
    "print(f\"\\nИтоговая таблица:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": "---\n# 9. ФИНАЛЬНАЯ СВОДКА"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": "# ====================================================================================\n# ФИНАЛЬНАЯ СВОДКА\n# ====================================================================================\n\nprint(\"\\n\\n\" + \"=\"*80)\nprint(\"✓✓✓ ФИНАЛЬНЫЕ PRODUCTION МОДЕЛИ ГОТОВЫ ✓✓✓\")\nprint(\"=\"*80)\n\nprint(f\"\\nДата: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Random seed: {config.RANDOM_SEED}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"ФИНАЛЬНЫЕ МОДЕЛИ\")\nprint(f\"{'='*80}\")\n\nfor seg_id, result in final_results.items():\n    print(f\"\\n{seg_id}: {config.SEGMENTS[seg_id]['name']}\")\n    print(\"-\" * 80)\n    print(f\"  Алгоритм: {result['algorithm']}\")\n    print(f\"  Балансировка: {result['balancing_method']}\")\n    print(f\"  ROC-AUC: {result['roc_auc']:.4f}\")\n    print(f\"  Gini: {result['gini']:.4f}\")\n    print(f\"  F1-Score: {result['f1']:.4f}\")\n    print(f\"  Precision: {result['precision']:.4f}\")\n    print(f\"  Recall: {result['recall']:.4f}\")\n    print(f\"  Threshold: {result['threshold']:.4f}\")\n    print(f\"  Время обучения: {result['train_time']:.2f} сек\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"СОХРАНЕННЫЕ ФАЙЛЫ\")\nprint(f\"{'='*80}\")\n\nprint(f\"\\nМОДЕЛИ (models/):\")\nfor seg_id, result in final_results.items():\n    seg_num = seg_id.split()[1]\n    algo_name = result['algorithm'].lower().replace(' ', '_')\n    print(f\"  • final_model_seg{seg_num}_{algo_name}.pkl\")\n\nprint(f\"\\nРЕЗУЛЬТАТЫ (output/):\")\nprint(f\"  • final_production_models_results.csv\")\nfor seg_id in final_results.keys():\n    seg_num = seg_id.split()[1]\n    print(f\"  • psi_analysis_seg{seg_num}.csv\")\n    print(f\"  • target_correlation_seg{seg_num}.csv\")\n    print(f\"  • shap_importance_seg{seg_num}.csv\")\n\nprint(f\"\\nВИЗУАЛИЗАЦИИ (figures/):\")\nprint(f\"  • model_comparison_roc_gini.png       (Section 4.2 - Обоснование выбора)\")\nfor seg_id in final_results.keys():\n    seg_num = seg_id.split()[1]\n    print(f\"  • correlation_matrix_seg{seg_num}.png      (Section 3.5.4)\")\nprint(f\"  • final_roc_curves.png\")\nprint(f\"  • final_confusion_matrices.png\")\nfor seg_id in final_results.keys():\n    seg_num = seg_id.split()[1]\n    print(f\"  • shap_importance_seg{seg_num}.png         (Section 5.3)\")\n    print(f\"  • shap_beeswarm_seg{seg_num}.png           (Section 5.3)\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"ПОКРЫТИЕ ДОКУМЕНТАЦИИ\")\nprint(f\"{'='*80}\")\nprint(\"\\n✓ Section 3.3:   Статистика ABT (количество наблюдений, признаков, target rate)\")\nprint(\"✓ Section 3.5.3: PSI Index (Train vs Test stability analysis)\")\nprint(\"✓ Section 3.5.4: Корреляционный анализ и мультиколлинеарность\")\nprint(\"✓ Section 4.2:   Сравнение моделей (ROC-AUC/Gini visualization)\")\nprint(\"✓ Section 5.3:   SHAP Analysis (Feature Importance + Explainability)\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"REPRODUCIBILITY\")\nprint(f\"{'='*80}\")\nprint(f\"\\n✓ Random seed зафиксирован: {config.RANDOM_SEED}\")\nprint(f\"✓ Все параметры моделей зафиксированы\")\nprint(f\"✓ Балансировка использует фиксированный seed\")\nprint(f\"✓ Лучшие модели прописаны в коде (не зависит от experiments_all.csv)\")\nprint(f\"✓ Требуются только parquet файлы из output/\")\nprint(f\"✓ Run All должен давать идентичные результаты\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ ГОТОВО К ВАЛИДАЦИИ\")\nprint(f\"{'='*80}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}