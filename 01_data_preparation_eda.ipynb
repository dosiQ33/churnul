{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ===============================================================================\n",
    "# ПОДГОТОВКА ДАННЫХ И ИССЛЕДОВАТЕЛЬСКИЙ АНАЛИЗ (EDA)\n",
    "# ===============================================================================\n",
    "\n",
    "**Цель:** Полная подготовка данных для моделирования оттока клиентов\n",
    "\n",
    "**Содержание:**\n",
    "1. Загрузка и первичный анализ данных\n",
    "2. EDA (Exploratory Data Analysis)\n",
    "3. Временное разбиение (Train/Val/Test-OOT)\n",
    "4. Gap Detection (удаление клиентов с пробелами)\n",
    "5. **ПОЛНЫЙ Preprocessing:**\n",
    "   - Удаление константных колонок\n",
    "   - **Обработка выбросов (IQR clipping)**\n",
    "   - Заполнение пропусков\n",
    "   - **Удаление высоких корреляций (threshold=0.85)**\n",
    "6. Разделение по сегментам (2 группы)\n",
    "7. Correlation Analysis с target\n",
    "8. Сохранение подготовленных данных\n",
    "\n",
    "**Дата:** 2025-01-13  \n",
    "**Random seed:** 42\n",
    "\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML (для импутации)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ПОДГОТОВКА ДАННЫХ И EDA (ПОЛНЫЙ PREPROCESSING)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация\"\"\"\n",
    "    \n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # ПУТИ\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # ФАЙЛЫ\n",
    "    TRAIN_FILE = \"churn_train_ul.parquet\"\n",
    "    \n",
    "    # КОЛОНКИ\n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    SEGMENT_COLUMN = 'segment_group'\n",
    "    DATE_COLUMN = 'observation_point'\n",
    "    CATEGORICAL_FEATURES = ['segment_group', 'obs_month', 'obs_quarter']\n",
    "    \n",
    "    # СЕГМЕНТЫ (ДВЕ МОДЕЛИ)\n",
    "    SEGMENT_1_NAME = \"Small Business\"\n",
    "    SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
    "    \n",
    "    SEGMENT_2_NAME = \"Middle + Large Business\"\n",
    "    SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "    \n",
    "    # ВРЕМЕННОЕ РАЗБИЕНИЕ\n",
    "    TRAIN_SIZE = 0.70\n",
    "    VAL_SIZE = 0.15\n",
    "    TEST_SIZE = 0.15\n",
    "    \n",
    "    # PREPROCESSING (как в исходном файле)\n",
    "    REMOVE_GAPS = True  # Gap detection\n",
    "    HANDLE_OUTLIERS = True  # IQR clipping\n",
    "    REMOVE_HIGH_CORRELATIONS = True  # Удаление коррелирующих признаков\n",
    "    CORRELATION_THRESHOLD = 0.85  # Порог корреляции между признаками\n",
    "    OUTLIER_IQR_MULTIPLIER = 1.5  # Множитель для IQR\n",
    "    \n",
    "    # CORRELATION ANALYSIS С TARGET\n",
    "    CORRELATION_P_VALUE_THRESHOLD = 0.05\n",
    "    DATA_LEAKAGE_THRESHOLD = 0.9\n",
    "    TOP_N_CORRELATIONS = 20\n",
    "    TOP_N_VISUALIZATION = 30\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_train_path(cls):\n",
    "        return cls.DATA_DIR / cls.TRAIN_FILE\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегмент 1: {config.SEGMENT_1_NAME} {config.SEGMENT_1_VALUES}\")\n",
    "print(f\"  Сегмент 2: {config.SEGMENT_2_NAME} {config.SEGMENT_2_VALUES}\")\n",
    "print(f\"  Split: {config.TRAIN_SIZE}/{config.VAL_SIZE}/{config.TEST_SIZE}\")\n",
    "print(f\"\\nPREPROCESSING FLAGS:\")\n",
    "print(f\"  Gap detection: {config.REMOVE_GAPS}\")\n",
    "print(f\"  Outliers handling: {config.HANDLE_OUTLIERS} (IQR × {config.OUTLIER_IQR_MULTIPLIER})\")\n",
    "print(f\"  High correlations removal: {config.REMOVE_HIGH_CORRELATIONS} (threshold={config.CORRELATION_THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "train_path = config.get_train_path()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Файл: {train_path}\")\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Файл не найден: {train_path}\\n\"\n",
    "        f\"Убедитесь, что файл {config.TRAIN_FILE} находится в папке data/\"\n",
    "    )\n",
    "\n",
    "file_size = train_path.stat().st_size / (1024**2)\n",
    "print(f\"Размер файла: {file_size:.2f} MB\")\n",
    "\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(train_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "memory = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "print(f\"\\n✓ Загружено за {load_time:.2f} сек\")\n",
    "print(f\"  Размер: {df_full.shape}\")\n",
    "print(f\"  Память: {memory:.2f} MB\")\n",
    "print(f\"  Строк: {len(df_full):,}\")\n",
    "print(f\"  Колонок: {df_full.shape[1]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. РАЗМЕР ДАННЫХ:\")\n",
    "print(f\"   Строк: {len(df_full):,}\")\n",
    "print(f\"   Колонок: {df_full.shape[1]}\")\n",
    "print(f\"   Память: {df_full.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n2. ТИПЫ ДАННЫХ:\")\n",
    "dtype_counts = df_full.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   {dtype}: {count}\")\n",
    "\n",
    "print(f\"\\n3. ПЕРВЫЕ 5 СТРОК:\")\n",
    "print(df_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ (TARGET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Общий churn rate\n",
    "churn_rate = df_full[config.TARGET_COLUMN].mean()\n",
    "n_churned = df_full[config.TARGET_COLUMN].sum()\n",
    "n_total = len(df_full)\n",
    "ratio = (1-churn_rate)/churn_rate\n",
    "\n",
    "print(f\"\\n1. ОБЩИЙ CHURN RATE:\")\n",
    "print(f\"   Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
    "print(f\"   Churned: {n_churned:,}\")\n",
    "print(f\"   Not churned: {n_total - n_churned:,}\")\n",
    "print(f\"   Class ratio: 1:{ratio:.1f}\")\n",
    "\n",
    "# Churn rate по сегментам\n",
    "print(f\"\\n2. CHURN RATE ПО СЕГМЕНТАМ:\")\n",
    "for segment in df_full[config.SEGMENT_COLUMN].unique():\n",
    "    segment_df = df_full[df_full[config.SEGMENT_COLUMN] == segment]\n",
    "    seg_churn = segment_df[config.TARGET_COLUMN].mean()\n",
    "    seg_count = len(segment_df)\n",
    "    seg_pct = seg_count / len(df_full) * 100\n",
    "    print(f\"   {segment}:\")\n",
    "    print(f\"     Размер: {seg_count:,} ({seg_pct:.1f}%)\")\n",
    "    print(f\"     Churn rate: {seg_churn:.4f} ({seg_churn*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing = df_full.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df_full) * 100).round(2)\n",
    "}).sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"\\nКолонок с пропусками: {len(missing_df)}\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nТоп-20 колонок с наибольшим количеством пропусков:\")\n",
    "    print(missing_df.head(20))\n",
    "else:\n",
    "    print(\"\\n✓ Пропущенных значений не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "constant_cols = []\n",
    "for col in df_full.columns:\n",
    "    if df_full[col].nunique(dropna=False) == 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "print(f\"\\nКонстантных колонок: {len(constant_cols)}\")\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"\\nСписок константных колонок:\")\n",
    "    for col in constant_cols:\n",
    "        print(f\"  - {col} (значение: {df_full[col].iloc[0]})\")\n",
    "else:\n",
    "    print(\"\\n✓ Константных колонок не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-temporal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Конвертация даты\n",
    "df_full[config.DATE_COLUMN] = pd.to_datetime(df_full[config.DATE_COLUMN])\n",
    "\n",
    "print(f\"\\n1. ВРЕМЕННОЙ ПЕРИОД:\")\n",
    "print(f\"   Начало: {df_full[config.DATE_COLUMN].min().date()}\")\n",
    "print(f\"   Конец: {df_full[config.DATE_COLUMN].max().date()}\")\n",
    "print(f\"   Уникальных дат: {df_full[config.DATE_COLUMN].nunique()}\")\n",
    "\n",
    "print(f\"\\n2. КЛИЕНТЫ:\")\n",
    "print(f\"   Уникальных cli_code: {df_full['cli_code'].nunique():,}\")\n",
    "print(f\"   Уникальных client_id: {df_full['client_id'].nunique():,}\")\n",
    "\n",
    "# Распределение по датам\n",
    "print(f\"\\n3. РАСПРЕДЕЛЕНИЕ ЗАПИСЕЙ ПО ДАТАМ:\")\n",
    "date_dist = df_full.groupby(config.DATE_COLUMN).size()\n",
    "print(f\"   Среднее записей на дату: {date_dist.mean():.0f}\")\n",
    "print(f\"   Минимум: {date_dist.min():,}\")\n",
    "print(f\"   Максимум: {date_dist.max():,}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: РАСПРЕДЕЛЕНИЕ TARGET\n",
    "# ====================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Общее распределение\n",
    "target_dist = df_full[config.TARGET_COLUMN].value_counts()\n",
    "axes[0].bar(['No Churn', 'Churn'], [target_dist[0], target_dist[1]],\n",
    "           color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Общее распределение Target', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Количество')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate([target_dist[0], target_dist[1]]):\n",
    "    axes[0].text(i, v, f'{v:,}\\n({v/len(df_full)*100:.2f}%)',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. По сегментам (stacked)\n",
    "segment_churn = df_full.groupby([config.SEGMENT_COLUMN, \n",
    "                                  config.TARGET_COLUMN]).size().unstack(fill_value=0)\n",
    "segment_churn.plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                  color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Распределение по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Сегмент')\n",
    "axes[1].set_ylabel('Количество')\n",
    "axes[1].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Churn rate по сегментам\n",
    "churn_rates = df_full.groupby(config.SEGMENT_COLUMN)[config.TARGET_COLUMN].mean() * 100\n",
    "axes[2].bar(range(len(churn_rates)), churn_rates.values,\n",
    "           color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_xticks(range(len(churn_rates)))\n",
    "axes[2].set_xticklabels(churn_rates.index, rotation=45, ha='right')\n",
    "axes[2].set_title('Churn Rate по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Churn Rate (%)')\n",
    "for i, v in enumerate(churn_rates.values):\n",
    "    axes[2].text(i, v, f'{v:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'eda_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Сохранено: figures/eda_target_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\n",
    "\n",
    "Разбиение данных по времени для предотвращения data leakage:  \n",
    "- **Train:** 70% первых по времени\n",
    "- **Validation:** 15%\n",
    "- **Test (OOT):** 15% последних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# TEMPORAL SPLIT\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сортировка по времени\n",
    "df_sorted = df_full.sort_values(config.DATE_COLUMN).reset_index(drop=True)\n",
    "unique_dates = sorted(df_sorted[config.DATE_COLUMN].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "print(f\"\\nУникальных дат: {n_dates}\")\n",
    "print(f\"Период: {unique_dates[0].date()} - {unique_dates[-1].date()}\")\n",
    "\n",
    "# Cutoff indices\n",
    "train_cutoff = int(n_dates * config.TRAIN_SIZE)\n",
    "val_cutoff = int(n_dates * (config.TRAIN_SIZE + config.VAL_SIZE))\n",
    "\n",
    "train_end = unique_dates[train_cutoff - 1]\n",
    "val_end = unique_dates[val_cutoff - 1]\n",
    "\n",
    "print(f\"\\nCutoff даты:\")\n",
    "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
    "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
    "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
    "\n",
    "# Создание split\n",
    "train_df = df_sorted[df_sorted[config.DATE_COLUMN] <= train_end].copy()\n",
    "val_df = df_sorted[(df_sorted[config.DATE_COLUMN] > train_end) & \n",
    "                   (df_sorted[config.DATE_COLUMN] <= val_end)].copy()\n",
    "test_df = df_sorted[df_sorted[config.DATE_COLUMN] > val_end].copy()\n",
    "\n",
    "# Статистика по split\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СТАТИСТИКА ПО SPLIT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for name, df in [('TRAIN', train_df), ('VALIDATION', val_df), ('TEST (OOT)', test_df)]:\n",
    "    churn_r = df[config.TARGET_COLUMN].mean()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Записей: {len(df):,}\")\n",
    "    print(f\"  Клиентов (cli_code): {df['cli_code'].nunique():,}\")\n",
    "    print(f\"  Период: {df[config.DATE_COLUMN].min().date()} - {df[config.DATE_COLUMN].max().date()}\")\n",
    "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
    "    print(f\"  Процент от общего: {len(df)/len(df_full)*100:.2f}%\")\n",
    "\n",
    "# Проверка data leakage\n",
    "assert train_df[config.DATE_COLUMN].max() < val_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "assert val_df[config.DATE_COLUMN].max() < test_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "print(f\"\\n✓ Temporal ordering verified - NO DATA LEAKAGE\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. GAP DETECTION - УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\n",
    "\n",
    "Удаляем клиентов, у которых есть пропуски в месячных наблюдениях (gaps).  \n",
    "Это важно для обеспечения качества данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gap-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# GAP DETECTION - УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\n",
    "# ====================================================================================\n",
    "\n",
    "if config.REMOVE_GAPS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GAP DETECTION - УДАЛЕНИЕ КЛИЕНТОВ С ПРОБЕЛАМИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\nАнализ пробелов в train данных...\")\n",
    "\n",
    "    # Chunked processing для экономии памяти\n",
    "    unique_clients = train_df['cli_code'].unique()\n",
    "    chunk_size = 10000\n",
    "    clients_with_gaps_list = []\n",
    "\n",
    "    for i in range(0, len(unique_clients), chunk_size):\n",
    "        chunk_clients = unique_clients[i:i+chunk_size]\n",
    "        chunk = train_df[train_df['cli_code'].isin(chunk_clients)].copy()\n",
    "        chunk = chunk.sort_values(['cli_code', config.DATE_COLUMN])\n",
    "\n",
    "        # Конвертируем даты в месячные номера\n",
    "        chunk['month_num'] = chunk[config.DATE_COLUMN].dt.to_period('M').apply(lambda x: x.ordinal)\n",
    "        chunk['month_diff'] = chunk.groupby('cli_code')['month_num'].diff()\n",
    "\n",
    "        # Анализ пробелов\n",
    "        gaps = chunk.groupby('cli_code')['month_diff'].agg([\n",
    "            ('max_gap', 'max'),\n",
    "            ('total_gaps', lambda x: (x > 1).sum())\n",
    "        ]).reset_index()\n",
    "\n",
    "        chunk_gaps = gaps[gaps['max_gap'] > 1]\n",
    "        clients_with_gaps_list.append(chunk_gaps)\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "            print(f\"  Обработано {i+chunk_size:,}/{len(unique_clients):,} клиентов\")\n",
    "\n",
    "    clients_with_gaps = pd.concat(clients_with_gaps_list, ignore_index=True)\n",
    "\n",
    "    gap_pct = len(clients_with_gaps) / len(unique_clients) * 100\n",
    "    print(f\"\\nКлиентов с пробелами: {len(clients_with_gaps):,} ({gap_pct:.2f}%)\")\n",
    "\n",
    "    if len(clients_with_gaps) > 0:\n",
    "        bad_clients = set(clients_with_gaps['cli_code'])\n",
    "\n",
    "        train_before = len(train_df)\n",
    "        val_before = len(val_df)\n",
    "        test_before = len(test_df)\n",
    "        \n",
    "        train_df = train_df[~train_df['cli_code'].isin(bad_clients)].copy()\n",
    "        val_df = val_df[~val_df['cli_code'].isin(bad_clients)].copy()\n",
    "        test_df = test_df[~test_df['cli_code'].isin(bad_clients)].copy()\n",
    "\n",
    "        print(f\"\\nУдалено:\")\n",
    "        print(f\"  Train: {train_before:,} → {len(train_df):,} (-{train_before - len(train_df):,})\")\n",
    "        print(f\"  Val: {val_before:,} → {len(val_df):,} (-{val_before - len(val_df):,})\")\n",
    "        print(f\"  Test: {test_before:,} → {len(test_df):,} (-{test_before - len(test_df):,})\")\n",
    "\n",
    "        del clients_with_gaps, bad_clients\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"\\n✓ Клиентов с пробелами не обнаружено\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n⚠️  Gap detection отключен (config.REMOVE_GAPS=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. ПОЛНЫЙ PREPROCESSING PIPELINE\n",
    "\n",
    "Применяем полный preprocessing из исходного файла:\n",
    "1. **Удаление константных колонок**\n",
    "2. **Обработка выбросов (IQR clipping)** - для финансовых признаков\n",
    "3. **Заполнение пропусков** (медиана/мода)\n",
    "4. **Удаление высоких корреляций** между признаками (threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# PREPROCESSING PIPELINE\n",
    "# ====================================================================================\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"Полный preprocessing pipeline из исходного файла\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.fitted_columns = None\n",
    "        self.final_features = None\n",
    "        self.constant_cols = []\n",
    "        self.outlier_bounds = {}\n",
    "        self.numeric_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numeric_cols_for_imputation = []\n",
    "        self.categorical_cols_for_imputation = []\n",
    "        self.features_to_drop_corr = []\n",
    "\n",
    "    def fit_transform(self, train_df):\n",
    "        \"\"\"Fit and transform training data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING: FIT_TRANSFORM ON TRAIN\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        df = train_df.copy()\n",
    "\n",
    "        # Store columns\n",
    "        self.fitted_columns = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        # 1. Remove constants\n",
    "        df = self._remove_constants(df, fit=True)\n",
    "\n",
    "        # 2. Handle outliers (IQR clipping)\n",
    "        df = self._handle_outliers(df, fit=True)\n",
    "\n",
    "        # 3. Handle missing\n",
    "        df = self._handle_missing(df, fit=True)\n",
    "\n",
    "        # 4. Remove high correlations\n",
    "        df = self._remove_correlations(df, fit=True)\n",
    "\n",
    "        # Final features\n",
    "        self.final_features = [c for c in df.columns\n",
    "                              if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "        print(f\"\\n✓ Preprocessing complete\")\n",
    "        print(f\"  Final features: {len(self.final_features)}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, dataset_name='test'):\n",
    "        \"\"\"Transform new data\"\"\"\n",
    "        print(f\"\\nPreprocessing: {dataset_name}\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        df = self._remove_constants(df, fit=False)\n",
    "        df = self._handle_outliers(df, fit=False)\n",
    "        df = self._handle_missing(df, fit=False)\n",
    "        df = self._remove_correlations(df, fit=False)\n",
    "        df = self._align_columns(df, dataset_name)\n",
    "\n",
    "        print(f\"  ✓ {dataset_name}: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_constants(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n1. Removing constant columns...\")\n",
    "            for col in df.columns:\n",
    "                if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "                    continue\n",
    "                if df[col].nunique(dropna=False) == 1:\n",
    "                    self.constant_cols.append(col)\n",
    "\n",
    "            if self.constant_cols:\n",
    "                df = df.drop(columns=self.constant_cols)\n",
    "                print(f\"   Removed: {len(self.constant_cols)}\")\n",
    "            else:\n",
    "                print(f\"   ✓ No constant columns found\")\n",
    "        return df\n",
    "\n",
    "    def _handle_outliers(self, df, fit):\n",
    "        \"\"\"IQR-based outlier clipping для финансовых признаков\"\"\"\n",
    "        if not config.HANDLE_OUTLIERS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(\"\\n2. Handling outliers (IQR clipping)...\")\n",
    "            # Ключевые слова для финансовых признаков\n",
    "            keywords = ['profit', 'income', 'expense', 'margin', 'provision',\n",
    "                       'balance', 'assets', 'liabilities', 'revenue', 'cost']\n",
    "            \n",
    "            cols = [c for c in df.columns\n",
    "                   if any(kw in c.lower() for kw in keywords)\n",
    "                   and c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            for col in cols:\n",
    "                if df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']:\n",
    "                    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "                    IQR = Q3 - Q1\n",
    "                    self.outlier_bounds[col] = {\n",
    "                        'lower': Q1 - config.OUTLIER_IQR_MULTIPLIER * IQR,\n",
    "                        'upper': Q3 + config.OUTLIER_IQR_MULTIPLIER * IQR\n",
    "                    }\n",
    "\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "            print(f\"   Clipped: {len(self.outlier_bounds)} columns (IQR × {config.OUTLIER_IQR_MULTIPLIER})\")\n",
    "        else:\n",
    "            for col, bounds in self.outlier_bounds.items():\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].clip(lower=bounds['lower'], upper=bounds['upper'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_missing(self, df, fit):\n",
    "        if fit:\n",
    "            print(\"\\n3. Handling missing values...\")\n",
    "            self.numeric_cols_for_imputation = [\n",
    "                c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]\n",
    "            ]\n",
    "            self.categorical_cols_for_imputation = [\n",
    "                c for c in config.CATEGORICAL_FEATURES if c in df.columns\n",
    "            ]\n",
    "\n",
    "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                df[self.numeric_cols_for_imputation] = self.numeric_imputer.fit_transform(\n",
    "                    df[self.numeric_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                df[self.categorical_cols_for_imputation] = self.categorical_imputer.fit_transform(\n",
    "                    df[self.categorical_cols_for_imputation]\n",
    "                )\n",
    "\n",
    "            print(f\"   Imputed: {len(self.numeric_cols_for_imputation)} numeric, \"\n",
    "                  f\"{len(self.categorical_cols_for_imputation)} categorical\")\n",
    "        else:\n",
    "            if len(self.numeric_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.numeric_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.numeric_imputer.transform(df[present])\n",
    "\n",
    "            if len(self.categorical_cols_for_imputation) > 0:\n",
    "                present = [c for c in self.categorical_cols_for_imputation if c in df.columns]\n",
    "                if present:\n",
    "                    df[present] = self.categorical_imputer.transform(df[present])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _remove_correlations(self, df, fit):\n",
    "        \"\"\"Удаление высоко коррелирующих признаков\"\"\"\n",
    "        if not config.REMOVE_HIGH_CORRELATIONS:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            print(f\"\\n4. Removing high correlations (threshold={config.CORRELATION_THRESHOLD})...\")\n",
    "            numeric = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                      if c not in config.ID_COLUMNS + [config.TARGET_COLUMN] + config.CATEGORICAL_FEATURES]\n",
    "\n",
    "            if len(numeric) > 1:\n",
    "                corr = df[numeric].corr().abs()\n",
    "                upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "                self.features_to_drop_corr = [c for c in upper.columns\n",
    "                                             if any(upper[c] > config.CORRELATION_THRESHOLD)]\n",
    "\n",
    "                if self.features_to_drop_corr:\n",
    "                    df = df.drop(columns=self.features_to_drop_corr)\n",
    "                    print(f\"   Removed: {len(self.features_to_drop_corr)} features\")\n",
    "                else:\n",
    "                    print(f\"   ✓ No highly correlated features found\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _align_columns(self, df, name):\n",
    "        \"\"\"Выравнивание колонок с train\"\"\"\n",
    "        preserve = [c for c in config.ID_COLUMNS if c in df.columns]\n",
    "        if config.TARGET_COLUMN in df.columns:\n",
    "            preserve.append(config.TARGET_COLUMN)\n",
    "\n",
    "        current = [c for c in df.columns if c not in preserve]\n",
    "        missing = [c for c in self.final_features if c not in current]\n",
    "        extra = [c for c in current if c not in self.final_features]\n",
    "\n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                df[c] = 0\n",
    "\n",
    "        if extra:\n",
    "            df = df.drop(columns=extra)\n",
    "\n",
    "        order = preserve + self.final_features\n",
    "        df = df[[c for c in order if c in df.columns]]\n",
    "\n",
    "        return df\n",
    "\n",
    "print(\"✓ PreprocessingPipeline класс определен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ПРИМЕНЕНИЕ PREPROCESSING\n",
    "# ====================================================================================\n",
    "\n",
    "pipeline = PreprocessingPipeline(config)\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "val_processed = pipeline.transform(val_df, 'validation')\n",
    "test_processed = pipeline.transform(test_df, 'test (OOT)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nШаги preprocessing:\")\n",
    "print(f\"  1. Константные колонки удалено: {len(pipeline.constant_cols)}\")\n",
    "print(f\"  2. Выбросы обработано (IQR clipping): {len(pipeline.outlier_bounds)} колонок\")\n",
    "print(f\"  3. Пропуски заполнено:\")\n",
    "print(f\"     - Числовых: {len(pipeline.numeric_cols_for_imputation)}\")\n",
    "print(f\"     - Категориальных: {len(pipeline.categorical_cols_for_imputation)}\")\n",
    "print(f\"  4. Коррелирующих признаков удалено: {len(pipeline.features_to_drop_corr)}\")\n",
    "print(f\"\\nИтоговое количество признаков: {len(pipeline.final_features)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ + УДАЛЕНИЕ ID\n",
    "\n",
    "Создание двух групп данных:  \n",
    "- **Segment 1:** Small Business только\n",
    "- **Segment 2:** Middle Business + Large Business\n",
    "\n",
    "**ВАЖНО:**  \n",
    "- Для Segment 1 удаляем `segment_group` (там одно значение - не информативно)\n",
    "- Для Segment 2 оставляем `segment_group` (там два значения - полезная информация)\n",
    "- Удаляем ID колонки перед сохранением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "segment-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SEGMENT 1: Small Business\n",
    "print(f\"\\n1. SEGMENT 1: {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg1_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg1_train.shape}\")\n",
    "print(f\"  Val: {seg1_val.shape}\")\n",
    "print(f\"  Test: {seg1_test.shape}\")\n",
    "\n",
    "# Удаляем segment_group для seg1 (там одно значение) + ID колонки\n",
    "print(f\"\\nУникальных значений segment_group: {seg1_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg1_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "print(f\"\\n→ Удаляем segment_group (не информативна) + ID колонки\")\n",
    "\n",
    "cols_to_drop_seg1 = [config.SEGMENT_COLUMN] + [c for c in config.ID_COLUMNS if c in seg1_train.columns]\n",
    "seg1_train = seg1_train.drop(columns=cols_to_drop_seg1)\n",
    "seg1_val = seg1_val.drop(columns=cols_to_drop_seg1)\n",
    "seg1_test = seg1_test.drop(columns=cols_to_drop_seg1)\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg1:\")\n",
    "print(f\"  Train: {seg1_train.shape} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg1_val.shape} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg1_test.shape} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "# SEGMENT 2: Middle + Large Business\n",
    "print(f\"\\n\\n2. SEGMENT 2: {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg2_train = train_processed[train_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_processed[val_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_processed[test_processed[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg2_train.shape}\")\n",
    "print(f\"  Val: {seg2_val.shape}\")\n",
    "print(f\"  Test: {seg2_test.shape}\")\n",
    "\n",
    "# Оставляем segment_group для seg2 (там два значения) + удаляем только ID\n",
    "print(f\"\\nУникальных значений segment_group: {seg2_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg2_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "print(f\"\\n→ Оставляем segment_group (полезный признак) + удаляем только ID колонки\")\n",
    "\n",
    "cols_to_drop_seg2 = [c for c in config.ID_COLUMNS if c in seg2_train.columns]\n",
    "seg2_train = seg2_train.drop(columns=cols_to_drop_seg2)\n",
    "seg2_val = seg2_val.drop(columns=cols_to_drop_seg2)\n",
    "seg2_test = seg2_test.drop(columns=cols_to_drop_seg2)\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg2:\")\n",
    "print(f\"  Train: {seg2_train.shape} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg2_val.shape} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg2_test.shape} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. CORRELATION ANALYSIS С TARGET\n",
    "\n",
    "Анализ корреляции всех числовых признаков с целевой переменной используя **Point-Biserial Correlation**.  \n",
    "Выполняется для обеих групп отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФУНКЦИИ ДЛЯ CORRELATION ANALYSIS\n",
    "# ====================================================================================\n",
    "\n",
    "def calculate_pointbiserial_correlations(df, target_col, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Рассчитывает Point-Biserial корреляцию для всех числовых признаков с бинарным target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Датафрейм с данными\n",
    "    target_col : str\n",
    "        Название целевой колонки (бинарной)\n",
    "    p_threshold : float\n",
    "        Порог p-value для определения значимости\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Таблица с корреляциями, p-value и флагом значимости\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем числовые колонки (кроме target)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col]\n",
    "    \n",
    "    results = []\n",
    "    target_values = df[target_col].values\n",
    "    \n",
    "    print(f\"Анализируем {len(numeric_cols)} числовых признаков...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        feature_values = df[col].values\n",
    "        \n",
    "        # Пропускаем константные колонки\n",
    "        if len(np.unique(feature_values)) == 1:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Point-Biserial correlation\n",
    "            corr, pval = pointbiserialr(target_values, feature_values)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': col,\n",
    "                'correlation': corr,\n",
    "                'abs_correlation': abs(corr),\n",
    "                'p_value': pval,\n",
    "                'significant': pval < p_threshold\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка для {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    corr_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Сортируем по модулю корреляции\n",
    "    corr_df = corr_df.sort_values('abs_correlation', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def plot_top_correlations(corr_df, segment_name, top_n=30, save_path=None):\n",
    "    \"\"\"\n",
    "    Визуализация топ-N корреляций.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    corr_df : pd.DataFrame\n",
    "        Таблица с корреляциями\n",
    "    segment_name : str\n",
    "        Название сегмента для заголовка\n",
    "    top_n : int\n",
    "        Количество топ корреляций для отображения\n",
    "    save_path : Path or None\n",
    "        Путь для сохранения графика\n",
    "    \"\"\"\n",
    "    \n",
    "    top_corr = corr_df.head(top_n).copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, top_n * 0.3)))\n",
    "    \n",
    "    # Цвета: положительная - зеленый, отрицательная - красный\n",
    "    colors = ['green' if x >= 0 else 'red' for x in top_corr['correlation']]\n",
    "    \n",
    "    # Barplot\n",
    "    bars = ax.barh(range(len(top_corr)), top_corr['correlation'].values,\n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Настройки\n",
    "    ax.set_yticks(range(len(top_corr)))\n",
    "    ax.set_yticklabels(top_corr['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Point-Biserial Correlation', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top-{top_n} Correlations with Target: {segment_name}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Инвертируем ось Y чтобы самая высокая корреляция была сверху\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Сохранено: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Функции для correlation analysis определены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION ANALYSIS: SEGMENT 1\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 1 - {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Расчет корреляций на train\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg1 = calculate_pointbiserial_correlations(\n",
    "    seg1_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Расчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg1)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg1['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg1['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg1['abs_correlation'].max():.4f}\")\n",
    "\n",
    "# Проверка на data leakage\n",
    "leakage_features = corr_seg1[corr_seg1['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n⚠️  ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n→ Это может указывать на data leakage! Проверьте эти признаки.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "# Топ-20 корреляций\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg1.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg1-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: CORRELATION SEGMENT 1\n",
    "# ====================================================================================\n",
    "\n",
    "plot_top_correlations(\n",
    "    corr_seg1,\n",
    "    config.SEGMENT_1_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION ANALYSIS: SEGMENT 2\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 2 - {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Расчет корреляций на train\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg2 = calculate_pointbiserial_correlations(\n",
    "    seg2_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Расчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg2)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg2['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg2['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg2['abs_correlation'].max():.4f}\")\n",
    "\n",
    "# Проверка на data leakage\n",
    "leakage_features = corr_seg2[corr_seg2['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n⚠️  ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n→ Это может указывать на data leakage! Проверьте эти признаки.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "# Топ-20 корреляций\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg2.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg2-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: CORRELATION SEGMENT 2\n",
    "# ====================================================================================\n",
    "\n",
    "plot_top_correlations(\n",
    "    corr_seg2,\n",
    "    config.SEGMENT_2_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment2.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. СОХРАНЕНИЕ ПОДГОТОВЛЕННЫХ ДАННЫХ\n",
    "\n",
    "Сохраняем все подготовленные данные и результаты анализа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ПОДГОТОВЛЕННЫХ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# SEGMENT 1 DATA\n",
    "print(\"\\n1. Сохранение данных Segment 1...\")\n",
    "seg1_train.to_parquet(config.OUTPUT_DIR / 'seg1_train.parquet', index=False)\n",
    "seg1_val.to_parquet(config.OUTPUT_DIR / 'seg1_val.parquet', index=False)\n",
    "seg1_test.to_parquet(config.OUTPUT_DIR / 'seg1_test.parquet', index=False)\n",
    "print(f\"   ✓ seg1_train.parquet ({seg1_train.shape})\")\n",
    "print(f\"   ✓ seg1_val.parquet ({seg1_val.shape})\")\n",
    "print(f\"   ✓ seg1_test.parquet ({seg1_test.shape})\")\n",
    "\n",
    "# SEGMENT 2 DATA\n",
    "print(\"\\n2. Сохранение данных Segment 2...\")\n",
    "seg2_train.to_parquet(config.OUTPUT_DIR / 'seg2_train.parquet', index=False)\n",
    "seg2_val.to_parquet(config.OUTPUT_DIR / 'seg2_val.parquet', index=False)\n",
    "seg2_test.to_parquet(config.OUTPUT_DIR / 'seg2_test.parquet', index=False)\n",
    "print(f\"   ✓ seg2_train.parquet ({seg2_train.shape})\")\n",
    "print(f\"   ✓ seg2_val.parquet ({seg2_val.shape})\")\n",
    "print(f\"   ✓ seg2_test.parquet ({seg2_test.shape})\")\n",
    "\n",
    "# CORRELATION TABLES\n",
    "print(\"\\n3. Сохранение таблиц корреляций...\")\n",
    "corr_seg1.to_csv(config.OUTPUT_DIR / 'correlation_segment1.csv', index=False)\n",
    "corr_seg2.to_csv(config.OUTPUT_DIR / 'correlation_segment2.csv', index=False)\n",
    "print(f\"   ✓ correlation_segment1.csv ({len(corr_seg1)} признаков)\")\n",
    "print(f\"   ✓ correlation_segment2.csv ({len(corr_seg2)} признаков)\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Все данные сохранены за {elapsed:.2f} сек\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИТОГОВАЯ СВОДКА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓✓✓ ПОДГОТОВКА ДАННЫХ И EDA ЗАВЕРШЕНЫ УСПЕШНО ✓✓✓\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nДата: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Random seed: {config.RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n✅ Gap detection: Удалено клиентов с пробелами\")\n",
    "print(f\"✅ Константные колонки: {len(pipeline.constant_cols)} удалено\")\n",
    "print(f\"✅ Выбросы (IQR clipping): {len(pipeline.outlier_bounds)} колонок обработано\")\n",
    "print(f\"✅ Пропуски заполнены: {len(pipeline.numeric_cols_for_imputation)} числовых\")\n",
    "print(f\"✅ Высокие корреляции: {len(pipeline.features_to_drop_corr)} признаков удалено (threshold={config.CORRELATION_THRESHOLD})\")\n",
    "print(f\"\\nИтоговое количество признаков: {len(pipeline.final_features)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СОХРАНЕННЫЕ ФАЙЛЫ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nDATA FILES (output/):\")\n",
    "print(f\"  • seg1_train.parquet - {seg1_train.shape}\")\n",
    "print(f\"  • seg1_val.parquet - {seg1_val.shape}\")\n",
    "print(f\"  • seg1_test.parquet - {seg1_test.shape}\")\n",
    "print(f\"  • seg2_train.parquet - {seg2_train.shape}\")\n",
    "print(f\"  • seg2_val.parquet - {seg2_val.shape}\")\n",
    "print(f\"  • seg2_test.parquet - {seg2_test.shape}\")\n",
    "\n",
    "print(f\"\\nCORRELATION ANALYSIS (output/):\")\n",
    "print(f\"  • correlation_segment1.csv - {len(corr_seg1)} признаков\")\n",
    "print(f\"  • correlation_segment2.csv - {len(corr_seg2)} признаков\")\n",
    "\n",
    "print(f\"\\nVISUALIZATIONS (figures/):\")\n",
    "print(f\"  • eda_target_distribution.png\")\n",
    "print(f\"  • correlation_segment1.png\")\n",
    "print(f\"  • correlation_segment2.png\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"КЛЮЧЕВЫЕ СТАТИСТИКИ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nSEGMENT 1: {config.SEGMENT_1_NAME}\")\n",
    "print(f\"  Train: {len(seg1_train):,} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg1_val):,} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg1_test):,} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Признаков: {seg1_train.shape[1] - 1} (без target)\")\n",
    "print(f\"  Значимых корреляций: {corr_seg1['significant'].sum()}\")\n",
    "\n",
    "print(f\"\\nSEGMENT 2: {config.SEGMENT_2_NAME}\")\n",
    "print(f\"  Train: {len(seg2_train):,} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg2_val):,} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg2_test):,} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Признаков: {seg2_train.shape[1] - 1} (без target)\")\n",
    "print(f\"  Значимых корреляций: {corr_seg2['significant'].sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СЛЕДУЮЩИЙ ШАГ: Обучение моделей (02_experiments_catboost_lightgbm.ipynb)\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
