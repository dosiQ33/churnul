{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ===============================================================================\n",
    "# ПОДГОТОВКА ДАННЫХ И ИССЛЕДОВАТЕЛЬСКИЙ АНАЛИЗ (EDA)\n",
    "# ===============================================================================\n",
    "\n",
    "**Цель:** Базовая подготовка данных для моделирования оттока клиентов\n",
    "\n",
    "**Содержание:**\n",
    "1. Загрузка и первичный анализ данных\n",
    "2. EDA (Exploratory Data Analysis)\n",
    "3. Временное разбиение (Train/Val/Test-OOT)\n",
    "4. Базовый preprocessing\n",
    "5. Разделение по сегментам (2 группы)\n",
    "6. Correlation Analysis с target\n",
    "7. Сохранение подготовленных данных\n",
    "\n",
    "**Дата:** 2025-01-13  \n",
    "**Random seed:** 42\n",
    "\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ИМПОРТ БИБЛИОТЕК И КОНФИГУРАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИМПОРТ БИБЛИОТЕК\n",
    "# ====================================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Данные\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ПОДГОТОВКА ДАННЫХ И EDA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Библиотеки импортированы\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Дата запуска: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Централизованная конфигурация\"\"\"\n",
    "    \n",
    "    # ВОСПРОИЗВОДИМОСТЬ\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # ПУТИ\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    FIGURES_DIR = Path(\"figures\")\n",
    "    \n",
    "    # ФАЙЛЫ\n",
    "    TRAIN_FILE = \"churn_train_ul.parquet\"\n",
    "    \n",
    "    # КОЛОНКИ\n",
    "    ID_COLUMNS = ['cli_code', 'client_id', 'observation_point']\n",
    "    TARGET_COLUMN = 'target_churn_3m'\n",
    "    SEGMENT_COLUMN = 'segment_group'\n",
    "    DATE_COLUMN = 'observation_point'\n",
    "    \n",
    "    # СЕГМЕНТЫ (ДВЕ МОДЕЛИ)\n",
    "    SEGMENT_1_NAME = \"Small Business\"\n",
    "    SEGMENT_1_VALUES = ['SMALL_BUSINESS']\n",
    "    \n",
    "    SEGMENT_2_NAME = \"Middle + Large Business\"\n",
    "    SEGMENT_2_VALUES = ['MIDDLE_BUSINESS', 'LARGE_BUSINESS']\n",
    "    \n",
    "    # ВРЕМЕННОЕ РАЗБИЕНИЕ\n",
    "    TRAIN_SIZE = 0.70\n",
    "    VAL_SIZE = 0.15\n",
    "    TEST_SIZE = 0.15\n",
    "    \n",
    "    # CORRELATION ANALYSIS\n",
    "    CORRELATION_P_VALUE_THRESHOLD = 0.05\n",
    "    DATA_LEAKAGE_THRESHOLD = 0.9\n",
    "    TOP_N_CORRELATIONS = 20\n",
    "    TOP_N_VISUALIZATION = 30\n",
    "    \n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        for dir_path in [cls.OUTPUT_DIR, cls.FIGURES_DIR]:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_train_path(cls):\n",
    "        return cls.DATA_DIR / cls.TRAIN_FILE\n",
    "\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n✓ Конфигурация инициализирована\")\n",
    "print(f\"  Random seed: {config.RANDOM_SEED}\")\n",
    "print(f\"  Сегмент 1: {config.SEGMENT_1_NAME} {config.SEGMENT_1_VALUES}\")\n",
    "print(f\"  Сегмент 2: {config.SEGMENT_2_NAME} {config.SEGMENT_2_VALUES}\")\n",
    "print(f\"  Split: {config.TRAIN_SIZE}/{config.VAL_SIZE}/{config.TEST_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ЗАГРУЗКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАГРУЗКА ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "train_path = config.get_train_path()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Файл: {train_path}\")\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Файл не найден: {train_path}\\n\"\n",
    "        f\"Убедитесь, что файл {config.TRAIN_FILE} находится в папке data/\"\n",
    "    )\n",
    "\n",
    "file_size = train_path.stat().st_size / (1024**2)\n",
    "print(f\"Размер файла: {file_size:.2f} MB\")\n",
    "\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(train_path)\n",
    "load_time = time.time() - start\n",
    "\n",
    "memory = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "print(f\"\\n✓ Загружено за {load_time:.2f} сек\")\n",
    "print(f\"  Размер: {df_full.shape}\")\n",
    "print(f\"  Память: {memory:.2f} MB\")\n",
    "print(f\"  Строк: {len(df_full):,}\")\n",
    "print(f\"  Колонок: {df_full.shape[1]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"БАЗОВАЯ ИНФОРМАЦИЯ О ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. РАЗМЕР ДАННЫХ:\")\n",
    "print(f\"   Строк: {len(df_full):,}\")\n",
    "print(f\"   Колонок: {df_full.shape[1]}\")\n",
    "print(f\"   Память: {df_full.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n2. ТИПЫ ДАННЫХ:\")\n",
    "dtype_counts = df_full.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   {dtype}: {count}\")\n",
    "\n",
    "print(f\"\\n3. ПЕРВЫЕ 5 СТРОК:\")\n",
    "print(df_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ЦЕЛЕВОЙ ПЕРЕМЕННОЙ (TARGET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Общий churn rate\n",
    "churn_rate = df_full[config.TARGET_COLUMN].mean()\n",
    "n_churned = df_full[config.TARGET_COLUMN].sum()\n",
    "n_total = len(df_full)\n",
    "ratio = (1-churn_rate)/churn_rate\n",
    "\n",
    "print(f\"\\n1. ОБЩИЙ CHURN RATE:\")\n",
    "print(f\"   Churn rate: {churn_rate:.4f} ({churn_rate*100:.2f}%)\")\n",
    "print(f\"   Churned: {n_churned:,}\")\n",
    "print(f\"   Not churned: {n_total - n_churned:,}\")\n",
    "print(f\"   Class ratio: 1:{ratio:.1f}\")\n",
    "\n",
    "# Churn rate по сегментам\n",
    "print(f\"\\n2. CHURN RATE ПО СЕГМЕНТАМ:\")\n",
    "for segment in df_full[config.SEGMENT_COLUMN].unique():\n",
    "    segment_df = df_full[df_full[config.SEGMENT_COLUMN] == segment]\n",
    "    seg_churn = segment_df[config.TARGET_COLUMN].mean()\n",
    "    seg_count = len(segment_df)\n",
    "    seg_pct = seg_count / len(df_full) * 100\n",
    "    print(f\"   {segment}:\")\n",
    "    print(f\"     Размер: {seg_count:,} ({seg_pct:.1f}%)\")\n",
    "    print(f\"     Churn rate: {seg_churn:.4f} ({seg_churn*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing = df_full.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df_full) * 100).round(2)\n",
    "}).sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"\\nКолонок с пропусками: {len(missing_df)}\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nТоп-20 колонок с наибольшим количеством пропусков:\")\n",
    "    print(missing_df.head(20))\n",
    "else:\n",
    "    print(\"\\n✓ Пропущенных значений не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ КОНСТАНТНЫХ КОЛОНОК\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "constant_cols = []\n",
    "for col in df_full.columns:\n",
    "    if df_full[col].nunique(dropna=False) == 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "print(f\"\\nКонстантных колонок: {len(constant_cols)}\")\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"\\nСписок константных колонок:\")\n",
    "    for col in constant_cols:\n",
    "        print(f\"  - {col} (значение: {df_full[col].iloc[0]})\")\n",
    "else:\n",
    "    print(\"\\n✓ Константных колонок не обнаружено\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-temporal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛИЗ ВРЕМЕННОГО РАСПРЕДЕЛЕНИЯ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Конвертация даты\n",
    "df_full[config.DATE_COLUMN] = pd.to_datetime(df_full[config.DATE_COLUMN])\n",
    "\n",
    "print(f\"\\n1. ВРЕМЕННОЙ ПЕРИОД:\")\n",
    "print(f\"   Начало: {df_full[config.DATE_COLUMN].min().date()}\")\n",
    "print(f\"   Конец: {df_full[config.DATE_COLUMN].max().date()}\")\n",
    "print(f\"   Уникальных дат: {df_full[config.DATE_COLUMN].nunique()}\")\n",
    "\n",
    "print(f\"\\n2. КЛИЕНТЫ:\")\n",
    "print(f\"   Уникальных cli_code: {df_full['cli_code'].nunique():,}\")\n",
    "print(f\"   Уникальных client_id: {df_full['client_id'].nunique():,}\")\n",
    "\n",
    "# Распределение по датам\n",
    "print(f\"\\n3. РАСПРЕДЕЛЕНИЕ ЗАПИСЕЙ ПО ДАТАМ:\")\n",
    "date_dist = df_full.groupby(config.DATE_COLUMN).size()\n",
    "print(f\"   Среднее записей на дату: {date_dist.mean():.0f}\")\n",
    "print(f\"   Минимум: {date_dist.min():,}\")\n",
    "print(f\"   Максимум: {date_dist.max():,}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: РАСПРЕДЕЛЕНИЕ TARGET\n",
    "# ====================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Общее распределение\n",
    "target_dist = df_full[config.TARGET_COLUMN].value_counts()\n",
    "axes[0].bar(['No Churn', 'Churn'], [target_dist[0], target_dist[1]],\n",
    "           color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Общее распределение Target', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Количество')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate([target_dist[0], target_dist[1]]):\n",
    "    axes[0].text(i, v, f'{v:,}\\n({v/len(df_full)*100:.2f}%)',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. По сегментам (stacked)\n",
    "segment_churn = df_full.groupby([config.SEGMENT_COLUMN, \n",
    "                                  config.TARGET_COLUMN]).size().unstack(fill_value=0)\n",
    "segment_churn.plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                  color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Распределение по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Сегмент')\n",
    "axes[1].set_ylabel('Количество')\n",
    "axes[1].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Churn rate по сегментам\n",
    "churn_rates = df_full.groupby(config.SEGMENT_COLUMN)[config.TARGET_COLUMN].mean() * 100\n",
    "axes[2].bar(range(len(churn_rates)), churn_rates.values,\n",
    "           color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_xticks(range(len(churn_rates)))\n",
    "axes[2].set_xticklabels(churn_rates.index, rotation=45, ha='right')\n",
    "axes[2].set_title('Churn Rate по сегментам', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Churn Rate (%)')\n",
    "for i, v in enumerate(churn_rates.values):\n",
    "    axes[2].text(i, v, f'{v:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.FIGURES_DIR / 'eda_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Сохранено: figures/eda_target_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\n",
    "\n",
    "Разбиение данных по времени для предотвращения data leakage:  \n",
    "- **Train:** 70% первых по времени\n",
    "- **Validation:** 15%\n",
    "- **Test (OOT):** 15% последних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# TEMPORAL SPLIT\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВРЕМЕННОЕ РАЗБИЕНИЕ (TEMPORAL SPLIT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сортировка по времени\n",
    "df_sorted = df_full.sort_values(config.DATE_COLUMN).reset_index(drop=True)\n",
    "unique_dates = sorted(df_sorted[config.DATE_COLUMN].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "print(f\"\\nУникальных дат: {n_dates}\")\n",
    "print(f\"Период: {unique_dates[0].date()} - {unique_dates[-1].date()}\")\n",
    "\n",
    "# Cutoff indices\n",
    "train_cutoff = int(n_dates * config.TRAIN_SIZE)\n",
    "val_cutoff = int(n_dates * (config.TRAIN_SIZE + config.VAL_SIZE))\n",
    "\n",
    "train_end = unique_dates[train_cutoff - 1]\n",
    "val_end = unique_dates[val_cutoff - 1]\n",
    "\n",
    "print(f\"\\nCutoff даты:\")\n",
    "print(f\"  Train: до {train_end.date()} ({train_cutoff} дат)\")\n",
    "print(f\"  Val: {unique_dates[train_cutoff].date()} - {val_end.date()} ({val_cutoff - train_cutoff} дат)\")\n",
    "print(f\"  Test (OOT): {unique_dates[val_cutoff].date()}+ ({n_dates - val_cutoff} дат)\")\n",
    "\n",
    "# Создание split\n",
    "train_df = df_sorted[df_sorted[config.DATE_COLUMN] <= train_end].copy()\n",
    "val_df = df_sorted[(df_sorted[config.DATE_COLUMN] > train_end) & \n",
    "                   (df_sorted[config.DATE_COLUMN] <= val_end)].copy()\n",
    "test_df = df_sorted[df_sorted[config.DATE_COLUMN] > val_end].copy()\n",
    "\n",
    "# Статистика по split\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СТАТИСТИКА ПО SPLIT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for name, df in [('TRAIN', train_df), ('VALIDATION', val_df), ('TEST (OOT)', test_df)]:\n",
    "    churn_r = df[config.TARGET_COLUMN].mean()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Записей: {len(df):,}\")\n",
    "    print(f\"  Клиентов (cli_code): {df['cli_code'].nunique():,}\")\n",
    "    print(f\"  Период: {df[config.DATE_COLUMN].min().date()} - {df[config.DATE_COLUMN].max().date()}\")\n",
    "    print(f\"  Churn rate: {churn_r:.4f} ({churn_r*100:.2f}%)\")\n",
    "    print(f\"  Процент от общего: {len(df)/len(df_full)*100:.2f}%\")\n",
    "\n",
    "# Проверка data leakage\n",
    "assert train_df[config.DATE_COLUMN].max() < val_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "assert val_df[config.DATE_COLUMN].max() < test_df[config.DATE_COLUMN].min(), \"Data leakage detected!\"\n",
    "print(f\"\\n✓ Temporal ordering verified - NO DATA LEAKAGE\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. БАЗОВЫЙ PREPROCESSING\n",
    "\n",
    "- Удаление константных колонок\n",
    "- Заполнение пропусков (медиана для числовых, мода для категориальных)\n",
    "- Удаление ID колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing-constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# УДАЛЕНИЕ КОНСТАНТНЫХ КОЛОНОК\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"БАЗОВЫЙ PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. УДАЛЕНИЕ КОНСТАНТНЫХ КОЛОНОК\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Определяем константные колонки на train\n",
    "constant_cols_train = []\n",
    "for col in train_df.columns:\n",
    "    if col in config.ID_COLUMNS + [config.TARGET_COLUMN]:\n",
    "        continue\n",
    "    if train_df[col].nunique(dropna=False) == 1:\n",
    "        constant_cols_train.append(col)\n",
    "\n",
    "print(f\"Константных колонок на train: {len(constant_cols_train)}\")\n",
    "\n",
    "if constant_cols_train:\n",
    "    print(f\"\\nУдаляемые колонки:\")\n",
    "    for col in constant_cols_train:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Удаляем из всех датасетов\n",
    "    train_df = train_df.drop(columns=constant_cols_train)\n",
    "    val_df = val_df.drop(columns=constant_cols_train)\n",
    "    test_df = test_df.drop(columns=constant_cols_train)\n",
    "    \n",
    "    print(f\"\\n✓ Удалено {len(constant_cols_train)} константных колонок\")\n",
    "else:\n",
    "    print(\"\\n✓ Константных колонок не обнаружено\")\n",
    "\n",
    "print(f\"\\nРазмеры после удаления константных колонок:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val: {val_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ЗАПОЛНЕНИЕ ПРОПУСКОВ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n2. ЗАПОЛНЕНИЕ ПРОПУСКОВ\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Определяем колонки с пропусками на train\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_cols = [c for c in categorical_cols if c not in config.ID_COLUMNS + [config.TARGET_COLUMN]]\n",
    "\n",
    "# Считаем медианы и моды на train\n",
    "numeric_fillvalues = {}\n",
    "for col in numeric_cols:\n",
    "    if train_df[col].isnull().any():\n",
    "        numeric_fillvalues[col] = train_df[col].median()\n",
    "\n",
    "categorical_fillvalues = {}\n",
    "for col in categorical_cols:\n",
    "    if train_df[col].isnull().any():\n",
    "        mode_val = train_df[col].mode()\n",
    "        categorical_fillvalues[col] = mode_val[0] if len(mode_val) > 0 else 'MISSING'\n",
    "\n",
    "print(f\"\\nЧисловых колонок с пропусками: {len(numeric_fillvalues)}\")\n",
    "print(f\"Категориальных колонок с пропусками: {len(categorical_fillvalues)}\")\n",
    "\n",
    "# Применяем заполнение\n",
    "if numeric_fillvalues:\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        for col, fill_val in numeric_fillvalues.items():\n",
    "            df[col] = df[col].fillna(fill_val)\n",
    "    print(f\"\\n✓ Заполнено {len(numeric_fillvalues)} числовых колонок медианой\")\n",
    "\n",
    "if categorical_fillvalues:\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        for col, fill_val in categorical_fillvalues.items():\n",
    "            df[col] = df[col].fillna(fill_val)\n",
    "    print(f\"✓ Заполнено {len(categorical_fillvalues)} категориальных колонок модой\")\n",
    "\n",
    "if not numeric_fillvalues and not categorical_fillvalues:\n",
    "    print(\"\\n✓ Пропусков не обнаружено\")\n",
    "\n",
    "# Проверка\n",
    "train_missing = train_df.isnull().sum().sum()\n",
    "val_missing = val_df.isnull().sum().sum()\n",
    "test_missing = test_df.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nПроверка пропусков после заполнения:\")\n",
    "print(f\"  Train: {train_missing}\")\n",
    "print(f\"  Val: {val_missing}\")\n",
    "print(f\"  Test: {test_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing-ids",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# УДАЛЕНИЕ ID КОЛОНОК\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n3. УДАЛЕНИЕ ID КОЛОНОК\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nУдаляемые колонки: {config.ID_COLUMNS}\")\n",
    "\n",
    "train_df = train_df.drop(columns=config.ID_COLUMNS)\n",
    "val_df = val_df.drop(columns=config.ID_COLUMNS)\n",
    "test_df = test_df.drop(columns=config.ID_COLUMNS)\n",
    "\n",
    "print(f\"\\n✓ Удалено {len(config.ID_COLUMNS)} ID колонок\")\n",
    "\n",
    "print(f\"\\nИтоговые размеры после preprocessing:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val: {val_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\n",
    "\n",
    "Создание двух групп данных:  \n",
    "- **Segment 1:** Small Business только\n",
    "- **Segment 2:** Middle Business + Large Business\n",
    "\n",
    "**ВАЖНО:**  \n",
    "- Для Segment 1 удаляем `segment_group` (там одно значение - не информативно)\n",
    "- Для Segment 2 оставляем `segment_group` (там два значения - полезная информация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "segment-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РАЗДЕЛЕНИЕ ПО СЕГМЕНТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SEGMENT 1: Small Business\n",
    "print(f\"\\n1. SEGMENT 1: {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg1_train = train_df[train_df[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_val = val_df[val_df[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "seg1_test = test_df[test_df[config.SEGMENT_COLUMN].isin(config.SEGMENT_1_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg1_train.shape}\")\n",
    "print(f\"  Val: {seg1_val.shape}\")\n",
    "print(f\"  Test: {seg1_test.shape}\")\n",
    "\n",
    "# Удаляем segment_group для seg1 (там одно значение)\n",
    "print(f\"\\nУникальных значений segment_group: {seg1_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg1_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "print(f\"\\n→ Удаляем segment_group (не информативна для seg1)\")\n",
    "\n",
    "seg1_train = seg1_train.drop(columns=[config.SEGMENT_COLUMN])\n",
    "seg1_val = seg1_val.drop(columns=[config.SEGMENT_COLUMN])\n",
    "seg1_test = seg1_test.drop(columns=[config.SEGMENT_COLUMN])\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg1:\")\n",
    "print(f\"  Train: {seg1_train.shape} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg1_val.shape} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg1_test.shape} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "# SEGMENT 2: Middle + Large Business\n",
    "print(f\"\\n\\n2. SEGMENT 2: {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seg2_train = train_df[train_df[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_val = val_df[val_df[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "seg2_test = test_df[test_df[config.SEGMENT_COLUMN].isin(config.SEGMENT_2_VALUES)].copy()\n",
    "\n",
    "print(f\"Исходные размеры:\")\n",
    "print(f\"  Train: {seg2_train.shape}\")\n",
    "print(f\"  Val: {seg2_val.shape}\")\n",
    "print(f\"  Test: {seg2_test.shape}\")\n",
    "\n",
    "# Оставляем segment_group для seg2 (там два значения)\n",
    "print(f\"\\nУникальных значений segment_group: {seg2_train[config.SEGMENT_COLUMN].nunique()}\")\n",
    "print(f\"Значения: {seg2_train[config.SEGMENT_COLUMN].unique()}\")\n",
    "print(f\"\\n→ Оставляем segment_group (полезный признак для seg2)\")\n",
    "\n",
    "print(f\"\\nИтоговые размеры seg2:\")\n",
    "print(f\"  Train: {seg2_train.shape} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {seg2_val.shape} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {seg2_test.shape} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. CORRELATION ANALYSIS С TARGET\n",
    "\n",
    "Анализ корреляции всех числовых признаков с целевой переменной используя **Point-Biserial Correlation**.  \n",
    "Выполняется для обеих групп отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ФУНКЦИИ ДЛЯ CORRELATION ANALYSIS\n",
    "# ====================================================================================\n",
    "\n",
    "def calculate_pointbiserial_correlations(df, target_col, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Рассчитывает Point-Biserial корреляцию для всех числовых признаков с бинарным target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Датафрейм с данными\n",
    "    target_col : str\n",
    "        Название целевой колонки (бинарной)\n",
    "    p_threshold : float\n",
    "        Порог p-value для определения значимости\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Таблица с корреляциями, p-value и флагом значимости\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем числовые колонки (кроме target)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col]\n",
    "    \n",
    "    results = []\n",
    "    target_values = df[target_col].values\n",
    "    \n",
    "    print(f\"Анализируем {len(numeric_cols)} числовых признаков...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        feature_values = df[col].values\n",
    "        \n",
    "        # Пропускаем константные колонки\n",
    "        if len(np.unique(feature_values)) == 1:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Point-Biserial correlation\n",
    "            corr, pval = pointbiserialr(target_values, feature_values)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': col,\n",
    "                'correlation': corr,\n",
    "                'abs_correlation': abs(corr),\n",
    "                'p_value': pval,\n",
    "                'significant': pval < p_threshold\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка для {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    corr_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Сортируем по модулю корреляции\n",
    "    corr_df = corr_df.sort_values('abs_correlation', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def plot_top_correlations(corr_df, segment_name, top_n=30, save_path=None):\n",
    "    \"\"\"\n",
    "    Визуализация топ-N корреляций.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    corr_df : pd.DataFrame\n",
    "        Таблица с корреляциями\n",
    "    segment_name : str\n",
    "        Название сегмента для заголовка\n",
    "    top_n : int\n",
    "        Количество топ корреляций для отображения\n",
    "    save_path : Path or None\n",
    "        Путь для сохранения графика\n",
    "    \"\"\"\n",
    "    \n",
    "    top_corr = corr_df.head(top_n).copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, top_n * 0.3)))\n",
    "    \n",
    "    # Цвета: положительная - зеленый, отрицательная - красный\n",
    "    colors = ['green' if x >= 0 else 'red' for x in top_corr['correlation']]\n",
    "    \n",
    "    # Barplot\n",
    "    bars = ax.barh(range(len(top_corr)), top_corr['correlation'].values,\n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Настройки\n",
    "    ax.set_yticks(range(len(top_corr)))\n",
    "    ax.set_yticklabels(top_corr['feature'].values, fontsize=9)\n",
    "    ax.set_xlabel('Point-Biserial Correlation', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top-{top_n} Correlations with Target: {segment_name}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Инвертируем ось Y чтобы самая высокая корреляция была сверху\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Сохранено: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Функции для correlation analysis определены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION ANALYSIS: SEGMENT 1\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 1 - {config.SEGMENT_1_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Расчет корреляций на train\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg1 = calculate_pointbiserial_correlations(\n",
    "    seg1_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Расчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg1)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg1['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg1['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg1['abs_correlation'].max():.4f}\")\n",
    "\n",
    "# Проверка на data leakage\n",
    "leakage_features = corr_seg1[corr_seg1['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n⚠️  ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n→ Это может указывать на data leakage! Проверьте эти признаки.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "# Топ-20 корреляций\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg1.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg1-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: CORRELATION SEGMENT 1\n",
    "# ====================================================================================\n",
    "\n",
    "plot_top_correlations(\n",
    "    corr_seg1,\n",
    "    config.SEGMENT_1_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# CORRELATION ANALYSIS: SEGMENT 2\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CORRELATION ANALYSIS: SEGMENT 2 - {config.SEGMENT_2_NAME.upper()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Расчет корреляций на train\n",
    "print(f\"\\nРасчет Point-Biserial корреляций на train данных...\")\n",
    "start_time = time.time()\n",
    "\n",
    "corr_seg2 = calculate_pointbiserial_correlations(\n",
    "    seg2_train, \n",
    "    config.TARGET_COLUMN,\n",
    "    config.CORRELATION_P_VALUE_THRESHOLD\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Расчет завершен за {elapsed:.2f} сек\")\n",
    "\n",
    "# Статистика\n",
    "print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Всего признаков: {len(corr_seg2)}\")\n",
    "print(f\"  Значимых (p<0.05): {corr_seg2['significant'].sum()}\")\n",
    "print(f\"  Средняя |корреляция|: {corr_seg2['abs_correlation'].mean():.4f}\")\n",
    "print(f\"  Максимальная |корреляция|: {corr_seg2['abs_correlation'].max():.4f}\")\n",
    "\n",
    "# Проверка на data leakage\n",
    "leakage_features = corr_seg2[corr_seg2['abs_correlation'] > config.DATA_LEAKAGE_THRESHOLD]\n",
    "if len(leakage_features) > 0:\n",
    "    print(f\"\\n⚠️  ВНИМАНИЕ: Обнаружены признаки с очень высокой корреляцией (>0.9):\")\n",
    "    print(leakage_features[['feature', 'correlation', 'p_value']].head(10))\n",
    "    print(f\"\\n→ Это может указывать на data leakage! Проверьте эти признаки.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Признаков с подозрением на data leakage не обнаружено\")\n",
    "\n",
    "# Топ-20 корреляций\n",
    "print(f\"\\nТОП-{config.TOP_N_CORRELATIONS} КОРРЕЛЯЦИЙ (по модулю):\")\n",
    "print(corr_seg2.head(config.TOP_N_CORRELATIONS)[['feature', 'correlation', 'p_value', 'significant']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-seg2-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ: CORRELATION SEGMENT 2\n",
    "# ====================================================================================\n",
    "\n",
    "plot_top_correlations(\n",
    "    corr_seg2,\n",
    "    config.SEGMENT_2_NAME,\n",
    "    top_n=config.TOP_N_VISUALIZATION,\n",
    "    save_path=config.FIGURES_DIR / 'correlation_segment2.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. СОХРАНЕНИЕ ПОДГОТОВЛЕННЫХ ДАННЫХ\n",
    "\n",
    "Сохраняем все подготовленные данные и результаты анализа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# СОХРАНЕНИЕ ДАННЫХ\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ПОДГОТОВЛЕННЫХ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# SEGMENT 1 DATA\n",
    "print(\"\\n1. Сохранение данных Segment 1...\")\n",
    "seg1_train.to_parquet(config.OUTPUT_DIR / 'seg1_train.parquet', index=False)\n",
    "seg1_val.to_parquet(config.OUTPUT_DIR / 'seg1_val.parquet', index=False)\n",
    "seg1_test.to_parquet(config.OUTPUT_DIR / 'seg1_test.parquet', index=False)\n",
    "print(f\"   ✓ seg1_train.parquet ({seg1_train.shape})\")\n",
    "print(f\"   ✓ seg1_val.parquet ({seg1_val.shape})\")\n",
    "print(f\"   ✓ seg1_test.parquet ({seg1_test.shape})\")\n",
    "\n",
    "# SEGMENT 2 DATA\n",
    "print(\"\\n2. Сохранение данных Segment 2...\")\n",
    "seg2_train.to_parquet(config.OUTPUT_DIR / 'seg2_train.parquet', index=False)\n",
    "seg2_val.to_parquet(config.OUTPUT_DIR / 'seg2_val.parquet', index=False)\n",
    "seg2_test.to_parquet(config.OUTPUT_DIR / 'seg2_test.parquet', index=False)\n",
    "print(f\"   ✓ seg2_train.parquet ({seg2_train.shape})\")\n",
    "print(f\"   ✓ seg2_val.parquet ({seg2_val.shape})\")\n",
    "print(f\"   ✓ seg2_test.parquet ({seg2_test.shape})\")\n",
    "\n",
    "# CORRELATION TABLES\n",
    "print(\"\\n3. Сохранение таблиц корреляций...\")\n",
    "corr_seg1.to_csv(config.OUTPUT_DIR / 'correlation_segment1.csv', index=False)\n",
    "corr_seg2.to_csv(config.OUTPUT_DIR / 'correlation_segment2.csv', index=False)\n",
    "print(f\"   ✓ correlation_segment1.csv ({len(corr_seg1)} признаков)\")\n",
    "print(f\"   ✓ correlation_segment2.csv ({len(corr_seg2)} признаков)\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Все данные сохранены за {elapsed:.2f} сек\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# ИТОГОВАЯ СВОДКА\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓✓✓ ПОДГОТОВКА ДАННЫХ И EDA ЗАВЕРШЕНЫ УСПЕШНО ✓✓✓\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nДата: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Random seed: {config.RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СОХРАНЕННЫЕ ФАЙЛЫ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nDATA FILES (output/):\")\n",
    "print(f\"  • seg1_train.parquet - {seg1_train.shape}\")\n",
    "print(f\"  • seg1_val.parquet - {seg1_val.shape}\")\n",
    "print(f\"  • seg1_test.parquet - {seg1_test.shape}\")\n",
    "print(f\"  • seg2_train.parquet - {seg2_train.shape}\")\n",
    "print(f\"  • seg2_val.parquet - {seg2_val.shape}\")\n",
    "print(f\"  • seg2_test.parquet - {seg2_test.shape}\")\n",
    "\n",
    "print(f\"\\nCORRELATION ANALYSIS (output/):\")\n",
    "print(f\"  • correlation_segment1.csv - {len(corr_seg1)} признаков\")\n",
    "print(f\"  • correlation_segment2.csv - {len(corr_seg2)} признаков\")\n",
    "\n",
    "print(f\"\\nVISUALIZATIONS (figures/):\")\n",
    "print(f\"  • eda_target_distribution.png\")\n",
    "print(f\"  • correlation_segment1.png\")\n",
    "print(f\"  • correlation_segment2.png\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"КЛЮЧЕВЫЕ СТАТИСТИКИ\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nSEGMENT 1: {config.SEGMENT_1_NAME}\")\n",
    "print(f\"  Train: {len(seg1_train):,} | Churn: {seg1_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg1_val):,} | Churn: {seg1_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg1_test):,} | Churn: {seg1_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Признаков: {seg1_train.shape[1] - 1} (без target)\")\n",
    "print(f\"  Значимых корреляций: {corr_seg1['significant'].sum()}\")\n",
    "\n",
    "print(f\"\\nSEGMENT 2: {config.SEGMENT_2_NAME}\")\n",
    "print(f\"  Train: {len(seg2_train):,} | Churn: {seg2_train[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Val: {len(seg2_val):,} | Churn: {seg2_val[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Test: {len(seg2_test):,} | Churn: {seg2_test[config.TARGET_COLUMN].mean()*100:.2f}%\")\n",
    "print(f\"  Признаков: {seg2_train.shape[1] - 1} (без target)\")\n",
    "print(f\"  Значимых корреляций: {corr_seg2['significant'].sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"СЛЕДУЮЩИЙ ШАГ: Feature Engineering и обучение моделей\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
